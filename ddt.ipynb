{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Input IDs shape: torch.Size([4, 16])\n",
      "[Model] Condition IDs shape: torch.Size([4, 16])\n",
      "[Model] Embedded input: torch.Size([4, 16, 256])\n",
      "[Model] Embedded condition: torch.Size([4, 16, 256])\n",
      "[Noise Schedule] alpha_bar[100] = 0.895141\n",
      "[Model] Noise level shape: torch.Size([1, 1])\n",
      "[Model] Noisy input embedding: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Input: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After Attention: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After FeedForward: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Output: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Input: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After Attention: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After FeedForward: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Output: torch.Size([4, 16, 256])\n",
      "[Model] Encoded condition: torch.Size([4, 16, 256])\n",
      "[Model] Fused input (x + condition): torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Input: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After Attention: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After FeedForward: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Output: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Input: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After Attention: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After FeedForward: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Output: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Input: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After Attention: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After FeedForward: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Output: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Input: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After Attention: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] After FeedForward: torch.Size([4, 16, 256])\n",
      "[SimpleTransformerBlock] Output: torch.Size([4, 16, 256])\n",
      "[Model] Encoded x: torch.Size([4, 16, 256])\n",
      "[Model] Output logits: torch.Size([4, 16, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self,dim,heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim ,heads, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        print(f\"[SimpleTransformerBlock] Input: {x.shape}\")\n",
    "        attn_out, _ = self.attn(x,x,x)\n",
    "        print(f\"[SimpleTransformerBlock] After Attention: {attn_out.shape}\")\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        print(f\"[SimpleTransformerBlock] After FeedForward: {ff_out.shape}\")\n",
    "        x_out = self.norm2(x + ff_out)\n",
    "        print(f\"[SimpleTransformerBlock] Output: {x_out.shape}\")\n",
    "        return x_out\n",
    "\n",
    "# diffusion noise schedule\n",
    "def noise_schedule(t, beta_start=1e-4, beta_end=0.02, steps=1000):\n",
    "    beta = torch.linspace(beta_start, beta_end, steps)\n",
    "    alpha = 1 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim = 0)\n",
    "    print(f\"[Noise Schedule] alpha_bar[{t}] = {alpha_bar[t].item():.6f}\")\n",
    "    return alpha_bar[t]\n",
    "\n",
    "# decoupled transformer diffusion model\n",
    "class DecoupledDiffusionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.transformer_x = nn.Sequential(*[SimpleTransformerBlock(embed_dim) for _ in range(num_layers)])\n",
    "        self.transformer_cond = nn.Sequential(*[SimpleTransformerBlock(embed_dim) for _ in range(2)])\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x_noisy, condition, t):\n",
    "        print(f\"[Model] Input IDs shape: {x_noisy.shape}\")\n",
    "        print(f\"[Model] Condition IDs shape: {condition.shape}\")\n",
    "\n",
    "        x_embed = self.embed(x_noisy)\n",
    "        cond_embed = self.embed(condition)\n",
    "        print(f\"[Model] Embedded input: {x_embed.shape}\")\n",
    "        print(f\"[Model] Embedded condition: {cond_embed.shape}\")\n",
    "\n",
    "        noise_level = noise_schedule(t).sqrt().unsqueeze(-1).unsqueeze(-1).to(x_embed.device)\n",
    "        print(f\"[Model] Noise level shape: {noise_level.shape}\")\n",
    "        x_embed = x_embed * noise_level\n",
    "        print(f\"[Model] Noisy input embedding: {x_embed.shape}\")\n",
    "\n",
    "        cond_encoded = self.transformer_cond(cond_embed)\n",
    "        print(f\"[Model] Encoded condition: {cond_encoded.shape}\")\n",
    "\n",
    "        fused_input = x_embed + cond_encoded\n",
    "        print(f\"[Model] Fused input (x + condition): {fused_input.shape}\")\n",
    "        x_encoded = self.transformer_x(fused_input)\n",
    "        print(f\"[Model] Encoded x: {x_encoded.shape}\")\n",
    "\n",
    "        output_logits = self.output(x_encoded)\n",
    "        print(f\"[Model] Output logits: {output_logits.shape}\")\n",
    "        return output_logits\n",
    "\n",
    "# Test the model\n",
    "batch_size = 4\n",
    "seq_len = 16\n",
    "vocab_size = 10000\n",
    "embed_dim = 256\n",
    "timestep = 100\n",
    "\n",
    "model = DecoupledDiffusionTransformer(embed_dim , vocab_size)\n",
    "\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)) #fake data\n",
    "condition_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "output_logits = model(input_ids , condition_ids , t=timestep) #forward pass with noisy input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDT (Decoupled Diffusion Transformer)\n",
    "1. a model that adds noise to input data progressively during training (diffusion)\n",
    "2. learns to remove that noise using transformer architecture\n",
    "3. separates the roles of input and conditioning informaton into decoupled transformers\n",
    "    * one transformer (transformer_x) learns how to denoise the noisy input\n",
    "    * another transformer (transformer_cond) handles additional context or conditioning information\n",
    "\n",
    "Diffusion Process\n",
    "1. start with clean input data\n",
    "2. add gaussian noise step by step -> simulate a \"diffusion\" process\n",
    "3. train a model to reverse this noise step by step (denoise)\n",
    "\n",
    "In Action\n",
    "* the condition provides context like a sentence prompt or a label\n",
    "* the input is a noisy version of what we want to generate\n",
    "* the model learns to denoise th einput guided by the condition\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
