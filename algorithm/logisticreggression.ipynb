{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is use algorithm using pytorch functions. To train in modifcations of said algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacement for sklearn.model_selection import train_test_split \n",
    "\n",
    "def train_test_split_torch(X, y, test_size=0.2, shuffle=True, random_state=None):\n",
    "\n",
    "    if not isinstance(X, torch.Tensor): #wanna make sure X and Y are tensor\n",
    "        X = torch.tensor(X,dtype=torch.float32)\n",
    "    if not isinstance(y, torch.Tensor):\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    if random_state is not None:\n",
    "        torch.manual_seed(random_state)\n",
    "    if shuffle:\n",
    "        perm = torch.randperm(X.size(0)) #generate random permutation indices\n",
    "        X = X[perm]\n",
    "        y = y[perm]\n",
    "\n",
    "    # calculate split index\n",
    "    split_idx = int(X.size(0) * (1 - test_size))\n",
    "\n",
    "    #split data\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    return X_train, X_test , y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacement for sklean. import StandardScaler\n",
    "\n",
    "# StandardScaler is to standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None \n",
    "\n",
    "    def fit(self,X):\n",
    "        self.mean = torch.mean(X,dim=0) # compute mean along the sample axis\n",
    "        self.std = torch.std(X, dim=0) # compute standard deviation along the sample axis\n",
    "        self.std[self.std == 0] = 1.0   # avoid division by zero for constant features\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise ValueError('Scaler has not been fitted yet.')\n",
    "        return (X - self.mean)/self.std \n",
    "\n",
    "    def fit_transform(self,X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_pred, num_classes, average='weighted'):\n",
    "    \"\"\"\n",
    "    Calculate the weighted precision score for multi-class classification.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (torch.Tensor): Ground truth labels.\n",
    "        y_pred (torch.Tensor): Predicted labels.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        float: Weighted precision score.\n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = torch.zeros(num_classes)\n",
    "    false_positives = torch.zeros(num_classes)\n",
    "    support = torch.zeros(num_classes)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        true_positives[cls] = ((y_pred == cls) & (y_true == cls)).sum().item()\n",
    "        false_positives[cls] = ((y_pred == cls) & (y_true != cls)).sum().item()\n",
    "        support[cls] = (y_true == cls).sum().item()\n",
    "\n",
    "    precision_per_class = true_positives / (true_positives + false_positives + 1e-8)  # Add epsilon to avoid division by zero\n",
    "\n",
    "    if average == 'weighted':\n",
    "        total_samples = support.sum().item()\n",
    "        return (precision_per_class * support / total_samples).sum().item()\n",
    "    elif average == 'macro':\n",
    "        return precision_per_class.mean().item()\n",
    "    elif average == 'micro':\n",
    "        total_tp = true_positives.sum().item()\n",
    "        total_fp = false_positives.sum().item()\n",
    "        return total_tp / (total_tp + total_fp + 1e-8)\n",
    "\n",
    "        #sum().item() so it return scalar instead of tensor\n",
    "\n",
    "    else: \n",
    "        raise ValueError('invalid value for average choose from weighted, macro or mciro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__() \n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fadzw\\AppData\\Local\\Temp\\ipykernel_9196\\812732374.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\fadzw\\AppData\\Local\\Temp\\ipykernel_9196\\812732374.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\fadzw\\AppData\\Local\\Temp\\ipykernel_9196\\812732374.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.long)\n",
      "C:\\Users\\fadzw\\AppData\\Local\\Temp\\ipykernel_9196\\812732374.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test,dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([120, 4])\n",
      "y_train shape: torch.Size([120])\n",
      "Epoch [100/1000], Loss: 0.6064\n",
      "Epoch [200/1000], Loss: 0.5022\n",
      "Epoch [300/1000], Loss: 0.4481\n",
      "Epoch [400/1000], Loss: 0.4131\n",
      "Epoch [500/1000], Loss: 0.3877\n",
      "Epoch [600/1000], Loss: 0.3679\n",
      "Epoch [700/1000], Loss: 0.3518\n",
      "Epoch [800/1000], Loss: 0.3381\n",
      "Epoch [900/1000], Loss: 0.3262\n",
      "Epoch [1000/1000], Loss: 0.3156\n",
      "Test Precision: 0.9007\n"
     ]
    }
   ],
   "source": [
    "# replacement for from sklearn.datasets import load_iris \n",
    "\n",
    "#load iris dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" # cant opt for DataLoader if the data is big\n",
    "columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "data = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# encode species as integers\n",
    "species_map = {'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2}\n",
    "data['species'] = data['species'].map(species_map)\n",
    "\n",
    "# extract features and labels\n",
    "X = data[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]].values\n",
    "y = data[\"species\"].values\n",
    "\n",
    "#split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split_torch(X, y,test_size=0.2,random_state=42)\n",
    "\n",
    "#standardize the feautres\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert to pytorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test,dtype=torch.long)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(torch.unique(y_train))\n",
    "model = LogisticRegression(input_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function for multi-class classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1000 \n",
    "for  epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs,1)\n",
    "\n",
    "    precision = precision_score(y_test.numpy(), predicted.numpy(),num_classes=num_classes,average='weighted')\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.6985, LR: 0.005000\n",
      "Epoch [200/1000], Loss: 0.5916, LR: 0.002500\n",
      "Epoch [300/1000], Loss: 0.5582, LR: 0.001250\n",
      "Epoch [400/1000], Loss: 0.5445, LR: 0.000625\n",
      "Epoch [500/1000], Loss: 0.5382, LR: 0.000313\n",
      "Epoch [600/1000], Loss: 0.5352, LR: 0.000156\n",
      "Epoch [700/1000], Loss: 0.5338, LR: 0.000078\n",
      "Epoch [800/1000], Loss: 0.5330, LR: 0.000039\n",
      "Epoch [900/1000], Loss: 0.5327, LR: 0.000020\n",
      "Epoch [1000/1000], Loss: 0.5325, LR: 0.000010\n",
      "Test Precision: 0.7860\n"
     ]
    }
   ],
   "source": [
    "# Experiment with learning rate schedulers\n",
    "\n",
    "# Reinitialize the model\n",
    "model = LogisticRegression(input_dim, num_classes)\n",
    "\n",
    "# Redefine the optimizer and scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print loss and current learning rate every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    precision = precision_score(y_test.numpy(), predicted.numpy(), num_classes=num_classes, average='weighted')\n",
    "    print(f\"Test Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment with lambda_reg = 0.01, lambda_bias_reg = 0.001\n",
      "Epoch [100/1000], Loss: 0.6890\n",
      "Epoch [200/1000], Loss: 0.5536\n",
      "Epoch [300/1000], Loss: 0.4911\n",
      "Epoch [400/1000], Loss: 0.4531\n",
      "Epoch [500/1000], Loss: 0.4267\n",
      "Epoch [600/1000], Loss: 0.4068\n",
      "Epoch [700/1000], Loss: 0.3909\n",
      "Epoch [800/1000], Loss: 0.3776\n",
      "Epoch [900/1000], Loss: 0.3662\n",
      "Epoch [1000/1000], Loss: 0.3561\n",
      "Test Precision: 0.9007\n",
      "\n",
      "Experiment with lambda_reg = 0.001, lambda_bias_reg = 0.001\n",
      "Epoch [100/1000], Loss: 0.6981\n",
      "Epoch [200/1000], Loss: 0.5522\n",
      "Epoch [300/1000], Loss: 0.4860\n",
      "Epoch [400/1000], Loss: 0.4456\n",
      "Epoch [500/1000], Loss: 0.4172\n",
      "Epoch [600/1000], Loss: 0.3955\n",
      "Epoch [700/1000], Loss: 0.3778\n",
      "Epoch [800/1000], Loss: 0.3630\n",
      "Epoch [900/1000], Loss: 0.3501\n",
      "Epoch [1000/1000], Loss: 0.3387\n",
      "Test Precision: 0.9007\n"
     ]
    }
   ],
   "source": [
    "# # add bias regulirzation\n",
    "\n",
    "# # regularzing wieght also regularize bias term\n",
    "\n",
    "# lambda_bias_reg = 0.001 \n",
    "\n",
    "# Define regularization strengths\n",
    "lambda_reg_values = [0.01, 0.001]  # Different weight regularization strengths\n",
    "lambda_bias_reg = 0.001  # Bias regularization strength\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for lambda_reg in lambda_reg_values:\n",
    "    print(f\"\\nExperiment with lambda_reg = {lambda_reg}, lambda_bias_reg = {lambda_bias_reg}\")\n",
    "\n",
    "    # Reinitialize the model\n",
    "    model = LogisticRegression(input_dim, num_classes)\n",
    "\n",
    "    # Reinitialize the optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        # Add L2 regularization for weights and bias\n",
    "        l2_reg = torch.tensor(0.0)\n",
    "        bias_reg = torch.tensor(0.0)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'bias' in name:  # Apply bias regularization\n",
    "                bias_reg += torch.norm(param, 2)\n",
    "            else:  # Apply weight regularization\n",
    "                l2_reg += torch.norm(param, 2)\n",
    "\n",
    "        loss += lambda_reg * l2_reg + lambda_bias_reg * bias_reg\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        precision = precision_score(y_test.numpy(), predicted.numpy(), num_classes=num_classes, average='weighted')\n",
    "        print(f\"Test Precision: {precision:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
