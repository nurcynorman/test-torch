{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dqn is the pioneer of deep reinforcement learning, has achieved significant sucess in various applications\n",
    "* here we train a DQN agent on CartPole with Tianshou\n",
    "* contrarty to existing Deep RL libraries such as RLIB\n",
    "* dqn only accept a config specification of hypermaraters , network and others, tianshou orovide easy wway of construction through code-level\n",
    "\n",
    "\n",
    "* agent to environemtn : action will be generated by agent and sent to environment\n",
    "* environtment to agent: env.step takes action and returns a tuple of observation ,reward,done,info\n",
    "* agent-environment interaction to agent training: the data generated by interaction will be stored and sent to the learner of agent\n",
    "* setup\n",
    "    1. verctorized environments\n",
    "    2. policy (with nueral networks)\n",
    "    3. collector (with buffer)\n",
    "    4. trainer to run RL traiing evaluation pipeline\n",
    "\n",
    "\n",
    "<img src=\"https://tianshou.org/en/stable/_images/pipeline.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import tianshou as ts \n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# CartPole-v1 includes a cart carrying a pole moving on a track\n",
    "# a simple environmetn with a discrete action space\n",
    "# you haave to identify whether the action space is continuous or discrete and apply suited algorithm\n",
    "\n",
    "# state is the position of the cart   , the velocity of the cart , the anglel of the pole and the velocity of the tip of the pole\n",
    "# action can only be of [0,1,2] for moving cart left, no move and right\n",
    "# reward : each timestep you last you will receove a+1 reward\n",
    "#done if Cartpole is out-of-range or timeout (the pole is more than 15 degrees from vertical or the cart moves more han 2.4 from the center or you last over 200 timesteps)\n",
    "# info , extra info from environment silmuation\n",
    "\n",
    "train_envs = gym.make('CartPole-v1')\n",
    "test_envs = gym.make('CartPole-v1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tianshou supports 4 types of verctorized environemtn wrapper\n",
    "* dummyvectorenv, the sequantional version using a single-thread for-loop\n",
    "* subprocvectorenv : use python multiprocessing and pipe for concurrent execution\n",
    "* shemvectorenv : usee share memry instead of pipe based on subprocvecotenv\n",
    "* rayvectorenv : useray for concurrent activities and is currentlt the only choice for parallel simulation in a clustiner with multiple machines, it can used as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([lambda: gym.make('CartPole-v1') for _ in range(10)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: gym.make('CartPole-v1') for _ in range(100)])\n",
    "\n",
    "#setup 10 env in train_env and 100 env in test_envs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'envpool'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# can also try super-fast vectorized environment EnvPool\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvpool\u001b[39;00m\n\u001b[32m      3\u001b[39m train_envs = envpool.make_gymnasium(\u001b[33m\"\u001b[39m\u001b[33mCartPole-v1\u001b[39m\u001b[33m\"\u001b[39m, num_envs=\u001b[32m10\u001b[39m)\n\u001b[32m      4\u001b[39m test_envs = envpool.make_gymnasium(\u001b[33m\"\u001b[39m\u001b[33mCartPole-v1\u001b[39m\u001b[33m\"\u001b[39m, num_envs=\u001b[32m100\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'envpool'"
     ]
    }
   ],
   "source": [
    "# can also try super-fast vectorized environment EnvPool\n",
    "import envpool\n",
    "train_envs = envpool.make_gymnasium(\"CartPole-v1\", num_envs=10)\n",
    "test_envs = envpool.make_gymnasium(\"CartPole-v1\", num_envs=100)\n",
    "\n",
    "#https://github.com/sail-sg/envpool/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np \n",
    "from torch import nn \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape),128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape)),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch,-1))\n",
    "        return logits , state \n",
    "\n",
    "state_shape = env.observation_space.shape or env.observation_space.n \n",
    "action_shape = env.action_space.shape or env.action_space.n \n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use pre-defined MLP networks in common, discrete and continuous. the rules of self-defined networks are \n",
    "1. input : observation obs (may be numpy.ndarray, torch.Tensor), hidden state provided by the environment\n",
    "2. output : some logits , the next hidden state the logits could be a tuple instead of a Torch.tensor or some other useful variables or results during the policy forwarding procedure. it depends on how the polict class process the network output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup poolicy\n",
    "# we use the defined net and optim above with extra policy hyper-parameters to define a policy Here we deffine a DQN policy with a target network\n",
    "\n",
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    action_space=env.action_space,\n",
    "    discount_factor=0.9,\n",
    "    target_update_freq=320\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup collector\n",
    "\n",
    "* collector allows the polict to interact with different type of environments\n",
    "* in each step, the collector will let the polict perform (at least) a specified numbers of steps or episodes and stoer the data in a replay buffer\n",
    "* notce that wehn setup a collector, VectorReplapyBuffer to be used in vectorized envrionment scenarios, adn the number of buffers in the following case 10 is preferred to be set as the nuumber of environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m test_collector = ts.data.Collector(policy, test_envs, exploration_noise=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# the main function of collector is the collect function which can be summarized in the following lines\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# result = self.policy(self.data, last_state)     #the agent predicts the batch action from batch observation\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# act = to_numpy(result.act)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# obs_next, rew, done, info = result \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# self.data.update(obs_next=obs_next, rew=rew, done=done, info=info)  #update the data with new state/reward/done/info\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m.policy(\u001b[38;5;28mself\u001b[39m.data, last_state)                         \u001b[38;5;66;03m# the agent predicts the batch action from batch observation\u001b[39;00m\n\u001b[32m     13\u001b[39m act = to_numpy(result.act)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.data.update(act=act)                                           \u001b[38;5;66;03m# update the data with new action/policy\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, 10), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)\n",
    "\n",
    "the main function of collector is the collect function which can be summarized in the following lines\n",
    "result = self.policy(self.data, last_state)     #the agent predicts the batch action from batch observation\n",
    "act = to_numpy(result.act)\n",
    "self.data.update(act=act) #update the data with new action/policy\n",
    "result = self.env.step(act, ready_env_ids)\n",
    "obs_next, rew, done, info = result \n",
    "self.data.update(obs_next=obs_next, rew=rew, done=done, info=info)  #update the data with new state/reward/done/info\n",
    "\n",
    "#self is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train policy with a Trainer \n",
    "\n",
    "* tianshou provides OnpolicyTrainer, OffpolicyTrainer, OfflineTrainer . \n",
    "* the trainer will automatically stop training when the policy reaches the stop condition stop_fn on test collector \n",
    "* Since DQN is an off-policy algorithm, we use the OffpolicyTraine as follows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:01, 637.63it/s, env_step=1000, gradient_step=100, len=21, n/ep=4, n/st=10, rew=21.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 14.870000 ± 1.764398, best_reward: 14.870000 ± 1.764398 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:00, 1093.64it/s, env_step=2000, gradient_step=200, len=20, n/ep=1, n/st=10, rew=20.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 33.290000 ± 47.105689, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1001it [00:00, 1143.12it/s, env_step=3000, gradient_step=300, len=9, n/ep=0, n/st=10, rew=9.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 22.360000 ± 5.197153, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1001it [00:00, 1032.20it/s, env_step=4000, gradient_step=400, len=11, n/ep=4, n/st=10, rew=11.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 9.500000 ± 0.911043, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1001it [00:00, 1091.60it/s, env_step=5000, gradient_step=500, len=22, n/ep=1, n/st=10, rew=22.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 9.540000 ± 1.043264, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 1001it [00:00, 1079.80it/s, env_step=6000, gradient_step=600, len=16, n/ep=1, n/st=10, rew=16.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 9.920000 ± 1.128539, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 1001it [00:00, 1135.29it/s, env_step=7000, gradient_step=700, len=10, n/ep=0, n/st=10, rew=10.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 9.480000 ± 0.865794, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 1001it [00:00, 1131.00it/s, env_step=8000, gradient_step=800, len=12, n/ep=3, n/st=10, rew=12.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 9.490000 ± 1.014840, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 1001it [00:00, 1094.87it/s, env_step=9000, gradient_step=900, len=22, n/ep=0, n/st=10, rew=22.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 9.410000 ± 0.895489, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 1001it [00:00, 1099.80it/s, env_step=10000, gradient_step=1000, len=15, n/ep=2, n/st=10, rew=15.50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 10.680000 ± 1.248038, best_reward: 33.290000 ± 47.105689 in #2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'InfoStats' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m result = ts.trainer.OffpolicyTrainer(\n\u001b[32m      2\u001b[39m     policy=policy,\n\u001b[32m      3\u001b[39m     train_collector=train_collector,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     stop_fn = \u001b[38;5;28;01mlambda\u001b[39;00m mean_rewards: mean_rewards >= env.spec.reward_threshold\n\u001b[32m     10\u001b[39m ).run()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfinished training! use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mduration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'InfoStats' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10, step_per_epoch=1000, step_per_collect=10,\n",
    "    update_per_step=0.1,episode_per_test=100,batch_size=64,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
    "    stop_fn = lambda mean_rewards: mean_rewards >= env.spec.reward_threshold\n",
    ").run()\n",
    "\n",
    "print(f'finished training! use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the meaning of each parameter is as follows \n",
    "* max_epoch: the maximum of epochs for training . the training process might be finished before reaching the max_epoch\n",
    "* step_per_epoch : the number of env step collected per epoch\n",
    "* step_per_collect: the number of transition of collector would collect before the network update. for example the code above means \"collect 10 transitiotions and do one policy network update\"\n",
    "* episode_per_test: the number of episodes for one policy evaluation\n",
    "* batch_size = the batch size of sample data, which is going to feed in the policy network\n",
    "* train_fn : a function receives the current number of epoch and step index, and performs some opertions at the beginning of training in this epoch. \n",
    "* test_fn : a function receives the average undscounted returns of the testing result, return a boolean which indicaetd whether goal reached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the trainer supports tensorbaord for logging\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from tianshou.utils import TensorboardLogger\n",
    "writer = SummaryWriter('log/dqn')\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass the logger into the trainer.\n",
    "\n",
    "the training result will be recorded into the tensorboard\n",
    "\n",
    "it shows that within approximately 4 seconds , we finished training a DQN agent on CartPole. The mean returns over 100 consecutive episodes 199.03\n",
    "\n",
    "## save/load policy\n",
    "\n",
    "policy inherits the class torch.nn.Module, so since the policy inhertis that, saving and loading the policy are exectly thee same as a torch.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch the agent's performance\n",
    "\n",
    "`Collector` supports rendering. Here is the example of watching the agent's performance in 35 FPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1/35)\n",
    "\n",
    "#can see action generated by trained agent\n",
    "action = policy(Batch(obs=np.array([obs]))).act[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Policy with Customised Codes\n",
    "\n",
    "if you dont want to use provided trainer and customize it, tianshou supports user-defined training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CollectStats' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m collect_result = train_collector.collect(n_step=\u001b[32m10\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# once if the collected episodes' mean returns reach the threshold,\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# or every 1000 steps, we test it on test_collector\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcollect_result\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrews\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.mean() >= env.spec.reward_threshold \u001b[38;5;129;01mor\u001b[39;00m i % \u001b[32m1000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     11\u001b[39m     policy.set_eps(\u001b[32m0.05\u001b[39m)\n\u001b[32m     12\u001b[39m     result = test_collector.collect(n_episode=\u001b[32m100\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'CollectStats' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# pre-collect at least 5000 transitions with random action before training\n",
    "train_collector.collect(n_step=5000, random=True)\n",
    "\n",
    "policy.set_eps(0.1)\n",
    "for i in range(int(1e6)):  # total step\n",
    "    collect_result = train_collector.collect(n_step=10)\n",
    "\n",
    "    # once if the collected episodes' mean returns reach the threshold,\n",
    "    # or every 1000 steps, we test it on test_collector\n",
    "    if collect_result['rews'].mean() >= env.spec.reward_threshold or i % 1000 == 0:\n",
    "        policy.set_eps(0.05)\n",
    "        result = test_collector.collect(n_episode=100)\n",
    "        if result['rews'].mean() >= env.spec.reward_threshold:\n",
    "            print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "            break\n",
    "        else:\n",
    "            # back to training eps\n",
    "            policy.set_eps(0.1)\n",
    "\n",
    "    # train policy with a sampled batch data from buffer\n",
    "    losses = policy.update(64, train_collector.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (269858882.py, line 97)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtorch.save(policy.state_dict(), import gymnasium as gym\u001b[39m\n                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BasePolicy.update() missing 1 required positional argument: 'buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(update_per_collect):\n\u001b[32m     90\u001b[39m     batch = train_collector.buffer.sample(batch_size)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Set the policy to evaluation mode for testing\u001b[39;00m\n\u001b[32m     94\u001b[39m policy.set_eps(eps_test)\n",
      "\u001b[31mTypeError\u001b[39m: BasePolicy.update() missing 1 required positional argument: 'buffer'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tianshou.policy import DQNPolicy\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-3\n",
    "gamma = 0.9\n",
    "n_step = 3\n",
    "target_update_freq = 320\n",
    "train_num = 8\n",
    "test_num = 8\n",
    "buffer_size = 20000\n",
    "batch_size = 64\n",
    "eps_train = 0.1\n",
    "eps_test = 0.05\n",
    "num_epochs = 10\n",
    "step_per_epoch = 1000\n",
    "collect_per_step = 10\n",
    "update_per_collect = 10\n",
    "episode_per_test = 10\n",
    "\n",
    "# Make environments\n",
    "train_envs = DummyVectorEnv([lambda: gym.make('CartPole-v1') for _ in range(train_num)])\n",
    "test_envs = DummyVectorEnv([lambda: gym.make('CartPole-v1') for _ in range(test_num)])\n",
    "train_envs.seed(seed)\n",
    "test_envs.seed(seed)\n",
    "\n",
    "# Define the network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if isinstance(obs, dict):\n",
    "            obs = obs['obs']\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        return self.model(obs), state\n",
    "\n",
    "# Create the policy\n",
    "env = gym.make('CartPole-v1')\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.n\n",
    "net = Net(state_shape, action_shape)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "policy = DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optimizer,\n",
    "    action_space=env.action_space,\n",
    "    discount_factor=gamma,\n",
    "    estimation_step=n_step,\n",
    "    target_update_freq=target_update_freq\n",
    ")\n",
    "\n",
    "# Collectors\n",
    "train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(buffer_size, len(train_envs))\n",
    ")\n",
    "test_collector = Collector(policy, test_envs)\n",
    "\n",
    "# Initial random data\n",
    "train_collector.collect(n_step=1000, reset_before_collect=True)\n",
    "\n",
    "# Manual training loop\n",
    "for epoch in range(num_epochs):\n",
    "    policy.set_eps(eps_train)\n",
    "    train_collector.collect(n_step=step_per_epoch)\n",
    "\n",
    "    # Start training\n",
    "    for _ in range(update_per_collect):\n",
    "        batch = train_collector.buffer.sample(batch_size)\n",
    "        policy.update(batch)\n",
    "\n",
    "    # Set the policy to evaluation mode for testing\n",
    "    policy.set_eps(eps_test)\n",
    "    result = test_collector.collect(n_episode=episode_per_test)\n",
    "    print(f\"Epoch #{epoch + 1}: test_reward: {result['rews'].mean():.2f}, length: {result['lens'].mean():.2f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(policy.state_dict(), 'dqn_cartpole_manual.pth')\n",
    "\n",
    "# Render one episode with the trained policy\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "policy.eval()\n",
    "policy.set_eps(0.01)\n",
    "collector = Collector(policy, env)\n",
    "collector.collect(n_episode=1)\n",
    "collector.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
