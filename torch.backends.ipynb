{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a6ad63-f02b-469b-8ce1-81a220396f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends availability and settings:\n",
      "\n",
      "CPU backend:\n",
      "<module 'torch.backends.cpu' from 'C:\\\\Users\\\\fadzw\\\\Downloads\\\\Trash\\\\test-torch\\\\venv\\\\Lib\\\\site-packages\\\\torch\\\\backends\\\\cpu\\\\__init__.py'>\n",
      "\n",
      "CUDA backend:\n",
      "False\n",
      "\n",
      "cuDNN backend:\n",
      "Enabled: True\n",
      "Available: False\n",
      "Version: None\n",
      "\n",
      "cuSparseLt backend:\n",
      "<module 'torch.backends.cusparselt' from 'C:\\\\Users\\\\fadzw\\\\Downloads\\\\Trash\\\\test-torch\\\\venv\\\\Lib\\\\site-packages\\\\torch\\\\backends\\\\cusparselt\\\\__init__.py'>\n",
      "\n",
      "MHA (Memory Efficient Attention):\n",
      "<module 'torch.backends.mha' from 'C:\\\\Users\\\\fadzw\\\\Downloads\\\\Trash\\\\test-torch\\\\venv\\\\Lib\\\\site-packages\\\\torch\\\\backends\\\\mha\\\\__init__.py'>\n",
      "\n",
      "MPS (Metal Performance Shaders for Apple):\n",
      "Is built: False\n",
      "\n",
      "MKL backend:\n",
      "<module 'torch.backends.mkl' from 'C:\\\\Users\\\\fadzw\\\\Downloads\\\\Trash\\\\test-torch\\\\venv\\\\Lib\\\\site-packages\\\\torch\\\\backends\\\\mkl\\\\__init__.py'>\n",
      "\n",
      "MKLDNN backend:\n",
      "Enabled: True\n",
      "\n",
      "NNPACK backend:\n",
      "Enabled: <module 'torch.backends.nnpack' from 'C:\\\\Users\\\\fadzw\\\\Downloads\\\\Trash\\\\test-torch\\\\venv\\\\Lib\\\\site-packages\\\\torch\\\\backends\\\\nnpack\\\\__init__.py'>\n",
      "\n",
      "OpenMP backend:\n",
      "<module 'torch.backends.openmp' from 'C:\\\\Users\\\\fadzw\\\\Downloads\\\\Trash\\\\test-torch\\\\venv\\\\Lib\\\\site-packages\\\\torch\\\\backends\\\\openmp\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch.backends availability and settings:\")\n",
    "\n",
    "print(\"\\nCPU backend:\")\n",
    "print(torch.backends.cpu)\n",
    "\n",
    "print(\"\\nCUDA backend:\")\n",
    "print(torch.backends.cuda.is_built())  # returns True if CUDA is available\n",
    "\n",
    "print(\"\\ncuDNN backend:\")\n",
    "print(f\"Enabled: {torch.backends.cudnn.enabled}\")\n",
    "print(f\"Available: {torch.backends.cudnn.is_available()}\")\n",
    "print(f\"Version: {torch.backends.cudnn.version()}\")\n",
    "\n",
    "print(\"\\ncuSparseLt backend:\")\n",
    "print(torch.backends.cusparselt)\n",
    "\n",
    "print(\"\\nMHA (Memory Efficient Attention):\")\n",
    "print(torch.backends.mha)\n",
    "\n",
    "print(\"\\nMPS (Metal Performance Shaders for Apple):\")\n",
    "print(f\"Is built: {torch.backends.mps.is_built()}\") if hasattr(torch.backends, 'mps') else print(\"MPS not available\")\n",
    "\n",
    "print(\"\\nMKL backend:\")\n",
    "print(torch.backends.mkl)\n",
    "\n",
    "print(\"\\nMKLDNN backend:\")\n",
    "print(f\"Enabled: {torch.backends.mkldnn.enabled}\")\n",
    "\n",
    "print(\"\\nNNPACK backend:\")\n",
    "print(f\"Enabled: {torch.backends.nnpack}\")\n",
    "\n",
    "print(\"\\nOpenMP backend:\")\n",
    "print(torch.backends.openmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9d15bf-cae2-4f32-ac11-76d046c1c898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU capability: AVX2\n",
      "Is PyTorch built with CUDA support? False\n",
      "CUDA is not built or not available\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "cpu_capability= torch.backends.cpu.get_cpu_capability() \n",
    "\n",
    "print(f'CPU capability: {cpu_capability}')\n",
    "# tells you what CPU instruction set PyTorch is optimized to use on your machine .\n",
    "\n",
    "cuda_built = torch.backends.cuda.is_built()\n",
    "\n",
    "print(f\"Is PyTorch built with CUDA support? {cuda_built}\")\n",
    "\n",
    "if torch.backends.cuda.is_built() and torch.cuda.is_available():\n",
    "    print('before changing:')\n",
    "    print(f'TF32 matmul allowed ? {torch.backends.cuda.matmul.allow_tf32}')\n",
    "\n",
    "    #disable tf32 matmul (for precision reasons)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    print(f'tf32 matmul allowed after disabling {torch.backends.cuda.matmul.allow_tf32}')\n",
    "\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    print(f'TF32 matmul allowed after re-enabling {torch.backends.cuda.matmul.allow_tf32}')\n",
    "else:\n",
    "    print('CUDA is not built or not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa351995",
   "metadata": {},
   "source": [
    "You're asking about a set of low-level PyTorch functions related to **Scaled Dot Product Attention (SDPA)**, especially different backend implementations for performance. These functions don't require CUDA to understand conceptually ‚Äî so let's walk through them clearly.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **What is Scaled Dot Product Attention (SDPA)?**\n",
    "\n",
    "This is the **core mechanism** in transformers (e.g., BERT, GPT). It works like this:\n",
    "\n",
    "```python\n",
    "Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd_k) * V\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* `Q`: Query matrix\n",
    "* `K`: Key matrix\n",
    "* `V`: Value matrix\n",
    "* `d_k`: Dimension of the key vectors (used to scale the dot products)\n",
    "* `softmax` gives attention weights\n",
    "\n",
    "This operation tells the model how much focus (attention) to give to each token when processing sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• What is **Flash Scaled Dot Product Attention**?\n",
    "\n",
    "FlashAttention is a **highly optimized GPU kernel** implementation of SDPA developed by *HazyResearch*. PyTorch integrated it for huge speed and memory improvements, especially on large models.\n",
    "\n",
    "Function:\n",
    "\n",
    "```python\n",
    "torch.backends.cuda.flash_sdp_enabled()\n",
    "```\n",
    "\n",
    "* ‚úÖ **Returns `True` if FlashAttention is available and used**\n",
    "* ‚ùå **Returns `False` if FlashAttention is not enabled or not available (e.g., no CUDA, old GPU)**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What is **Memory Efficient SDPA**?\n",
    "\n",
    "A different kernel that uses **less memory** than the standard attention algorithm. It's especially useful for long sequences or large batches.\n",
    "\n",
    "Function:\n",
    "\n",
    "```python\n",
    "torch.backends.cuda.mem_efficient_sdp(enabled: bool)\n",
    "```\n",
    "\n",
    "* **Enables/disables memory-efficient attention** in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ûó What is **Math SDPA**?\n",
    "\n",
    "This is the **default math-based implementation** (not optimized via GPU kernels like Flash or memory-efficient ones). It's **accurate**, but slower and uses more memory.\n",
    "\n",
    "Function:\n",
    "\n",
    "```python\n",
    "torch.backends.cuda.math_sdp_enabled()\n",
    "```\n",
    "\n",
    "* Checks if the math-based version is currently being used.\n",
    "\n",
    "Function:\n",
    "\n",
    "```python\n",
    "torch.backends.cuda.enable_math_sdp(enable: bool)\n",
    "```\n",
    "\n",
    "* Manually turn it on or off.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What is **FP16/BF16 reduction in math SDPA**?\n",
    "\n",
    "These are **reduced-precision floating point formats**:\n",
    "\n",
    "* **FP16**: 16-bit float\n",
    "* **BF16**: 16-bit, but with a different exponent format (better for training stability)\n",
    "\n",
    "Reduction refers to **summing over elements** (as in softmax or matmul). Using FP16/BF16 helps reduce memory and speed up computation ‚Äî but can cause **numerical instability** if not handled properly.\n",
    "\n",
    "Function:\n",
    "\n",
    "```python\n",
    "torch.backends.cuda.fp16_reduction_math_sdp_allowed()\n",
    "```\n",
    "\n",
    "* Returns whether this fast-reduction path is allowed in the math version of SDPA.\n",
    "\n",
    "Function:\n",
    "\n",
    "```python\n",
    "torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(enabled: bool)\n",
    "```\n",
    "\n",
    "* Enables/disables this behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Summary\n",
    "\n",
    "| Function                                      | Meaning                                                 |\n",
    "| --------------------------------------------- | ------------------------------------------------------- |\n",
    "| `flash_sdp_enabled()`                         | FlashAttention enabled or not (fastest, most efficient) |\n",
    "| `mem_efficient_sdp(enabled)`                  | Enables memory-efficient SDPA backend                   |\n",
    "| `math_sdp_enabled()`                          | Is the basic math implementation in use                 |\n",
    "| `enable_math_sdp(enable)`                     | Turn math-based SDPA on/off                             |\n",
    "| `fp16_reduction_math_sdp_allowed()`           | Can math SDPA use reduced-precision sums                |\n",
    "| `allow_fp16_bf16_reduction_math_sdp(enabled)` | Enable/disable fp16/bf16 reduction                      |\n",
    "\n",
    "---\n",
    "\n",
    "If you're running without CUDA, these will all return `False` or raise errors if the features aren‚Äôt available ‚Äî but they give you insight into the **backend strategy PyTorch is using for attention layers**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a36e0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîÅ **What is cuDNN Scaled Dot Product Attention?**\n",
    "\n",
    "### ‚úÖ `torch.backends.cuda.cudnn_sdp_enabled()`\n",
    "\n",
    "* Returns `True` if PyTorch is using **cuDNN‚Äôs implementation** of **Scaled Dot Product Attention (SDPA)**.\n",
    "\n",
    "### ‚úÖ `torch.backends.cuda.enable_cudnn_sdp(enabled: bool)`\n",
    "\n",
    "* Enables or disables the use of **cuDNN's SDPA** kernel.\n",
    "\n",
    "### üß† What is cuDNN SDPA?\n",
    "\n",
    "* **cuDNN (CUDA Deep Neural Network Library)** is NVIDIA's GPU-accelerated library for deep learning primitives.\n",
    "* Starting with cuDNN v8.9+, it includes a **fused attention kernel** (scaled dot-product attention as a single optimized GPU kernel).\n",
    "* This offers a balance of **performance, memory usage, and precision**.\n",
    "\n",
    "### üöÄ Use Cases\n",
    "\n",
    "* cuDNN SDPA is used **automatically** by PyTorch when:\n",
    "\n",
    "  * A compatible GPU is present (usually recent Ampere/Hopper architecture)\n",
    "  * Input dimensions and settings allow it\n",
    "* Works well for:\n",
    "\n",
    "  * **Short-to-medium sequence lengths**\n",
    "  * Use in models like **BERT**, **T5**, etc.\n",
    "  * **Inference or training** with decent batch sizes\n",
    "\n",
    "---\n",
    "\n",
    "## üìú Are there any research papers on cuDNN SDPA?\n",
    "\n",
    "* **No standalone research paper** on cuDNN SDPA ‚Äî it's **proprietary** and part of NVIDIA's **cuDNN documentation** and release notes:\n",
    "\n",
    "  * You can check [NVIDIA cuDNN Release Notes](https://docs.nvidia.com/deeplearning/cudnn/release-notes/index.html)\n",
    "  * Or [cuDNN Developer Guide](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "## üî• What is FlashAttention?\n",
    "\n",
    "FlashAttention is a **high-performance attention kernel** developed by **HazyResearch** (Stanford).\n",
    "\n",
    "### üìò Paper:\n",
    "\n",
    "> **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**\n",
    "> Tri Dao et al., 2022\n",
    "> [üìÑ Read it here](https://arxiv.org/abs/2205.14135)\n",
    "\n",
    "### üß† Key Ideas:\n",
    "\n",
    "* Traditional attention is **memory bound** (slow because of memory reads/writes)\n",
    "* FlashAttention uses **tiling and fused kernels** to:\n",
    "\n",
    "  * Avoid storing intermediate results\n",
    "  * Reduce memory usage\n",
    "  * Increase speed by keeping everything on GPU registers/shared memory\n",
    "\n",
    "### üß™ Use Cases:\n",
    "\n",
    "* Ideal for **long sequences** and **large models** (like LLMs)\n",
    "* Particularly useful during **training**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Check if PyTorch is Built with FlashAttention\n",
    "\n",
    "### `torch.backends.cuda.is_flash_attention_available()`\n",
    "\n",
    "* Returns `True` if FlashAttention backend is compiled and CUDA is available.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Can FlashAttention Be Used for This Attention Call?\n",
    "\n",
    "### `torch.backends.cuda.can_flash_attention(params: SDPAParams, debug=False)`\n",
    "\n",
    "* This checks if the current inputs (shape, dtype, etc.) are **compatible** with FlashAttention.\n",
    "* If not, PyTorch will **fallback to other backends** (like cuDNN or math).\n",
    "\n",
    "---\n",
    "\n",
    "## üîß What is `SDPAParams`?\n",
    "\n",
    "This is a PyTorch internal struct that packages all inputs needed for **Scaled Dot Product Attention**, such as:\n",
    "\n",
    "```python\n",
    "SDPAParams(\n",
    "  query: Tensor,\n",
    "  key: Tensor,\n",
    "  value: Tensor,\n",
    "  attn_mask: Optional[Tensor] = None,\n",
    "  dropout_p: float = 0.0,\n",
    "  is_causal: bool = False\n",
    ")\n",
    "```\n",
    "\n",
    "### Why it exists:\n",
    "\n",
    "* PyTorch provides **multiple SDPA backends** (Flash, cuDNN, memory-efficient, math).\n",
    "* `SDPAParams` is used to let these backends inspect the inputs and decide if they can run the operation efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Table\n",
    "\n",
    "| Function                         | Purpose                                                |\n",
    "| -------------------------------- | ------------------------------------------------------ |\n",
    "| `cudnn_sdp_enabled()`            | Is cuDNN's SDPA being used?                            |\n",
    "| `enable_cudnn_sdp(enabled)`      | Turn cuDNN SDPA on/off                                 |\n",
    "| `is_flash_attention_available()` | Is FlashAttention compiled & usable?                   |\n",
    "| `can_flash_attention(params)`    | Can FlashAttention run with given tensors?             |\n",
    "| `SDPAParams`                     | Struct for query, key, value, etc., passed to backends |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c3683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ `torch.backends.cuda.can_use_efficient_attention(params, debug=False)`\n",
    "\n",
    "* This function checks if the **efficient attention backend** (also called **\"memory-efficient attention\"**) **can be used** for your specific input tensors and settings (passed via `SDPAParams`).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What is Efficient Attention?\n",
    "\n",
    "**Efficient Attention** (aka **Memory-Efficient Attention**) is an optimized implementation that:\n",
    "\n",
    "* Reduces memory usage by **avoiding explicit attention matrices**.\n",
    "* Fuses operations to minimize intermediate memory.\n",
    "* Is suitable for **large batch sizes** or **long sequences**.\n",
    "\n",
    "It was inspired by:\n",
    "\n",
    "> **\"Memory-Efficient Attention\" by Tri Dao**\n",
    "> (not the same as FlashAttention but related idea)\n",
    "\n",
    "Used heavily in:\n",
    "\n",
    "* Training **transformers**\n",
    "* Large models where **GPU memory** is a limiting factor\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ `torch.backends.cuda.can_use_cudnn_attention(params, debug=False)`\n",
    "\n",
    "* Checks if the **cuDNN backend** can handle the current attention configuration.\n",
    "* If not, PyTorch will **fallback** to another backend like Flash, math, or memory-efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ `torch.backends.cuda.sdp_kernel(...)`\n",
    "\n",
    "This **context manager** is used to **enable/disable specific backends** for `scaled_dot_product_attention()`.\n",
    "\n",
    "```python\n",
    "torch.backends.cuda.sdp_kernel(\n",
    "    enable_flash=True,\n",
    "    enable_math=True,\n",
    "    enable_mem_efficient=True,\n",
    "    enable_cudnn=True\n",
    ")\n",
    "```\n",
    "\n",
    "You can use it like this:\n",
    "\n",
    "```python\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=False):\n",
    "    # This block will not use FlashAttention\n",
    "    out = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ What Are the \"Three\" (actually **Four**) Backends?\n",
    "\n",
    "PyTorch's SDPA system dynamically chooses between the following:\n",
    "\n",
    "| Backend                                       | Description                                                              |\n",
    "| --------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| ‚úÖ **FlashAttention**                          | Fastest, GPU-efficient, best for large sequence & batch sizes.           |\n",
    "| ‚úÖ **Memory-Efficient** (efficient\\_attention) | Uses less memory, suitable when GPU RAM is tight.                        |\n",
    "| ‚úÖ **cuDNN Attention**                         | Uses NVIDIA's cuDNN implementation; often a good general-purpose choice. |\n",
    "| ‚úÖ **Math-based (unfused)**                    | Pure PyTorch fallback, slower, used when others can‚Äôt.                   |\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ How PyTorch Chooses a Backend (Simplified):\n",
    "\n",
    "1. If FlashAttention is available and inputs are compatible ‚Üí use it\n",
    "2. Else, if Memory-Efficient works ‚Üí use it\n",
    "3. Else, if cuDNN works ‚Üí use it\n",
    "4. Else ‚Üí fall back to math implementation\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What is `SDPAParams`?\n",
    "\n",
    "It's an internal struct passed to backend decision functions. Contains:\n",
    "\n",
    "```python\n",
    "SDPAParams(\n",
    "  query: Tensor,\n",
    "  key: Tensor,\n",
    "  value: Tensor,\n",
    "  attn_mask: Optional[Tensor] = None,\n",
    "  dropout_p: float = 0.0,\n",
    "  is_causal: bool = False\n",
    ")\n",
    "```\n",
    "\n",
    "Used in:\n",
    "\n",
    "* `can_flash_attention(...)`\n",
    "* `can_use_efficient_attention(...)`\n",
    "* `can_use_cudnn_attention(...)`\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Summary\n",
    "\n",
    "* **FlashAttention** = Fastest but GPU- and shape-sensitive.\n",
    "* **EfficientAttention** = Uses less memory, slower than Flash, great fallback.\n",
    "* **cuDNNAttention** = Good general purpose, fused, stable.\n",
    "* **Math** = Slow fallback.\n",
    "\n",
    "Use `sdp_kernel()` to test or force which backend is used.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
