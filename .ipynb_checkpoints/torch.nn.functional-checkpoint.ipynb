{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/docs/stable/nn.functional.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 16, 50])\n",
      "Weight shape: torch.Size([16, 33, 5])\n",
      "Bias shape: torch.Size([33])\n",
      "Output shape: torch.Size([2, 33, 102])\n"
     ]
    }
   ],
   "source": [
    "#transpose1d \n",
    "\n",
    "#applies a 1D transposed convolution to an input signal\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "inputs = torch.randn(2,16,50) \n",
    "weights = torch.randn(16,33,5)\n",
    "\n",
    "bias = torch.rand(33)\n",
    "\n",
    "output = F.conv_transpose1d(\n",
    "    input = inputs,\n",
    "    weight = weights,\n",
    "    bias = bias,\n",
    "    stride = 2,\n",
    "    padding = 1,\n",
    "    output_padding=1,\n",
    "    groups=1,\n",
    "    dilation=1\n",
    ")\n",
    "\n",
    "print('Input shape:',inputs.shape)\n",
    "print('Weight shape:', weights.shape)\n",
    "print('Bias shape:', bias.shape)\n",
    "print('Output shape:', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 4, 5, 5])\n",
      "Weight shape: torch.Size([4, 8, 3, 3])\n",
      "Bias shape: torch.Size([8])\n",
      "Output shape: torch.Size([1, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "#transpose2d\n",
    "\n",
    "#applies 2d transposed convolution to an input image. \n",
    "# Transposed convolutions are commonly used in tasks like upsampling image generation and semantic segmentations\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# batch_size = 1, in_channels = 4 , h = 5 w =5\n",
    "inputs = torch.randn(1,4,5,5)\n",
    "\n",
    "# in_channel = 4 , out_channels = 8 kernel_height = 3, kernel_width = 3\n",
    "weights = torch.randn( 4, 8, 3,3)\n",
    "bias = torch.randn(8)\n",
    "\n",
    "output = F.conv_transpose2d(\n",
    "    input = inputs,\n",
    "    weight = weights,\n",
    "    bias = bias,\n",
    "    stride = 2,\n",
    "    padding = 1,\n",
    "    output_padding = 1,\n",
    "    groups = 1,\n",
    "    dilation =1\n",
    ")\n",
    "\n",
    "print('Input shape:',inputs.shape)\n",
    "print('Weight shape:',weights.shape)\n",
    "print('Bias shape:',bias.shape)\n",
    "print('Output shape:', output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shaoe: torch.Size([2, 16, 50, 10, 20])\n",
      "Weight shape: torch.Size([16, 33, 3, 3, 3])\n",
      "Bias shape: torch.Size([33])\n",
      "Output shape: torch.Size([2, 33, 100, 20, 40])\n"
     ]
    }
   ],
   "source": [
    "#transpose3d \n",
    "\n",
    "#function applies a 3d transposed convolution to a 3d tensor\n",
    "#transposed convolutions in 3d are commonly used in tasks like volumetric upsamping ,medical imagin and video processing\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define input tensor\n",
    "    # batch_size = 2 , in_channels = 16, depth = 50, kernel_height =3, kernel_widht = 3\n",
    "inputs = torch.randn(2,16,50,10,20) \n",
    "\n",
    "#define weight tensor\n",
    "    #in_channels = 16, out_channels =33, kernel_depth=3, kernel_height=3, kernel_width =3\n",
    "weights = torch.randn(16,33,3,3,3)\n",
    "\n",
    "bias = torch.randn(33) #define optional bias tensor (Out_channel = 33)\n",
    "\n",
    "output = F.conv_transpose3d(\n",
    "    input=inputs,\n",
    "    weight=weights,\n",
    "    bias = bias,\n",
    "    stride =2, #stride of the conv\n",
    "    padding = 1,\n",
    "    output_padding =1,\n",
    "    groups = 1,\n",
    "    dilation =1 #dilation rate\n",
    ")\n",
    "\n",
    "print('Input shaoe:',inputs.shape)\n",
    "print('Weight shape:',weights.shape)\n",
    "print('Bias shape:', bias.shape)\n",
    "print('Output shape:', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://turbolearn.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 3, 4, 4])\n",
      "Input Tensor:\n",
      " tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]],\n",
      "\n",
      "         [[16., 17., 18., 19.],\n",
      "          [20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27.],\n",
      "          [28., 29., 30., 31.]],\n",
      "\n",
      "         [[32., 33., 34., 35.],\n",
      "          [36., 37., 38., 39.],\n",
      "          [40., 41., 42., 43.],\n",
      "          [44., 45., 46., 47.]]]])\n",
      "\n",
      " Unfolder Tensor Shape: torch.Size([1, 12, 9])\n",
      "Unfolded Tensor:\n",
      " tensor([[[ 0.,  1.,  2.,  4.,  5.,  6.,  8.,  9., 10.],\n",
      "         [ 1.,  2.,  3.,  5.,  6.,  7.,  9., 10., 11.],\n",
      "         [ 4.,  5.,  6.,  8.,  9., 10., 12., 13., 14.],\n",
      "         [ 5.,  6.,  7.,  9., 10., 11., 13., 14., 15.],\n",
      "         [16., 17., 18., 20., 21., 22., 24., 25., 26.],\n",
      "         [17., 18., 19., 21., 22., 23., 25., 26., 27.],\n",
      "         [20., 21., 22., 24., 25., 26., 28., 29., 30.],\n",
      "         [21., 22., 23., 25., 26., 27., 29., 30., 31.],\n",
      "         [32., 33., 34., 36., 37., 38., 40., 41., 42.],\n",
      "         [33., 34., 35., 37., 38., 39., 41., 42., 43.],\n",
      "         [36., 37., 38., 40., 41., 42., 44., 45., 46.],\n",
      "         [37., 38., 39., 41., 42., 43., 45., 46., 47.]]])\n",
      "\n",
      "Folded Tensor Shape: torch.Size([1, 3, 4, 4])\n",
      "Folded Tensor:\n",
      " tensor([[[[  0.,   2.,   4.,   3.],\n",
      "          [  8.,  20.,  24.,  14.],\n",
      "          [ 16.,  36.,  40.,  22.],\n",
      "          [ 12.,  26.,  28.,  15.]],\n",
      "\n",
      "         [[ 16.,  34.,  36.,  19.],\n",
      "          [ 40.,  84.,  88.,  46.],\n",
      "          [ 48., 100., 104.,  54.],\n",
      "          [ 28.,  58.,  60.,  31.]],\n",
      "\n",
      "         [[ 32.,  66.,  68.,  35.],\n",
      "          [ 72., 148., 152.,  78.],\n",
      "          [ 80., 164., 168.,  86.],\n",
      "          [ 44.,  90.,  92.,  47.]]]])\n"
     ]
    }
   ],
   "source": [
    "#unfold \n",
    "\n",
    "# function extracts sliding local bloacls from batched input tensor\n",
    "# this is commonly used in tasks like convolutional neural networks where you need to extract small regions of an image for processing\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define a 4d input\n",
    "    # batch_size = 1 , channels =3, height = 4, width =4\n",
    "input_tensor = torch.arange(48, dtype=torch.float32).reshape(1,3,4,4)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "#parameters for the unfold operation\n",
    "kernel_size = (2,2)\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation = 1\n",
    "\n",
    "unfolded = F.unfold(input=input_tensor, kernel_size = kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "#print the unfolder tensor\n",
    "print('\\n Unfolder Tensor Shape:',unfolded.shape)\n",
    "print('Unfolded Tensor:\\n', unfolded)\n",
    "\n",
    "#fold\n",
    "#Optinaly , fold the tensor back to its original shape\n",
    "#it aggragtes overlapping patches using summation by default\n",
    "folded = F.fold(input=unfolded, output_size=(4,4),kernel_size=kernel_size, padding=padding, dilation=dilation)\n",
    "print('\\nFolded Tensor Shape:', folded.shape)\n",
    "print('Folded Tensor:\\n', folded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 7])\n",
      "Input Tensor:\n",
      " tensor([[1., 2., 3., 4., 5., 6., 7.]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 3])\n",
      "Output Tensor:\n",
      " tensor([[2., 4., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#avg_pool1d\n",
    "\n",
    "# applies 1d average pooling over an input singal\n",
    "# used in tasks like time series analysis or audio processing \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define a 3d input tensor \n",
    "    #batch_size = 1, channels = 1, length =7\n",
    "input_tensor = torch.tensor([[1,2,3,4,5,6,7]], dtype = torch.float32)\n",
    "\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n',input_tensor)\n",
    "\n",
    "kernel_size = 3,\n",
    "stride = 2\n",
    "padding = 0\n",
    "ceil_mode = False\n",
    "count_include_pad = True \n",
    "\n",
    "#aaply 1d average pooling \n",
    "output = F.avg_pool1d(\n",
    "    input = input_tensor,\n",
    "    kernel_size = kernel_size,\n",
    "    stride = stride ,\n",
    "    padding = padding,\n",
    "    ceil_mode = ceil_mode,\n",
    "    count_include_pad = count_include_pad\n",
    ")\n",
    "\n",
    "print('\\nOutput Tensor Shape:', output.shape)\n",
    "print('Output Tensor:\\n', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 6, 6])\n",
      "Input Tensor:\n",
      " tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27., 28., 29.],\n",
      "          [30., 31., 32., 33., 34., 35.]]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 3, 3])\n",
      "Output Tensor:\n",
      " tensor([[[[ 3.5000,  5.5000,  7.5000],\n",
      "          [15.5000, 17.5000, 19.5000],\n",
      "          [27.5000, 29.5000, 31.5000]]]])\n"
     ]
    }
   ],
   "source": [
    "#torch.nn.functional.avg_pool2d \n",
    "\n",
    "# function applies 2d average pooling over an input image or feature map\n",
    "# commonly used in CNN for downsampling\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "#define 4d input tensor\n",
    "    #batch_size =1, channels=1, height=6, width=6\n",
    "input_tensor = torch.arange(36, dtype=torch.float32).reshape(1, 1, 6, 6)\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Input Tensor:\\n\", input_tensor)\n",
    "\n",
    "# Parameters for the avg_pool2d operation\n",
    "kernel_size = (2, 2)  # Size of the pooling window\n",
    "stride = (2, 2)       # Stride of the pooling window\n",
    "padding = 0           # Padding added to the input\n",
    "ceil_mode = False     # Use floor instead of ceil to compute output size\n",
    "count_include_pad = True  # Include zero-padding in averaging calculation\n",
    "divisor_override = None   # Use the default divisor (pooling region size)\n",
    "\n",
    "# Apply 2D average pooling\n",
    "output = F.avg_pool2d(\n",
    "    input=input_tensor,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    ceil_mode=ceil_mode,\n",
    "    count_include_pad=count_include_pad,\n",
    "    divisor_override=divisor_override\n",
    ")\n",
    "\n",
    "# Print the output tensor\n",
    "print(\"\\nOutput Tensor Shape:\", output.shape)\n",
    "print(\"Output Tensor:\\n\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 4, 4, 4])\n",
      "Input Tensor: tensor([[[[[ 0.,  1.,  2.,  3.],\n",
      "           [ 4.,  5.,  6.,  7.],\n",
      "           [ 8.,  9., 10., 11.],\n",
      "           [12., 13., 14., 15.]],\n",
      "\n",
      "          [[16., 17., 18., 19.],\n",
      "           [20., 21., 22., 23.],\n",
      "           [24., 25., 26., 27.],\n",
      "           [28., 29., 30., 31.]],\n",
      "\n",
      "          [[32., 33., 34., 35.],\n",
      "           [36., 37., 38., 39.],\n",
      "           [40., 41., 42., 43.],\n",
      "           [44., 45., 46., 47.]],\n",
      "\n",
      "          [[48., 49., 50., 51.],\n",
      "           [52., 53., 54., 55.],\n",
      "           [56., 57., 58., 59.],\n",
      "           [60., 61., 62., 63.]]]]])\n",
      "\n",
      " OUtput Tensor Shape: torch.Size([1, 1, 2, 2, 2])\n",
      "Output Tensor: tensor([[[[[10.5000, 12.5000],\n",
      "           [18.5000, 20.5000]],\n",
      "\n",
      "          [[42.5000, 44.5000],\n",
      "           [50.5000, 52.5000]]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#torch.nn.functional.avg_pool3d\n",
    "\n",
    "# apples 3d average pooling over a volumetirc input tensor\n",
    "# commonly used in tasks like medical imaging\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(64, dtype=torch.float32).reshape(1,1,4,4,4)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:', input_tensor)\n",
    "\n",
    "#parameters for the avg_pool2d operation\n",
    "kernel_size = (2,2,2) #size of the pooling window\n",
    "stride = (2,2,2) #stride of the pooling window \n",
    "padding = 0 #padding added to the input\n",
    "ceil_mode = False  #use floor instead of ceil to compute output size\n",
    "count_include_pad = True #include zero-padding in averaging calculation\n",
    "divisor_override = None  #use the default divisor (pooling region size)\n",
    "\n",
    "#apply 3d average pooling\n",
    "output = F.avg_pool3d(\n",
    "    input = input_tensor,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    ceil_mode=ceil_mode,\n",
    "    count_include_pad=count_include_pad,\n",
    "    divisor_override=divisor_override\n",
    ")\n",
    "\n",
    "#print the output tensor\n",
    "print('\\n OUtput Tensor Shape:', output.shape)\n",
    "print('Output Tensor:', output)\n",
    "\n",
    "# For each pooling window, the average of the values inside the window is computed.\n",
    "# The output shape depends on the input shape, kernel size, stride, and padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 4, 4, 4])\n",
      "Input Tensor:\n",
      " tensor([[[[[ 0.,  1.,  2.,  3.],\n",
      "           [ 4.,  5.,  6.,  7.],\n",
      "           [ 8.,  9., 10., 11.],\n",
      "           [12., 13., 14., 15.]],\n",
      "\n",
      "          [[16., 17., 18., 19.],\n",
      "           [20., 21., 22., 23.],\n",
      "           [24., 25., 26., 27.],\n",
      "           [28., 29., 30., 31.]],\n",
      "\n",
      "          [[32., 33., 34., 35.],\n",
      "           [36., 37., 38., 39.],\n",
      "           [40., 41., 42., 43.],\n",
      "           [44., 45., 46., 47.]],\n",
      "\n",
      "          [[48., 49., 50., 51.],\n",
      "           [52., 53., 54., 55.],\n",
      "           [56., 57., 58., 59.],\n",
      "           [60., 61., 62., 63.]]]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 2, 2, 2])\n",
      "Output Tensor:\n",
      " tensor([[[[[10.5000, 12.5000],\n",
      "           [18.5000, 20.5000]],\n",
      "\n",
      "          [[42.5000, 44.5000],\n",
      "           [50.5000, 52.5000]]]]])\n"
     ]
    }
   ],
   "source": [
    "#torch.nn.functional.max_pool1d\n",
    "\n",
    "# apples 1d max pooling over an input signal, which is commonly used in task like time series analysis or audio processing\n",
    "# max pooling extracts the maximum value from each sliding window\n",
    "# reducint the dimensionality of the input while preserving important features\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[1,3,5,6,9,2,4]], dtype=torch.float32)\n",
    "print('Input Tensor Shape:',input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 0\n",
    "dilation = 1\n",
    "ceil_mode = False \n",
    "return_indices = True \n",
    "\n",
    "#apply 1d max pooling\n",
    "output, indices = F.max_pool1d(\n",
    "    input=input_tensor,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    ceil_model=ceil_mode,\n",
    "    return_indices=return_indices\n",
    ")\n",
    "\n",
    "#print the output tensor and indices\n",
    "print('\\nOutput Tensor Shape:',output.shape)\n",
    "print('Output Tensor (Max Values):\\n',output)\n",
    "print('Indices of Max Values:\\n',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 6, 6])\n",
      "Input Tensor: tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27., 28., 29.],\n",
      "          [30., 31., 32., 33., 34., 35.]]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 3, 3])\n",
      "Output Tensor (Max values):\n",
      " tensor([[[[ 7.,  9., 11.],\n",
      "          [19., 21., 23.],\n",
      "          [31., 33., 35.]]]])\n",
      "Indices of Max Values:\n",
      " tensor([[[[ 7,  9, 11],\n",
      "          [19, 21, 23],\n",
      "          [31, 33, 35]]]])\n"
     ]
    }
   ],
   "source": [
    "#torch.nn.functional.max_pool2d\n",
    "\n",
    "# fuction apples 2d max pooling over an input tensor which is commonly used in CNN for downsampling feature mapes while preserving the most significant features\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.arange(36,dtype=torch.float32).reshape(1,1,6,6)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:', input_tensor)\n",
    "\n",
    "#parameters for the max_pool2d operation\n",
    "kernel_size = (2,2)\n",
    "stride = (2,2)\n",
    "padding = 0\n",
    "dilation = 1\n",
    "ceil_mode = False \n",
    "return_indices = True\n",
    "\n",
    "output, indices = F.max_pool2d(\n",
    "    input=input_tensor,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    ceil_mode=ceil_mode,\n",
    "    return_indices=return_indices\n",
    ")\n",
    "\n",
    "print('\\nOutput Tensor Shape:',output.shape)\n",
    "print('Output Tensor (Max values):\\n',output)\n",
    "print('Indices of Max Values:\\n',indices)\n",
    "\n",
    "# for each pooling window, the maxium value is extracted \n",
    "# if return_indices=True, the indices of the maximum values are also returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 4, 4, 4])\n",
      "Input Tensor:\n",
      " tensor([[[[[ 0.,  1.,  2.,  3.],\n",
      "           [ 4.,  5.,  6.,  7.],\n",
      "           [ 8.,  9., 10., 11.],\n",
      "           [12., 13., 14., 15.]],\n",
      "\n",
      "          [[16., 17., 18., 19.],\n",
      "           [20., 21., 22., 23.],\n",
      "           [24., 25., 26., 27.],\n",
      "           [28., 29., 30., 31.]],\n",
      "\n",
      "          [[32., 33., 34., 35.],\n",
      "           [36., 37., 38., 39.],\n",
      "           [40., 41., 42., 43.],\n",
      "           [44., 45., 46., 47.]],\n",
      "\n",
      "          [[48., 49., 50., 51.],\n",
      "           [52., 53., 54., 55.],\n",
      "           [56., 57., 58., 59.],\n",
      "           [60., 61., 62., 63.]]]]])\n",
      "\n",
      " Output Tensor Shape: torch.Size([1, 1, 2, 2, 2])\n",
      "Output Tensor (Max Values):\n",
      " tensor([[[[[21., 23.],\n",
      "           [29., 31.]],\n",
      "\n",
      "          [[53., 55.],\n",
      "           [61., 63.]]]]])\n",
      "Indices of Max Values:\n",
      " tensor([[[[[21, 23],\n",
      "           [29, 31]],\n",
      "\n",
      "          [[53, 55],\n",
      "           [61, 63]]]]])\n"
     ]
    }
   ],
   "source": [
    "#max_pool3d\n",
    "\n",
    "# function applies 3d max pooling over a volumetric input tensor which iscommonly used in tasks like medical imaging or video processing. max pooling extracts the maxium value from each sliding window reducing the dimensionality of the input while preserving important faetures\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "#define a 5D input tensor\n",
    "    # batch_size = 1, channels = 1, depth = 4, height = 4, width = 4\n",
    "input_tensor = torch.arange(64, dtype=torch.float32).reshape(1,1,4,4,4)\n",
    "print('Input Tensor Shape:',input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "#parameters for the max_pool3d operation\n",
    "kernel_size = (2,2,2) #size of the poolingwindow\n",
    "stride = (2,2,2) #stride of the pooling window\n",
    "padding= 0\n",
    "dilation = 1\n",
    "ceil_mode = False \n",
    "return_indices = True\n",
    "\n",
    "output , indices = F.max_pool3d(\n",
    "    input=input_tensor,\n",
    "    kernel_size = kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    dilation=dilation,\n",
    "    ceil_mode=ceil_mode,\n",
    "    return_indices=return_indices\n",
    ")\n",
    "\n",
    "print('\\n Output Tensor Shape:', output.shape)\n",
    "print('Output Tensor (Max Values):\\n',output)\n",
    "print('Indices of Max Values:\\n',indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 7])\n",
      "Input Tensor:\n",
      " tensor([[1., 3., 5., 7., 9., 2., 4.]])\n",
      "\n",
      "Pooled Output Shape: torch.Size([1, 3])\n",
      "Pooled Output:\n",
      " tensor([[5., 9., 9.]])\n",
      "Indices of Max Values:\n",
      " tensor([[2, 4, 4]])\n",
      "\n",
      " Unnpooled Output Shape: torch.Size([1, 7])\n",
      "Unpooled Output:\n",
      " tensor([[0., 0., 5., 0., 9., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#max_unpool1d,\n",
    "#perform the inverse operation of max_pool1d, reconstructing the orignal input tensor from the pooled output and indices returned by max_pool1d\n",
    "# requre outou from a previos=is max_pool1d operation , including both the pooled vauues and the indices of the maximum values\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define a 3d input tensor\n",
    "input_tensor = torch.tensor([[1,3,5,7,9,2,4]], dtype=torch.float32)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n',input_tensor)\n",
    "\n",
    "#parameters for max_pool1d\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 0\n",
    "return_indices = True \n",
    "\n",
    "pooled_output, indices = F.max_pool1d(\n",
    "    input=input_tensor,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    return_indices=return_indices\n",
    ")\n",
    "\n",
    "print('\\nPooled Output Shape:',pooled_output.shape)\n",
    "print('Pooled Output:\\n',pooled_output)\n",
    "print('Indices of Max Values:\\n', indices)\n",
    "\n",
    "output_size = (1,1,7)\n",
    "\n",
    "unpooled_output = F.max_unpool1d(\n",
    "    input = pooled_output,\n",
    "    indices = indices,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "\n",
    "print('\\n Unnpooled Output Shape:', unpooled_output.shape)\n",
    "print('Unpooled Output:\\n', unpooled_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 6, 6])\n",
      "Input Tensor:\n",
      " tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27., 28., 29.],\n",
      "          [30., 31., 32., 33., 34., 35.]]]])\n",
      "\n",
      "Pooled Output Shape: torch.Size([1, 1, 3, 3])\n",
      "Pooled Output:\n",
      " tensor([[[[ 7.,  9., 11.],\n",
      "          [19., 21., 23.],\n",
      "          [31., 33., 35.]]]])\n",
      "Indices of Max Values:\n",
      " tensor([[[[ 7,  9, 11],\n",
      "          [19, 21, 23],\n",
      "          [31, 33, 35]]]])\n",
      "\n",
      "Unpooled Output Shape: torch.Size([1, 1, 6, 6])\n",
      "Unpooled Output:\n",
      " tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  7.,  0.,  9.,  0., 11.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0., 19.,  0., 21.,  0., 23.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0., 31.,  0., 33.,  0., 35.]]]])\n"
     ]
    }
   ],
   "source": [
    "#max_unpool2d\n",
    "\n",
    "# this function performs the inver operation of max_pool2d, reconstructing the original input tensor form the pooled output and the indices returned by max_pool2d\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.arange(36, dtype=torch.float32).reshape(1,1,6,6)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "kernel_size = (2,2)\n",
    "stride = (2,2)\n",
    "padding = 0\n",
    "return_indices = True \n",
    "\n",
    "pooled_output, indices = F.max_pool2d(\n",
    "    input = input_tensor,\n",
    "    kernel_size = kernel_size,\n",
    "    stride =stride,\n",
    "    padding = padding,\n",
    "    return_indices=return_indices\n",
    ")\n",
    "\n",
    "print('\\nPooled Output Shape:', pooled_output.shape)\n",
    "print('Pooled Output:\\n',pooled_output)\n",
    "print('Indices of Max Values:\\n',indices)\n",
    "\n",
    "output_size = (1,1,6,6)\n",
    "\n",
    "#apply 2d max unpooling\n",
    "unpooled_output = F.max_unpool2d(\n",
    "    input = pooled_output,\n",
    "    indices = indices,\n",
    "    kernel_size = kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "print('\\nUnpooled Output Shape:', unpooled_output.shape)\n",
    "print('Unpooled Output:\\n', unpooled_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 4, 4, 4])\n",
      "Input Tensor:\n",
      " tensor([[[[[ 0.,  1.,  2.,  3.],\n",
      "           [ 4.,  5.,  6.,  7.],\n",
      "           [ 8.,  9., 10., 11.],\n",
      "           [12., 13., 14., 15.]],\n",
      "\n",
      "          [[16., 17., 18., 19.],\n",
      "           [20., 21., 22., 23.],\n",
      "           [24., 25., 26., 27.],\n",
      "           [28., 29., 30., 31.]],\n",
      "\n",
      "          [[32., 33., 34., 35.],\n",
      "           [36., 37., 38., 39.],\n",
      "           [40., 41., 42., 43.],\n",
      "           [44., 45., 46., 47.]],\n",
      "\n",
      "          [[48., 49., 50., 51.],\n",
      "           [52., 53., 54., 55.],\n",
      "           [56., 57., 58., 59.],\n",
      "           [60., 61., 62., 63.]]]]])\n",
      "\n",
      "Pooled Output SHape: torch.Size([1, 1, 2, 2, 2])\n",
      "Pooled Output:\n",
      " tensor([[[[[21., 23.],\n",
      "           [29., 31.]],\n",
      "\n",
      "          [[53., 55.],\n",
      "           [61., 63.]]]]])\n",
      "Indices of Max Values:\n",
      " tensor([[[[[21, 23],\n",
      "           [29, 31]],\n",
      "\n",
      "          [[53, 55],\n",
      "           [61, 63]]]]])\n",
      "\n",
      "Unpooled Output Shape: torch.Size([1, 1, 4, 4, 4])\n",
      "Unpooled Output:\n",
      " tensor([[[[[ 0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.]],\n",
      "\n",
      "          [[ 0.,  0.,  0.,  0.],\n",
      "           [ 0., 21.,  0., 23.],\n",
      "           [ 0.,  0.,  0.,  0.],\n",
      "           [ 0., 29.,  0., 31.]],\n",
      "\n",
      "          [[ 0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0.,  0.]],\n",
      "\n",
      "          [[ 0.,  0.,  0.,  0.],\n",
      "           [ 0., 53.,  0., 55.],\n",
      "           [ 0.,  0.,  0.,  0.],\n",
      "           [ 0., 61.,  0., 63.]]]]])\n"
     ]
    }
   ],
   "source": [
    "#max_unpool3d \n",
    "\n",
    "# inverse operation of max_pool3d, reconstructing input tensor form the pooled output and indics retuned by max_pool3d\n",
    "# function requires the outpu of a prevois max_pool3d, including both the pooled values and indices of the maximum values.\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.arange(64, dtype=torch.float32).reshape(1,1,4,4,4)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "kernel_size = (2,2,2)\n",
    "stride = (2,2,2)\n",
    "padding = 0\n",
    "return_indices = True \n",
    "\n",
    "pooled_output, indices = F.max_pool3d(\n",
    "    input = input_tensor,\n",
    "    kernel_size = kernel_size,\n",
    "    stride =stride,\n",
    "    padding =padding,\n",
    "    return_indices=return_indices\n",
    ")\n",
    "\n",
    "\n",
    "print('\\nPooled Output SHape:', pooled_output.shape)\n",
    "print('Pooled Output:\\n', pooled_output)\n",
    "print('Indices of Max Values:\\n', indices)\n",
    "\n",
    "output_size = (1,1,4,4,4)\n",
    "\n",
    "unpooled_output = F.max_unpool3d(\n",
    "    input=pooled_output,\n",
    "    indices=indices,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "print('\\nUnpooled Output Shape:', unpooled_output.shape)\n",
    "print('Unpooled Output:\\n', unpooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 1, 6])\n",
      "input tensor:\n",
      " tensor([[[1., 2., 3., 4., 5., 6.]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 2])\n",
      "Output Tensor\"\n",
      " tensor([[[3.7417, 7.0711]]])\n"
     ]
    }
   ],
   "source": [
    "#1p_pool1d \n",
    "\n",
    "#function applies 1d power-average pooling operation over an input signal.\n",
    "# the 'power-average' is computed as the Lp-norm of the values in each pooling window.\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[[1.0,2.0,3.0,4.0,5.0,6.0]]],dtype=torch.float32)\n",
    "print('Input tensor shape:', input_tensor.shape)\n",
    "print('input tensor:\\n', input_tensor)\n",
    "\n",
    "norm_type = 2.0\n",
    "kernel_size = 3\n",
    "stride = 2 \n",
    "ceil_mode = False \n",
    "\n",
    "output = F.lp_pool1d(\n",
    "    input = input_tensor,\n",
    "    norm_type=norm_type,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    ceil_mode=ceil_mode\n",
    ")\n",
    "\n",
    "\n",
    "print('\\nOutput Tensor Shape:', output.shape)\n",
    "print('Output Tensor\"\\n', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape torch.Size([1, 1, 6, 6])\n",
      "input tensor tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27., 28., 29.],\n",
      "          [30., 31., 32., 33., 34., 35.]]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 3, 3])\n",
      "Output Tensor:\n",
      " tensor([[[[ 9.2736, 12.5698, 16.1864],\n",
      "          [31.5911, 35.5246, 39.4715],\n",
      "          [55.3353, 59.3127, 63.2930]]]])\n"
     ]
    }
   ],
   "source": [
    "#1l_pool2d \n",
    "# function applies a 2d power-average pooling opration over an input tensor \n",
    "# the power-average is computed as the Lp norm of the value sin each pooling wind\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(36, dtype=torch.float32).reshape(1,1,6,6)\n",
    "print('input tensor shape',input_tensor.shape)\n",
    "print('input tensor', input_tensor)\n",
    "\n",
    "norm_type = 2.0 \n",
    "kernel_size = (2,2)\n",
    "stride = (2,2)\n",
    "ceil_mode = False \n",
    "\n",
    "output = F.lp_pool2d(\n",
    "    input = input_tensor,\n",
    "    norm_type = norm_type,\n",
    "    kernel_size = kernel_size,\n",
    "    stride = stride, \n",
    "    ceil_mode = ceil_mode\n",
    ")\n",
    "\n",
    "print('\\nOutput Tensor Shape:', output.shape)\n",
    "print('Output Tensor:\\n', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape torch.Size([1, 1, 4, 4, 4])\n",
      "Input Tensor: tensor([[[[[ 0.,  1.,  2.,  3.],\n",
      "           [ 4.,  5.,  6.,  7.],\n",
      "           [ 8.,  9., 10., 11.],\n",
      "           [12., 13., 14., 15.]],\n",
      "\n",
      "          [[16., 17., 18., 19.],\n",
      "           [20., 21., 22., 23.],\n",
      "           [24., 25., 26., 27.],\n",
      "           [28., 29., 30., 31.]],\n",
      "\n",
      "          [[32., 33., 34., 35.],\n",
      "           [36., 37., 38., 39.],\n",
      "           [40., 41., 42., 43.],\n",
      "           [44., 45., 46., 47.]],\n",
      "\n",
      "          [[48., 49., 50., 51.],\n",
      "           [52., 53., 54., 55.],\n",
      "           [56., 57., 58., 59.],\n",
      "           [60., 61., 62., 63.]]]]])\n",
      "\n",
      " Output Tensor Shape: torch.Size([1, 1, 2, 2, 2])\n",
      "Output Tensor: tensor([[[[[ 37.7889,  42.3792],\n",
      "           [ 57.3062,  62.5140]],\n",
      "\n",
      "          [[122.4582, 128.0156],\n",
      "           [144.7342, 150.3197]]]]])\n"
     ]
    }
   ],
   "source": [
    "#lp_pool3d\n",
    "# function applies a 3d power-average pooling operation over an input tensor.\n",
    "# the power-average is computed as the Lp-norm of the values in each pooling window.\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(64, dtype=torch.float32).reshape(1,1,4,4,4)\n",
    "print('Input Tensor Shape',input_tensor.shape)\n",
    "print('Input Tensor:', input_tensor)\n",
    "\n",
    "norm_type = 2.0 \n",
    "kernel_size = (2,2,2)\n",
    "stride = (2,2,2)\n",
    "ceil_mode = False \n",
    "\n",
    "output = F.lp_pool3d(\n",
    "    input = input_tensor,\n",
    "    norm_type = norm_type,\n",
    "    kernel_size = kernel_size,\n",
    "    ceil_mode = ceil_mode\n",
    ")\n",
    "\n",
    "print('\\n Output Tensor Shape:', output.shape)\n",
    "print('Output Tensor:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 7])\n",
      "Input Tensor:\n",
      " tensor([[[1., 2., 3., 4., 5., 6., 7.]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 3])\n",
      "Output Tensor (Max Values):\n",
      " tensor([[[3., 5., 7.]]])\n",
      "Indices of Max Values:\n",
      " tensor([[[2, 4, 6]]])\n"
     ]
    }
   ],
   "source": [
    "# adaptive_max_pool1d \n",
    "# function applies 1d adaptive max pooling which dynamically adjusts the size of the pooling window to produce a fixed output size. It is particularly useful when you need a specific output size regardless of the input size \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[[1.0,2.0,3.0,4.0,5.0,6.0,7.0]]], dtype=torch.float32)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "output_size = 3\n",
    "return_indices = True\n",
    "\n",
    "output, indices = F.adaptive_max_pool1d(\n",
    "    input = input_tensor,\n",
    "    output_size = output_size, \n",
    "    return_indices = return_indices\n",
    ")\n",
    "\n",
    "print('\\nOutput Tensor Shape:', output.shape)\n",
    "print('Output Tensor (Max Values):\\n', output)\n",
    "print('Indices of Max Values:\\n', indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape torch.Size([1, 1, 6, 6])\n",
      "Input Tensor tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27., 28., 29.],\n",
      "          [30., 31., 32., 33., 34., 35.]]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 3, 3])\n",
      "Output Tensor (Max Values) tensor([[[[ 7.,  9., 11.],\n",
      "          [19., 21., 23.],\n",
      "          [31., 33., 35.]]]])\n",
      "Indices of Max Values:\n",
      " tensor([[[[ 7,  9, 11],\n",
      "          [19, 21, 23],\n",
      "          [31, 33, 35]]]])\n"
     ]
    }
   ],
   "source": [
    "#max_pool2d \n",
    "# applies 2d adaptive max pooling, dynamically adjusts the size of the pooling windows to produce \n",
    "# a fixed output size , it is particularly useful when you need a specific output size regardless of the input size\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(36, dtype=torch.float32).reshape(1,1,6,6)\n",
    "print('Input Tensor Shape', input_tensor.shape)\n",
    "print('Input Tensor', input_tensor)\n",
    "\n",
    "output_size = (3,3)\n",
    "return_indices = True \n",
    "\n",
    "output, indices = F.adaptive_max_pool2d(\n",
    "    input = input_tensor,\n",
    "    output_size = output_size,\n",
    "    return_indices = return_indices\n",
    ")\n",
    "\n",
    "print('\\nOutput Tensor Shape:',output.shape)\n",
    "print('Output Tensor (Max Values)',output)\n",
    "print('Indices of Max Values:\\n',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 4, 4, 4])\n",
      "Input Tensor:\n",
      " tensor([[[[[ 0.,  1.,  2.,  3.],\n",
      "           [ 4.,  5.,  6.,  7.],\n",
      "           [ 8.,  9., 10., 11.],\n",
      "           [12., 13., 14., 15.]],\n",
      "\n",
      "          [[16., 17., 18., 19.],\n",
      "           [20., 21., 22., 23.],\n",
      "           [24., 25., 26., 27.],\n",
      "           [28., 29., 30., 31.]],\n",
      "\n",
      "          [[32., 33., 34., 35.],\n",
      "           [36., 37., 38., 39.],\n",
      "           [40., 41., 42., 43.],\n",
      "           [44., 45., 46., 47.]],\n",
      "\n",
      "          [[48., 49., 50., 51.],\n",
      "           [52., 53., 54., 55.],\n",
      "           [56., 57., 58., 59.],\n",
      "           [60., 61., 62., 63.]]]]])\n",
      "\n",
      " Output Tensor Shape: torch.Size([1, 1, 2, 2, 2])\n",
      "Output Tensor (Max Values):\n",
      " tensor([[[[[21., 23.],\n",
      "           [29., 31.]],\n",
      "\n",
      "          [[53., 55.],\n",
      "           [61., 63.]]]]])\n",
      "Indices of Max Values:\n",
      " tensor([[[[[21, 23],\n",
      "           [29, 31]],\n",
      "\n",
      "          [[53, 55],\n",
      "           [61, 63]]]]])\n"
     ]
    }
   ],
   "source": [
    "#adaptive_max_pool3d \n",
    "# function applies 3d adaptive max pooling, \n",
    "# dynamically adjust the size of the pooling windows to produce a fixed output size. \n",
    "# it particularly useful when yo need a specific output size regardless the input size \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(64,dtype=torch.float32).reshape(1,1,4,4,4)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "output_size = (2,2,2)\n",
    "return_indices = True \n",
    "\n",
    "output, indices = F.adaptive_max_pool3d(\n",
    "    input = input_tensor,\n",
    "    output_size = output_size,\n",
    "    return_indices = return_indices \n",
    ")\n",
    "print('\\n Output Tensor Shape:', output.shape)\n",
    "print('Output Tensor (Max Values):\\n', output)\n",
    "print('Indices of Max Values:\\n', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 6, 6])\n",
      "Input Tensor:\n",
      " tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27., 28., 29.],\n",
      "          [30., 31., 32., 33., 34., 35.]]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 3, 3])\n",
      "Output\n"
     ]
    }
   ],
   "source": [
    "#adapative_avg_pool2d\n",
    "\n",
    "#function applies 2d adaptive average pooling, dynamically adjust the size of the pooling windows to produce fixed outut size.\n",
    "# it sizes it computes the average value wihtint each region mkaing it useful for downsampling while spatiol \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(36, dtype=torch.float32).reshape(1,1,6,6)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "output_size = (3,3)\n",
    "\n",
    "output = F.adaptive_avg_pool2d(\n",
    "    input = input_tensor,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "print('\\nOutput Tensor Shape:', output.shape)\n",
    "print('Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape torch.Size([1, 1, 4, 4, 4])\n",
      "Input Tensor: tensor([[[[[ 0.,  1.,  2.,  3.],\n",
      "           [ 4.,  5.,  6.,  7.],\n",
      "           [ 8.,  9., 10., 11.],\n",
      "           [12., 13., 14., 15.]],\n",
      "\n",
      "          [[16., 17., 18., 19.],\n",
      "           [20., 21., 22., 23.],\n",
      "           [24., 25., 26., 27.],\n",
      "           [28., 29., 30., 31.]],\n",
      "\n",
      "          [[32., 33., 34., 35.],\n",
      "           [36., 37., 38., 39.],\n",
      "           [40., 41., 42., 43.],\n",
      "           [44., 45., 46., 47.]],\n",
      "\n",
      "          [[48., 49., 50., 51.],\n",
      "           [52., 53., 54., 55.],\n",
      "           [56., 57., 58., 59.],\n",
      "           [60., 61., 62., 63.]]]]])\n",
      "\n",
      " Output Tensor Shape: torch.Size([1, 1, 2, 2, 2])\n",
      "\n",
      " Output Tensor (Average Values): tensor([[[[[10.5000, 12.5000],\n",
      "           [18.5000, 20.5000]],\n",
      "\n",
      "          [[42.5000, 44.5000],\n",
      "           [50.5000, 52.5000]]]]])\n"
     ]
    }
   ],
   "source": [
    "# adaptive_avg_pool3d \n",
    "\n",
    "# function applies 3d adaptive average pooling, which dynmically adjusts the size of the poooling windows to produce a dixed output size. \n",
    "# it computes the average value within each region , making it useful for downsampling volunteric data while preserving spatioal information.\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(64, dtype=torch.float32).reshape(1,1,4,4,4)\n",
    "print('Input Tensor Shape', input_tensor.shape)\n",
    "print('Input Tensor:', input_tensor)\n",
    "\n",
    "output_size = (2,2,2)\n",
    "\n",
    "output = F.adaptive_avg_pool3d(\n",
    "    input = input_tensor,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "print('\\n Output Tensor Shape:', output.shape)\n",
    "print('\\n Output Tensor (Average Values):', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 8, 8])\n",
      "Input Tensor tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
      "          [16., 17., 18., 19., 20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27., 28., 29., 30., 31.],\n",
      "          [32., 33., 34., 35., 36., 37., 38., 39.],\n",
      "          [40., 41., 42., 43., 44., 45., 46., 47.],\n",
      "          [48., 49., 50., 51., 52., 53., 54., 55.],\n",
      "          [56., 57., 58., 59., 60., 61., 62., 63.]]]])\n",
      "\n",
      "Output Tensor Shape: torch.Size([1, 1, 4, 4])\n",
      "Output Tensor (Max Values):\n",
      " tensor([[[[18., 19., 21., 23.],\n",
      "          [34., 35., 37., 39.],\n",
      "          [42., 43., 45., 47.],\n",
      "          [58., 59., 61., 63.]]]])\n",
      "Indices of Max Values\n",
      " tensor([[[[18, 19, 21, 23],\n",
      "          [34, 35, 37, 39],\n",
      "          [42, 43, 45, 47],\n",
      "          [58, 59, 61, 63]]]])\n"
     ]
    }
   ],
   "source": [
    "#max_pool2d \n",
    "\n",
    "# function applies 2d fractional max pooling whcih downsamples the input by selecting a random subset of pooling regions \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(64, dtype=torch.float32).reshape(1,1,8,8)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor', input_tensor)\n",
    "\n",
    "kernel_size = 3 \n",
    "output_size = (4,4)\n",
    "return_indices = True \n",
    "\n",
    "output , indices = F.fractional_max_pool2d(\n",
    "    input = input_tensor,\n",
    "    kernel_size = kernel_size, \n",
    "    output_size = output_size, \n",
    "    return_indices = return_indices\n",
    ")\n",
    "\n",
    "print('\\nOutput Tensor Shape:', output.shape)\n",
    "print('Output Tensor (Max Values):\\n', output)\n",
    "print('Indices of Max Values\\n', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 1, 8, 8, 8])\n",
      "Input Tensor:\n",
      " tensor([[[[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],\n",
      "           [  8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.],\n",
      "           [ 16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.],\n",
      "           [ 24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.],\n",
      "           [ 32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n",
      "           [ 40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.],\n",
      "           [ 48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.],\n",
      "           [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.]],\n",
      "\n",
      "          [[ 64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.],\n",
      "           [ 72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.],\n",
      "           [ 80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.],\n",
      "           [ 88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.],\n",
      "           [ 96.,  97.,  98.,  99., 100., 101., 102., 103.],\n",
      "           [104., 105., 106., 107., 108., 109., 110., 111.],\n",
      "           [112., 113., 114., 115., 116., 117., 118., 119.],\n",
      "           [120., 121., 122., 123., 124., 125., 126., 127.]],\n",
      "\n",
      "          [[128., 129., 130., 131., 132., 133., 134., 135.],\n",
      "           [136., 137., 138., 139., 140., 141., 142., 143.],\n",
      "           [144., 145., 146., 147., 148., 149., 150., 151.],\n",
      "           [152., 153., 154., 155., 156., 157., 158., 159.],\n",
      "           [160., 161., 162., 163., 164., 165., 166., 167.],\n",
      "           [168., 169., 170., 171., 172., 173., 174., 175.],\n",
      "           [176., 177., 178., 179., 180., 181., 182., 183.],\n",
      "           [184., 185., 186., 187., 188., 189., 190., 191.]],\n",
      "\n",
      "          [[192., 193., 194., 195., 196., 197., 198., 199.],\n",
      "           [200., 201., 202., 203., 204., 205., 206., 207.],\n",
      "           [208., 209., 210., 211., 212., 213., 214., 215.],\n",
      "           [216., 217., 218., 219., 220., 221., 222., 223.],\n",
      "           [224., 225., 226., 227., 228., 229., 230., 231.],\n",
      "           [232., 233., 234., 235., 236., 237., 238., 239.],\n",
      "           [240., 241., 242., 243., 244., 245., 246., 247.],\n",
      "           [248., 249., 250., 251., 252., 253., 254., 255.]],\n",
      "\n",
      "          [[256., 257., 258., 259., 260., 261., 262., 263.],\n",
      "           [264., 265., 266., 267., 268., 269., 270., 271.],\n",
      "           [272., 273., 274., 275., 276., 277., 278., 279.],\n",
      "           [280., 281., 282., 283., 284., 285., 286., 287.],\n",
      "           [288., 289., 290., 291., 292., 293., 294., 295.],\n",
      "           [296., 297., 298., 299., 300., 301., 302., 303.],\n",
      "           [304., 305., 306., 307., 308., 309., 310., 311.],\n",
      "           [312., 313., 314., 315., 316., 317., 318., 319.]],\n",
      "\n",
      "          [[320., 321., 322., 323., 324., 325., 326., 327.],\n",
      "           [328., 329., 330., 331., 332., 333., 334., 335.],\n",
      "           [336., 337., 338., 339., 340., 341., 342., 343.],\n",
      "           [344., 345., 346., 347., 348., 349., 350., 351.],\n",
      "           [352., 353., 354., 355., 356., 357., 358., 359.],\n",
      "           [360., 361., 362., 363., 364., 365., 366., 367.],\n",
      "           [368., 369., 370., 371., 372., 373., 374., 375.],\n",
      "           [376., 377., 378., 379., 380., 381., 382., 383.]],\n",
      "\n",
      "          [[384., 385., 386., 387., 388., 389., 390., 391.],\n",
      "           [392., 393., 394., 395., 396., 397., 398., 399.],\n",
      "           [400., 401., 402., 403., 404., 405., 406., 407.],\n",
      "           [408., 409., 410., 411., 412., 413., 414., 415.],\n",
      "           [416., 417., 418., 419., 420., 421., 422., 423.],\n",
      "           [424., 425., 426., 427., 428., 429., 430., 431.],\n",
      "           [432., 433., 434., 435., 436., 437., 438., 439.],\n",
      "           [440., 441., 442., 443., 444., 445., 446., 447.]],\n",
      "\n",
      "          [[448., 449., 450., 451., 452., 453., 454., 455.],\n",
      "           [456., 457., 458., 459., 460., 461., 462., 463.],\n",
      "           [464., 465., 466., 467., 468., 469., 470., 471.],\n",
      "           [472., 473., 474., 475., 476., 477., 478., 479.],\n",
      "           [480., 481., 482., 483., 484., 485., 486., 487.],\n",
      "           [488., 489., 490., 491., 492., 493., 494., 495.],\n",
      "           [496., 497., 498., 499., 500., 501., 502., 503.],\n",
      "           [504., 505., 506., 507., 508., 509., 510., 511.]]]]])\n",
      "\n",
      " Output Tensor Shape: torch.Size([1, 1, 4, 4, 4])\n",
      "Output Tensor (Max Values)\n",
      " tensor([[[[[146., 148., 149., 151.],\n",
      "           [162., 164., 165., 167.],\n",
      "           [178., 180., 181., 183.],\n",
      "           [186., 188., 189., 191.]],\n",
      "\n",
      "          [[274., 276., 277., 279.],\n",
      "           [290., 292., 293., 295.],\n",
      "           [306., 308., 309., 311.],\n",
      "           [314., 316., 317., 319.]],\n",
      "\n",
      "          [[338., 340., 341., 343.],\n",
      "           [354., 356., 357., 359.],\n",
      "           [370., 372., 373., 375.],\n",
      "           [378., 380., 381., 383.]],\n",
      "\n",
      "          [[466., 468., 469., 471.],\n",
      "           [482., 484., 485., 487.],\n",
      "           [498., 500., 501., 503.],\n",
      "           [506., 508., 509., 511.]]]]])\n",
      "Indices of Max Values:\n",
      " tensor([[[[[146, 148, 149, 151],\n",
      "           [162, 164, 165, 167],\n",
      "           [178, 180, 181, 183],\n",
      "           [186, 188, 189, 191]],\n",
      "\n",
      "          [[274, 276, 277, 279],\n",
      "           [290, 292, 293, 295],\n",
      "           [306, 308, 309, 311],\n",
      "           [314, 316, 317, 319]],\n",
      "\n",
      "          [[338, 340, 341, 343],\n",
      "           [354, 356, 357, 359],\n",
      "           [370, 372, 373, 375],\n",
      "           [378, 380, 381, 383]],\n",
      "\n",
      "          [[466, 468, 469, 471],\n",
      "           [482, 484, 485, 487],\n",
      "           [498, 500, 501, 503],\n",
      "           [506, 508, 509, 511]]]]])\n"
     ]
    }
   ],
   "source": [
    "#max_pool3d\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.arange(512, dtype=torch.float32).reshape(1,1,8,8,8)\n",
    "print('Input Tensor Shape:', input_tensor.shape)\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "kernel_size = 3\n",
    "output_size = (4,4,4)\n",
    "return_indices = True \n",
    "\n",
    "output, indices = F.fractional_max_pool3d(\n",
    "    input = input_tensor,\n",
    "    kernel_size = kernel_size,\n",
    "    output_size = output_size,\n",
    "    return_indices = return_indices \n",
    ")\n",
    "\n",
    "print('\\n Output Tensor Shape:', output.shape)\n",
    "print('Output Tensor (Max Values)\\n', output)\n",
    "print('Indices of Max Values:\\n',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([2, 4, 8, 64])\n",
      "Attention Output:\n",
      " tensor([[[[0.2649, 0.4282, 0.9530,  ..., 0.6633, 0.2831, 0.6720],\n",
      "          [0.6438, 0.5081, 0.8578,  ..., 0.7205, 0.1772, 0.8333],\n",
      "          [0.4574, 0.6657, 0.6882,  ..., 0.7768, 0.4474, 0.9025],\n",
      "          ...,\n",
      "          [0.5056, 0.4177, 0.4656,  ..., 0.5272, 0.3969, 0.6563],\n",
      "          [0.7042, 0.5366, 0.5024,  ..., 0.4751, 0.5923, 0.7479],\n",
      "          [0.6596, 0.5777, 0.4738,  ..., 0.5001, 0.5570, 0.6923]],\n",
      "\n",
      "         [[1.0778, 0.5960, 0.7437,  ..., 0.3600, 0.9837, 0.7020],\n",
      "          [0.2299, 0.4553, 0.0534,  ..., 0.1464, 0.0436, 0.2850],\n",
      "          [0.5496, 0.5532, 0.4163,  ..., 0.3606, 0.6794, 0.5596],\n",
      "          ...,\n",
      "          [0.2894, 0.4028, 0.4763,  ..., 0.4075, 0.4890, 0.4804],\n",
      "          [0.4760, 0.3388, 0.4006,  ..., 0.3396, 0.4056, 0.4513],\n",
      "          [0.5099, 0.3814, 0.4191,  ..., 0.4421, 0.4335, 0.4065]],\n",
      "\n",
      "         [[0.6909, 0.0219, 0.7772,  ..., 0.5716, 0.5740, 1.0567],\n",
      "          [0.7070, 0.0311, 0.3964,  ..., 0.7204, 0.7064, 0.8329],\n",
      "          [0.5841, 0.1819, 0.3495,  ..., 0.4853, 0.5845, 0.3286],\n",
      "          ...,\n",
      "          [0.6483, 0.5259, 0.5270,  ..., 0.4163, 0.4494, 0.6635],\n",
      "          [0.5759, 0.5115, 0.3806,  ..., 0.4622, 0.5303, 0.6807],\n",
      "          [0.6461, 0.5318, 0.4868,  ..., 0.5000, 0.5789, 0.6559]],\n",
      "\n",
      "         [[0.6130, 0.0377, 0.4680,  ..., 0.7096, 0.1969, 1.0291],\n",
      "          [0.4858, 0.4231, 0.4369,  ..., 0.7130, 0.2155, 0.7661],\n",
      "          [0.6034, 0.5334, 0.6215,  ..., 0.6672, 0.4870, 0.5346],\n",
      "          ...,\n",
      "          [0.5016, 0.5491, 0.5697,  ..., 0.5675, 0.3829, 0.3632],\n",
      "          [0.5755, 0.5236, 0.5615,  ..., 0.5750, 0.4554, 0.4145],\n",
      "          [0.6154, 0.4570, 0.5261,  ..., 0.6205, 0.4423, 0.4149]]],\n",
      "\n",
      "\n",
      "        [[[0.5310, 0.4730, 0.6066,  ..., 0.1733, 0.4148, 0.5526],\n",
      "          [0.6543, 0.2662, 0.4927,  ..., 0.5743, 0.3732, 0.6521],\n",
      "          [0.4115, 0.3379, 0.2442,  ..., 0.1639, 0.1485, 0.2314],\n",
      "          ...,\n",
      "          [0.6462, 0.5434, 0.5283,  ..., 0.5317, 0.4502, 0.6578],\n",
      "          [0.5856, 0.5088, 0.5868,  ..., 0.6075, 0.5313, 0.6160],\n",
      "          [0.4504, 0.3739, 0.4554,  ..., 0.5084, 0.4240, 0.4795]],\n",
      "\n",
      "         [[0.7671, 0.1680, 0.6348,  ..., 0.2594, 0.9513, 0.7670],\n",
      "          [0.3537, 0.0775, 0.2927,  ..., 0.1196, 0.4387, 0.3537],\n",
      "          [0.4922, 0.6415, 0.2833,  ..., 0.6269, 0.7351, 0.5440],\n",
      "          ...,\n",
      "          [0.3386, 0.4560, 0.1611,  ..., 0.3707, 0.4663, 0.5553],\n",
      "          [0.4175, 0.5094, 0.1601,  ..., 0.4758, 0.5386, 0.5204],\n",
      "          [0.5606, 0.5485, 0.2265,  ..., 0.6311, 0.5614, 0.6052]],\n",
      "\n",
      "         [[0.3046, 0.5868, 0.4254,  ..., 0.0029, 0.1491, 0.2746],\n",
      "          [0.5239, 0.5895, 0.4209,  ..., 0.3486, 0.2887, 0.4326],\n",
      "          [0.5678, 0.4703, 0.4695,  ..., 0.5981, 0.4191, 0.4182],\n",
      "          ...,\n",
      "          [0.5769, 0.4846, 0.6188,  ..., 0.5272, 0.4619, 0.3971],\n",
      "          [0.6471, 0.5341, 0.6413,  ..., 0.5551, 0.5172, 0.4013],\n",
      "          [0.6492, 0.5020, 0.6309,  ..., 0.5998, 0.5116, 0.3868]],\n",
      "\n",
      "         [[0.8681, 0.1657, 0.6956,  ..., 0.4707, 0.4004, 0.1433],\n",
      "          [0.8666, 0.2994, 0.6419,  ..., 0.6602, 0.5586, 0.1114],\n",
      "          [0.5926, 0.4772, 0.6850,  ..., 0.4857, 0.5277, 0.3455],\n",
      "          ...,\n",
      "          [0.3928, 0.4030, 0.5963,  ..., 0.4352, 0.4882, 0.4551],\n",
      "          [0.3763, 0.4242, 0.5039,  ..., 0.3077, 0.4393, 0.2887],\n",
      "          [0.3755, 0.3994, 0.5723,  ..., 0.4845, 0.5835, 0.4898]]]])\n"
     ]
    }
   ],
   "source": [
    "#scaled_dot_product_attention\n",
    "\n",
    "# computes scaled dot-product attention \n",
    "# which core operation transformer-based models \n",
    "# it supports features like attention masking, dropout and causal attension.\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define query, key, and value tensors\n",
    "batch_size = 2\n",
    "num_heads = 4\n",
    "seq_len_query = 8\n",
    "seq_len_key_value = 10\n",
    "embedding_dim = 64\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, seq_len_query, embedding_dim)  # Shape: (N, Hq, L, E)\n",
    "key = torch.rand(batch_size, num_heads, seq_len_key_value, embedding_dim)  # Shape: (N, H, S, E)\n",
    "value = torch.rand(batch_size, num_heads, seq_len_key_value, embedding_dim)  # Shape: (N, H, S, Ev)\n",
    "\n",
    "# Optional attention mask (float mask that adds bias to attention scores)\n",
    "attn_mask = torch.tril(torch.ones(seq_len_query, seq_len_key_value))  # Lower triangular mask\n",
    "attn_mask = attn_mask.masked_fill(attn_mask == 0, float(\"-inf\")).masked_fill(attn_mask == 1, 0.0)\n",
    "\n",
    "# Parameters for scaled_dot_product_attention\n",
    "dropout_p = 0.1  # Dropout probability\n",
    "is_causal = False  # Whether to apply causal masking\n",
    "scale = None  # Scaling factor (default is 1 / sqrt(embedding_dim))\n",
    "enable_gqa = False  # Enable Grouped Query Attention (experimental)\n",
    "\n",
    "# Apply scaled dot-product attention\n",
    "attention_output = F.scaled_dot_product_attention(\n",
    "    query=query,\n",
    "    key=key,\n",
    "    value=value,\n",
    "    attn_mask=attn_mask,\n",
    "    dropout_p=dropout_p,\n",
    "    is_causal=is_causal,\n",
    "    scale=scale,\n",
    "    enable_gqa=enable_gqa\n",
    ")\n",
    "\n",
    "# Print the output tensor\n",
    "print(\"Attention Output Shape:\", attention_output.shape)\n",
    "print(\"Attention Output:\\n\", attention_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[-1.,  2., -3.],\n",
      "        [ 4., -5.,  6.]])\n",
      "\n",
      "Output Tensor:\n",
      " tensor([[0., 2., 0.],\n",
      "        [4., 0., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#threshold \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-1.0 , 2.0, -3.0], [4.0, -5.0, 6.0]])\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "threshold = 0.0 \n",
    "value = 0.0 \n",
    "inplace = False \n",
    "\n",
    "output_tensor = F.threshold(input = input_tensor, threshold=threshold, value =value, inplace=inplace)\n",
    "\n",
    "print('\\nOutput Tensor:\\n',output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input Tensor:\n",
      " tensor([[-1.,  2., -3.],\n",
      "        [ 4., -5.,  6.]])\n",
      "\n",
      " Modified Input Tensor (In-Place):\n",
      " tensor([[0., 2., 0.],\n",
      "        [4., 0., 6.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-1.0, 2.0, -3.0],[4.0, -5.0, 6.0]])\n",
    "print('Original Input Tensor:\\n', input_tensor)\n",
    "\n",
    "threshold = 0.0\n",
    "value = 0.0 \n",
    "\n",
    "F.threshold_(input=input_tensor, threshold=threshold, value=value)\n",
    "\n",
    "print('\\n Modified Input Tensor (In-Place):\\n', input_tensor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[-1.,  2., -3.],\n",
      "        [ 4., -5.,  6.]])\n",
      "\n",
      "Output Tensor(ReLU):\n",
      " tensor([[0., 2., 0.],\n",
      "        [4., 0., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#relu \n",
    "\n",
    "# this function applies the rectified linear unit (ReLU)\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-1.0, 2.0, -3.0],[4.0, -5.0, 6.0]])\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "output_tensor = F.relu(input=input_tensor, inplace = False)\n",
    "print('\\nOutput Tensor(ReLU):\\n', output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input Tensor:\n",
      " tensor([[-1.,  2., -3.],\n",
      "        [ 4., -5.,  6.]])\n",
      "\n",
      "Modified Input Tensor (ReLU In-Place):\n",
      " tensor([[0., 2., 0.],\n",
      "        [4., 0., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#relu_\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "input_tensor = torch.tensor([[-1.0, 2.0, -3.0],[4.0, -5.0, 6.0]])\n",
    "print('Original Input Tensor:\\n', input_tensor)\n",
    "F.relu_(input=input_tensor)\n",
    "print('\\nModified Input Tensor (ReLU In-Place):\\n', input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[-2.0000, -0.5000,  0.0000],\n",
      "        [ 0.5000,  1.5000,  3.0000]])\n",
      "\n",
      "Output Tensor (HardTanh):\n",
      " tensor([[-1.0000, -0.5000,  0.0000],\n",
      "        [ 0.5000,  1.0000,  1.0000]])\n",
      "Oirignal Activations:\n",
      " tensor([[-5., -2.,  0.],\n",
      "        [ 1.,  3., 10.]])\n",
      "\n",
      "Clamped Activations:\n",
      " tensor([[-1., -1.,  0.],\n",
      "        [ 1.,  1.,  1.]])\n",
      "Original Gradietns:\n",
      " tensor([[-10.,  -5.,   0.],\n",
      "        [ -1.,   5.,  20.]])\n",
      "\n",
      "Clupped Gradients:\n",
      " tensor([[-1., -1.,  0.],\n",
      "        [-1.,  1.,  1.]])\n",
      "Original Activations:\n",
      " tensor([[-5., -2.,  0.],\n",
      "        [ 1.,  3., 10.]])\n",
      "\n",
      "Modified Activations (In-Place):\n",
      " tensor([[-1., -1.,  0.],\n",
      "        [ 1.,  1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nValue Clamping Restricts values to a specific range\\nGradient Stability Limits gradients to a safe range to prevent exploding or vanishing gradients\\nIn-Place HardTanh Modifies the input tensor directly to save memory\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hardtanh\n",
    "# applies hardtanh function element-wise\n",
    "# hardtanh is an activation function used for neural networks, it is cheaper and more computatinally efficient version of the tanh activation\n",
    "# hardtanh function clamps all values in the tensor to a specified range [min_val, max_val] \n",
    "# value beelow min_val are set to min_val and values above max-val are set to max_val\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-2.0,-0.5,0.0],[0.5,1.5,3.0]])\n",
    "print('Input Tensor:\\n', input_tensor)\n",
    "\n",
    "min_val = -1.0 \n",
    "max_val = 1.0\n",
    "inplace = False \n",
    "\n",
    "output_tensor = F.hardtanh(input=input_tensor, min_val=min_val,max_val=max_val,inplace=inplace)\n",
    "\n",
    "print('\\nOutput Tensor (HardTanh):\\n', output_tensor)\n",
    "\n",
    "\n",
    "##value clamping\n",
    "#clamping values to a specific range is useful in scenarios where you want to restrict the output or intermediate results of computations to a predefined range\n",
    "# help prevent numerical instability or ensure that outputs meet certain constraints\n",
    "\n",
    "#simulated activations from a layer\n",
    "activations = torch.tensor([[-5.0,-2.0,0.0],[1.0,3.0,10.0]])\n",
    "\n",
    "#clamp activations to the range\n",
    "clamped_activations = F.hardtanh(activations, min_val=-1.0, max_val=1.0)\n",
    "print('Oirignal Activations:\\n', activations)\n",
    "print('\\nClamped Activations:\\n', clamped_activations)\n",
    "\n",
    "#Gradient Stability\n",
    "# gradients can someines explode (becomde very large) or vanis become very small.applying a clapming operation like hartanh can help stabilize gradeints by restricint their magnitde.\n",
    "\n",
    "\n",
    "gradients = torch.tensor([[-10.0, -5.0, 0.0],[-1.0, 5.0, 20.0]])\n",
    "\n",
    "clipped_gradients = torch.clamp(gradients, min=-1.0, max=1.0)\n",
    "\n",
    "print('Original Gradietns:\\n',gradients)\n",
    "print('\\nClupped Gradients:\\n',clipped_gradients)\n",
    "\n",
    "#in-place hardtanh\n",
    "activations = torch.tensor([[-5.0, -2.0, 0.0],[1.0, 3.0, 10.0]])\n",
    "\n",
    "print('Original Activations:\\n', activations)\n",
    "F.hardtanh_(activations, min_val=-1.0, max_val=1.0)\n",
    "print('\\nModified Activations (In-Place):\\n',activations)\n",
    "\n",
    "\"\"\"\n",
    "Value Clamping Restricts values to a specific range\n",
    "Gradient Stability Limits gradients to a safe range to prevent exploding or vanishing gradients\n",
    "In-Place HardTanh Modifies the input tensor directly to save memory\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[-4., -2.,  0.],\n",
      "        [ 1.,  3.,  5.]])\n",
      "\n",
      "Output Tensor (HardSwish):\n",
      " tensor([[-0.0000, -0.3333,  0.0000],\n",
      "        [ 0.6667,  3.0000,  5.0000]])\n"
     ]
    }
   ],
   "source": [
    "#hardswish , applies hardswish activation function element-wise to the input tensor.\n",
    "# hwardswish is a computationally efficient approximation of the swish activation function\n",
    "# coonly used in lightweight nuerak netrks like mobilenetv3\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-4.0, -2.0, 0.0],[1.0,3.0,5.0]])\n",
    "\n",
    "print('input tensor:\\n', input_tensor)\n",
    "\n",
    "output_tensor = F.hardswish(input=input_tensor, inplace=False)\n",
    "print('\\nOutput Tensor (HardSwish):\\n',output_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor Shape: torch.Size([1, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#here are hardswihc with MobileNetV3Block\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class MobileNetV3Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor=6, kernel_size=3, stride=1):\n",
    "        super(MobileNetV3Block, self).__init__()\n",
    "\n",
    "        #expandions phase\n",
    "        hidden_dim = int(in_channels * expansion_factor)\n",
    "        self.use_residual = (stride == 1 and in_channels == out_channels)\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        if expansion_factor != 1:\n",
    "            layers.append(nn.Conv2d(in_channels, hidden_dim,kernel_size=1,bias=False))\n",
    "            layers.append(nn.BatchNorm2d(hidden_dim))\n",
    "            layers.append(nn.Hardswish())\n",
    "\n",
    "        layers.append(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1, bias =False ))\n",
    "        layers.append(nn.BatchNorm2d(hidden_dim))\n",
    "        layers.append(nn.Hardswish())\n",
    "\n",
    "        #pointwise convolution (projection)\n",
    "        layers.append(nn.Conv2d(hidden_dim, out_channels,kernel_size=1, bias =False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        if self.use_residual:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "input_tensor = torch.randn(1,16,32,32)\n",
    "mobilenet_block = MobileNetV3Block(in_channels=16, out_channels=16, expansion_factor=6, kernel_size=3, stride=1)\n",
    "output_tensor = mobilenet_block(input_tensor)\n",
    "print('Output Tensor Shape:', output_tensor.shape)\n",
    "\n",
    "\n",
    "# MobileNetV3 Block Structure :\n",
    "# The block follows the structure of a typical MobileNetV3 bottleneck block:\n",
    "# Expansion Phase : A pointwise convolution increases the number of channels by a factor (expansion_factor), followed by BatchNorm and HardSwish activation.\n",
    "# Depthwise Convolution : A depthwise convolution applies spatial filtering, followed by BatchNorm and HardSwish activation.\n",
    "# Projection Phase : Another pointwise convolution reduces the number of channels back to out_channels, followed by BatchNorm.\n",
    "# HardSwish Activation :\n",
    "# HardSwish is applied after both the expansion and depthwise convolution phases to introduce non-linearity efficiently.\n",
    "# Residual Connection :\n",
    "# If the input and output dimensions match (in_channels == out_channels and stride == 1), a residual connection is added to improve gradient flow.\n",
    "# Input Tensor :\n",
    "# A dummy input tensor of shape (1, 16, 32, 32) is passed through the block.\n",
    "# Output Tensor :\n",
    "# The output tensor has the same shape as the input tensor due to the residual connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[-2.,  3.,  8.],\n",
      "        [ 5., -1.,  6.]])\n",
      "\n",
      "Output Tensor (Relu6):\n",
      " tensor([[0., 3., 6.],\n",
      "        [5., 0., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#relu6 activattion function element-wise to the input tensor.\n",
    "#relu6 clamps all values in the tensor to the range [0,6] specifically\n",
    "# * values less than 0 and are set to 0 \n",
    "# * values greater than 6 are set to 6\n",
    "# values within range [0,6] remained unchanged\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.tensor([[-2.0,3.0,8.0],[5.0,-1.0,6.0]])\n",
    "print('Input Tensor:\\n',input_tensor)\n",
    "\n",
    "#apply relu6 \n",
    "output_tensor = F.relu6(input=input_tensor, inplace=False)\n",
    "print('\\nOutput Tensor (Relu6):\\n', output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[-2., -1.,  0.],\n",
      "        [ 1.,  2.,  3.]])\n",
      "\n",
      "Output Tensur (ELU):\n",
      " tensor([[-0.8647, -0.6321,  0.0000],\n",
      "        [ 1.0000,  2.0000,  3.0000]])\n"
     ]
    }
   ],
   "source": [
    "#elu \n",
    "# applies exponential linear unit (elu) \n",
    "# elu activation function element-wise to the input tensor\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-2.0, -1.0, 0.0], [1.0, 2.0, 3.0]])\n",
    "print('input tensor:\\n', input_tensor)\n",
    "\n",
    "alpha = 1.0\n",
    "inplace = False \n",
    "\n",
    "output_tensor = F.elu(input=input_tensor, alpha=alpha, inplace=inplace)\n",
    "\n",
    "print('\\nOutput Tensur (ELU):\\n',output_tensor)\n",
    "\n",
    "# F.elu_(input=input_tensor, alpha=alpha) #apply elu activation in-place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[-2., -1.,  0.],\n",
      "        [ 1.,  2.,  3.]])\n",
      "output tensor (selu):\n",
      " tensor([[-1.5202, -1.1113,  0.0000],\n",
      "        [ 1.0507,  2.1014,  3.1521]])\n"
     ]
    }
   ],
   "source": [
    "#selu\n",
    "\n",
    "# this function applies the Scaled Explonential Linear Unit (SELU) activation function element-wise to the input tensor.\n",
    "# SLUE is a self normalizing activation function that helps maintain mean and variance stability during training, making it particularly useful in deep nueral networks\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-2.0, -1.0, 0.0], [1.0, 2.0, 3.0]])\n",
    "print('input tensor:\\n', input_tensor)\n",
    "\n",
    "inplace = False\n",
    "\n",
    "output_tensor = F.selu(input=input_tensor, inplace=inplace)\n",
    "print('output tensor (selu):\\n', output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[-2., -1.,  0.],\n",
      "        [ 1.,  2.,  3.]])\n",
      "\n",
      "output tensor (celu):\n",
      " tensor([[-0.8647, -0.6321,  0.0000],\n",
      "        [ 1.0000,  2.0000,  3.0000]])\n"
     ]
    }
   ],
   "source": [
    "#celu \n",
    "# this function applies the continously differentiable exponential linear unit (celu) activation function element-wise to the input tensor.\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "input_tensor = torch.tensor([[-2.0, -1.0, 0.0], [1.0, 2.0, 3.0]])\n",
    "print('input tensor:\\n', input_tensor)\n",
    "\n",
    "alpha = 1.0\n",
    "inplace = False \n",
    "\n",
    "output_tensor = F.celu(input= input_tensor, alpha=alpha, inplace=inplace)\n",
    "print('\\noutput tensor (celu):\\n', output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[-2., -1.,  0.],\n",
      "        [ 1.,  2.,  3.]])\n",
      "output tensor (leaky ReLU):\n",
      " tensor([[-0.0200, -0.0100,  0.0000],\n",
      "        [ 1.0000,  2.0000,  3.0000]])\n"
     ]
    }
   ],
   "source": [
    "#leaky-relu\n",
    "# function applies the leaky rectified linear unit activation function element-wise to the input tensor\n",
    "# leaky relu is a variant of relu that allows a small, non-zero gradient for negetive inputs, controlled by the negative_slope parameter\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-2.0, -1.0, 0.0],[1.0, 2.0, 3.0]])\n",
    "print('input tensor:\\n',input_tensor)\n",
    "\n",
    "negative_slope = 0.01 \n",
    "inplace = False \n",
    "\n",
    "output_tensor = F.leaky_relu(input=input_tensor, negative_slope=negative_slope, inplace=inplace)\n",
    "\n",
    "print('output tensor (leaky ReLU):\\n',output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[-2., -1.,  0.],\n",
      "        [ 1.,  2.,  3.]])\n",
      "\n",
      "Ouput Tensor (PReLU)\n",
      " tensor([[-0.5000, -0.2500,  0.0000],\n",
      "        [ 1.0000,  2.0000,  3.0000]])\n"
     ]
    }
   ],
   "source": [
    "#parametric rectified linear unit (PReLU)\n",
    "# activation function element-wise to the input tensor.\n",
    "# unlike leaky relu, PReLUL allows the slope for negative inputs (weight) to be a learnable parameter which can adapt during training \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-2.0,-1.0,0.0],[1.0,2.0,3.0]])\n",
    "print('input tensor:\\n', input_tensor)\n",
    "\n",
    "#define the learnable weight parameter for PReLU\n",
    "#weight can be a scaler or a 1-D tensor matching the number of input chanels\n",
    "weight = torch.tensor(0.25)\n",
    "output_tensor = F.prelu(input=input_tensor, weight=weight)\n",
    "\n",
    "#print the output tensor\n",
    "print('\\nOuput Tensor (PReLU)\\n', output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor\n",
      " tensor([[-2., -2.,  0.],\n",
      "        [ 1.,  2.,  3.]])\n",
      "\n",
      "output tensor (RReLU):\n",
      " tensor([[-0.6247, -0.3263,  0.0000],\n",
      "        [ 1.0000,  2.0000,  3.0000]])\n"
     ]
    }
   ],
   "source": [
    "#rrelu\n",
    "# randomized leaky rectified linear unit (rrelu) activtaion function element-wise to the input tensor\n",
    "# RReLU introduces randomness in the slope for negative inputs during training , which can help with generalization\n",
    "# a is a randmly sampled from a unifrom distribution [lower, upper] during training, during inference a is fixed o the average of [lower, upper]\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[-2.0,-2.0,0.0],[1.0,2.0,3.0]])\n",
    "print('input tensor\\n',input_tensor)\n",
    "\n",
    "lower = 1./8\n",
    "upper = 1./3\n",
    "training = True \n",
    "inplace = False \n",
    "\n",
    "output_tensor = F.rrelu(input=input_tensor, lower=lower, upper=upper, training=training, inplace=inplace)\n",
    "print('\\noutput tensor (RReLU):\\n', output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[ 0.7922, -1.1832, -0.2774, -1.9988,  0.0403,  0.0956, -1.7005, -0.1549],\n",
      "        [ 1.4411, -0.7484, -0.8632,  1.1396,  1.0872, -0.6204, -0.6409,  0.7428]])\n",
      "\\output tensor (glu):\n",
      " tensor([[ 0.4041, -0.6199, -0.0428, -0.9222],\n",
      "        [ 1.0777, -0.2617, -0.2978,  0.7722]])\n"
     ]
    }
   ],
   "source": [
    "#glu (gated linear unit)\n",
    "# splits the input tensor along a specified dimension into two parts (a and b) applies the sigmoid function to b and computs the lement-wisr product a and a(b)\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.randn(2,8)\n",
    "print('input tensor:\\n', input_tensor)\n",
    "\n",
    "dim = -1\n",
    "output_tensor = F.glu(input=input_tensor, dim=dim)\n",
    "\n",
    "print('\\output tensor (glu):\\n', output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([-2., -1.,  0.,  1.,  2.])\n",
      "output tensor (exact GELU):\n",
      " tensor([-0.0455, -0.1587,  0.0000,  0.8413,  1.9545])\n",
      "\n",
      "output tensor (approximate GELU):\n",
      " tensor([-0.0454, -0.1588,  0.0000,  0.8412,  1.9546])\n"
     ]
    }
   ],
   "source": [
    "#gelu \n",
    "#(gaussian error linear unit)\n",
    "#activation function element-wise to the input tensor\n",
    "# GELU is smooth non-linear activation function that is widely used in modern deep learning models\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([-2.0,-1.0,0.0,1.0,2.0])\n",
    "print('input tensor:\\n',input_tensor)\n",
    "\n",
    "output_exact = F.gelu(input=input_tensor, approximate='none')\n",
    "output_approximate = F.gelu(input=input_tensor, approximate='tanh')\n",
    "\n",
    "print('output tensor (exact GELU):\\n',output_exact)\n",
    "print('\\noutput tensor (approximate GELU):\\n',output_approximate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([-2., -1.,  0.,  1.,  2.])\n",
      "\n",
      "output tensor (LogSigmoid):\n",
      " tensor([-2.1269, -1.3133, -0.6931, -0.3133, -0.1269])\n"
     ]
    }
   ],
   "source": [
    "#logsigmoid\n",
    "# function applies the logsigmoid activation functoin element-wise to the input tensor.\n",
    "# the logsigmoid function is coonly used in probabilitstic models and binary classfication tasks , particularly when working with log probabilits\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tenspr = torch.tensor([-2.0,-1.0,0.00,1.0,2.0])\n",
    "print('input tensor:\\n', input_tensor)\n",
    "output_tensor = F.logsigmoid(input=input_tensor)\n",
    "print('\\noutput tensor (LogSigmoid):\\n', output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  0.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hardshrink \n",
    "# this function applies the Hard Shrinkage function function element-wise to the input tensor .\n",
    "# where lamd in the function is threshold parameter that determines the range\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([-2.0,-1.0,-0.4,0.0,0.4,1.0,2.0])\n",
    "lambd= 0.5\n",
    "output_tensor = F.hardshrink(input=input_tensor, lambd=lambd)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0360, -0.2384, -0.0201,  0.0000,  0.0201,  0.2384,  1.0360])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tanshrink\n",
    "# applies the element-wise tanhshrink function\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([-2.0,-1.0,-0.4,0.0,0.4,1.0,2.0])\n",
    "output_tensor = F.tanhshrink(input=input_tensor)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6667, -0.5000,  0.0000,  0.5000,  0.6667])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softsign \n",
    "# function applies the softsign activation function element-wise to the input tensor\n",
    "# where x is the absolute value of x . softsign is a smooth , non-linear activation function that squashes the input into range (-1,1)\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "input_tensor = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_tensor = F.softsign(input=input_tensor)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1269, 0.3133, 0.6931, 1.3133, 2.1269])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softplus\n",
    "# applies element-wise \n",
    "# for numerical stability the implementaion reverts to the linear function when input X B > threshold. \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "input_tensor = torch.tensor([-2.0,-1.0,0.0,1.0,2.0])\n",
    "output_tensor = F.softplus(input=input_tensor)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6652, 0.2447, 0.0900],\n",
       "        [0.6652, 0.2447, 0.0900]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softmin \n",
    "# this function applies the softmin function essentially the negative of the softmax function\n",
    "# softmin function normalizes the input tensor along a specified dimension such that the output values sum to 1 \n",
    "# it is often used in scenarios where you want to compute probabilites or weights inversely propotional to the input values\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "dim = 1\n",
    "output_tensor = F.softmin(input=input_tensor,dim=dim)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softmax \n",
    "#apples softmax activation function\n",
    "# softmax normalizes the input tensor along a specified dimension such that the output values lie in the range [0,1] and sum to 1\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[2.0,1.0,0.5],[3.0,2.0,1.0]])\n",
    "dim = 1\n",
    "output_tensor = F.softmax(input=input_tensor,dim=dim)\n",
    "#verify each slice sums to 1\n",
    "output_tensor.sum(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5000, -0.5000, -0.0000,  0.0000,  0.0000,  0.5000,  1.5000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#soft shrinkage (softshrink)\n",
    "# function element-wise to the input tensor.\n",
    "# lambd , threshold parameter that determines the range values to be shrunk to zero\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.tensor([-2.0,-1.0,-0.4,0.0,0.4,1.0,2.0])\n",
    "lambd = 0.5 \n",
    "output_tensor = F.softshrink(input=input_tensor, lambd=lambd)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Soft Samples (Probability Distribution):\n",
      " tensor([[2.6780e-01, 4.7145e-01, 2.6075e-01],\n",
      "        [1.1713e-01, 3.7062e-02, 8.4581e-01],\n",
      "        [3.5186e-02, 1.2928e-01, 8.3553e-01],\n",
      "        [2.0046e-04, 2.7033e-02, 9.7277e-01],\n",
      "        [1.2448e-01, 3.4024e-01, 5.3528e-01]])\n",
      "\n",
      "hard samples (one-hode encoded):\n",
      " tensor([[0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]])\n",
      "\n",
      "sum along dimension (should be 1 for hard samples):\n",
      " tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "#gumble_softmax\n",
    "# gumble_softmax function samples form the Gumble-Softmax distribution\n",
    "# which is a continous relaxation of the categorical distribution.\n",
    "# its commonly used in scenarios where you need differentiable sampling\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "logits = torch.randn(5,3)\n",
    "tau = 1.0\n",
    "hard = False \n",
    "dim = -1\n",
    "soft_samples = F.gumbel_softmax(logits=logits, tau=tau, hard=hard, dim=dim) #apply gumble-softmax with soft sampling\n",
    "print('\\nSoft Samples (Probability Distribution):\\n',soft_samples)\n",
    "hard_samples=F.gumbel_softmax(logits=logits,tau=tau,hard=True,dim=dim)\n",
    "print('\\nhard samples (one-hode encoded):\\n', hard_samples)\n",
    "\n",
    "#verify that hard samples are one-hot encoded\n",
    "print('\\nsum along dimension (should be 1 for hard samples):\\n',hard_samples.sum(dim=dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output tensor (log probabilities):\n",
      " tensor([[-0.4644, -1.4644, -1.9644],\n",
      "        [-0.4076, -1.4076, -2.4076]])\n",
      "\n",
      "exponentiated output (softmax probabilites):\n",
      " tensor([[0.6285, 0.2312, 0.1402],\n",
      "        [0.6652, 0.2447, 0.0900]])\n",
      "\n",
      " sum along dimension 1 (should be approximately 1):\n",
      " tensor([1.0000, 1.0000])\n",
      "Log Probabilities (log_softmax):\n",
      " tensor([[-0.4644, -1.4644, -1.9644],\n",
      "        [-0.4076, -1.4076, -2.4076]])\n",
      "\n",
      "NLL Loss:\n",
      " tensor(1.4360)\n"
     ]
    }
   ],
   "source": [
    "#log_softmax \n",
    "# function applies the logirithm of the softmax function in a single step,\n",
    "# which is both computationally efficient and numericalls stable.\n",
    "#commonly used in combinetion with loss functions like torch.nnn.NLLLoss (negative log likelihood loss) for classication tasks\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[2.0,1.0,0.5],[3.0,2.0,1.0]])\n",
    "dim =1\n",
    "output_tensor = F.log_softmax(input=input_tensor,dim=dim)\n",
    "print('\\noutput tensor (log probabilities):\\n',output_tensor)\n",
    "softmax_probabilities = torch.exp(output_tensor)\n",
    "print('\\nexponentiated output (softmax probabilites):\\n', softmax_probabilities)\n",
    "\n",
    "#verify that each slice sums to 1\n",
    "print('\\n sum along dimension 1 (should be approximately 1):\\n',softmax_probabilities.sum(dim=dim))\n",
    "\n",
    "#NLLoss\n",
    "logits = torch.tensor([[2.0, 1.0, 0.5], [3.0, 2.0, 1.0]])  # Shape: (batch_size=2, num_classes=3)\n",
    "labels = torch.tensor([0,2])\n",
    "dim = 1\n",
    "log_probs = F.log_softmax(logits,dim=dim)\n",
    "print('Log Probabilities (log_softmax):\\n',log_probs)\n",
    "loss = F.nll_loss(log_probs, labels)\n",
    "print('\\nNLL Loss:\\n',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9640, -0.7616,  0.0000,  0.7616,  0.9640])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tanh \n",
    "\n",
    "# this function applies the hyperbolic tangent (tanh) activation function \n",
    "\n",
    "# the output of the tanh function lies in the range (-1,1), making it a popular choiec for activation function in neural networks\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([-2.0,-1.0,0.0,1.0,2.0])\n",
    "output_tensor = F.tanh(input=input_tensor)\n",
    "output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1192, 0.2689, 0.5000, 0.7311, 0.8808])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigmoid\n",
    "\n",
    "# applies sigmoid activation finction elemet-wise to the input tensor\n",
    "# outut sigmoid lies in the range (0,1) making it a popular choice for tasks where probabilities or normalized values are required\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "input_tensor = torch.tensor([-2.0,-1.0,0.0,1.0,2.0])\n",
    "output_tensor = F.sigmoid(input=input_tensor)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.3333, 0.5000, 0.6667, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hardsigmoid \n",
    "# hardsigmoid function is a piecewise linear approximation of the sigmoid unction\n",
    "# hardsigmoid functions is computationally efficent because it avoids exepnsive exponential calculations and is often used in lightweight neural networks\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([-4.0, -3.0, -1.0, 0.0, 1.0, 3.0, 4.0])\n",
    "output_tensor = F.hardsigmoid(input=input_tensor, inplace = False)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2384, -0.2689,  0.0000,  0.7311,  1.7616])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#silu \n",
    "# function applies the sigmoid linear unit activation function elemetn-wise to the input tensor \n",
    "# the silu function is also known as swish activation funcion\n",
    "# the silu function comvines the input x with its signmoid activation making it a smooth non-monotonic actiation function that has beedn shown to ikmprove performance in deep nerual neural networks\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([-2.0,-1.0,0.0,1.0,2.0])\n",
    "output_tensor = F.silu(input=input_tensor, inplace =False)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2525, -0.3034,  0.0000,  0.8651,  1.9440])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mish\n",
    "\n",
    "# mish(x) = x .tanh(Softplux(x))\n",
    "# softplus(x) = log(1 + exp(x))\n",
    "# tanh(x) is the hyperbolic tangent function \n",
    "\n",
    "# mish is smooth , non-monotonic activation function that has been shown to improve performance in deep neural netowrks compared to other acatoin funcions like ReLU or swish \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([-2.0,-1.0,0.0,1.0,2.0])\n",
    "output_tensor = F.mish(input=input_tensor, inplace =False)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor: tensor([[[[[ 0.2939, -0.7838, -0.3633,  1.0398],\n",
      "           [-0.5306,  0.3425, -0.2094,  2.0015],\n",
      "           [-1.0057, -0.4197, -1.0050,  0.7385],\n",
      "           [ 0.5214,  0.5199, -0.3382,  0.3056]],\n",
      "\n",
      "          [[ 0.3278,  1.3689, -0.3543,  0.1538],\n",
      "           [-1.3689,  0.8954,  1.0587, -0.3418],\n",
      "           [ 0.2685, -0.8686,  0.3122,  0.3304],\n",
      "           [-0.4221, -0.3994,  0.4599, -0.1123]],\n",
      "\n",
      "          [[-0.0404, -1.1547, -0.6198, -0.3274],\n",
      "           [-0.9721,  1.5516,  2.3822, -0.1542],\n",
      "           [ 1.2160, -0.8965,  1.1996, -1.1597],\n",
      "           [-0.2350,  0.5723, -0.4735, -1.4145]],\n",
      "\n",
      "          [[ 1.4348,  1.3779, -0.8737, -1.1406],\n",
      "           [-0.1624,  0.0335,  1.5107,  1.1748],\n",
      "           [ 0.5517, -0.5191, -1.3490, -0.2447],\n",
      "           [ 1.0799, -0.1226,  0.2073, -0.0922]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5044,  0.6221, -1.0633,  1.4891],\n",
      "           [ 0.9370,  0.6921, -0.0542, -0.8889],\n",
      "           [ 0.0923,  1.7893, -0.0672, -1.0229],\n",
      "           [-0.2375,  0.3284, -0.4782,  0.1566]],\n",
      "\n",
      "          [[-0.1735, -0.6886,  0.4222,  0.4389],\n",
      "           [-1.4311,  2.0375, -0.8034,  0.3006],\n",
      "           [ 0.4617, -0.2355, -0.7188,  1.1356],\n",
      "           [-1.8689, -0.3311, -0.8973, -0.4672]],\n",
      "\n",
      "          [[-0.8389, -0.2989,  1.7340, -0.2182],\n",
      "           [ 0.1897, -0.1340,  1.5129, -0.8975],\n",
      "           [-0.0106, -1.9264, -1.0720, -1.3307],\n",
      "           [ 0.4809, -0.4788,  2.3127, -0.1344]],\n",
      "\n",
      "          [[-0.7679, -0.6670,  1.1280, -1.5813],\n",
      "           [-1.0749,  0.4662, -1.1537,  0.3110],\n",
      "           [ 0.4890, -0.9589, -0.7576,  0.1132],\n",
      "           [-0.9555,  0.0428,  0.3652, -1.1668]]],\n",
      "\n",
      "\n",
      "         [[[-0.8207, -1.6672,  0.7465,  0.3308],\n",
      "           [-0.3742, -1.5013,  2.1770,  0.0757],\n",
      "           [-0.7090, -0.7063,  0.1087, -1.6870],\n",
      "           [ 1.1183, -0.2832,  1.6354, -1.0395]],\n",
      "\n",
      "          [[ 2.0066, -0.2407,  0.0102,  0.8556],\n",
      "           [-0.1375,  0.8176,  0.3634, -0.0511],\n",
      "           [-0.4569,  0.4977,  0.6742, -1.4808],\n",
      "           [-1.6811, -1.0556, -1.7666, -0.0115]],\n",
      "\n",
      "          [[ 0.1231, -1.4384, -1.7925,  0.7063],\n",
      "           [-0.0211, -0.2236, -0.0686,  0.1891],\n",
      "           [-0.4297, -0.4116,  0.1257,  0.3521],\n",
      "           [ 0.3112, -0.0483, -0.7314,  1.9095]],\n",
      "\n",
      "          [[-0.4227,  1.6689, -0.2967,  0.1827],\n",
      "           [ 0.8247, -0.3810, -0.4782,  0.4332],\n",
      "           [ 0.9792,  0.5607,  1.0322, -1.4334],\n",
      "           [-1.0841, -1.1099, -0.5734, -1.4217]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.1129,  0.0491, -0.6194,  1.6338],\n",
      "           [ 0.6225, -1.2605,  0.5497,  2.2548],\n",
      "           [ 1.2743,  0.6705, -0.4127,  0.0207],\n",
      "           [-1.1143, -2.4911,  1.0459, -0.4675]],\n",
      "\n",
      "          [[-1.7246, -0.6383,  0.3690,  0.1840],\n",
      "           [-2.1540,  1.2918, -1.3832, -0.9646],\n",
      "           [-0.2749, -0.0156,  0.4389,  0.6187],\n",
      "           [ 1.2597,  0.4657,  1.7591, -0.6530]],\n",
      "\n",
      "          [[ 1.6076, -0.2066,  0.0995, -1.3157],\n",
      "           [ 1.4833,  0.4278,  1.4198, -0.0515],\n",
      "           [-0.4917,  0.2274, -1.3447, -0.0351],\n",
      "           [ 1.2874, -0.8298,  0.0623,  1.1294]],\n",
      "\n",
      "          [[ 0.6439, -1.0360,  0.6440,  0.5794],\n",
      "           [-0.1062,  0.8525,  0.3271, -0.8257],\n",
      "           [-2.5283,  0.1073, -0.4857, -0.0697],\n",
      "           [ 3.0516,  0.1061, -0.9943, -0.4547]]],\n",
      "\n",
      "\n",
      "         [[[-0.5314, -0.5098, -0.4536,  0.9792],\n",
      "           [-1.1877,  1.4845,  0.4460,  0.2629],\n",
      "           [ 1.1374,  0.8498, -0.0606,  1.0045],\n",
      "           [-1.2312, -0.9405, -0.4802,  0.5497]],\n",
      "\n",
      "          [[ 0.0968, -1.3228, -2.0325, -0.6122],\n",
      "           [ 2.7351,  2.2204, -0.2568,  0.0900],\n",
      "           [ 0.9920, -1.4473, -0.3843, -2.3087],\n",
      "           [-1.0941,  0.5592,  0.1129,  1.6194]],\n",
      "\n",
      "          [[-0.5781, -1.1198,  1.6308, -0.2950],\n",
      "           [-1.1857, -2.1873, -0.6340,  0.0044],\n",
      "           [ 0.5868, -0.7726,  0.1864, -0.5937],\n",
      "           [-0.3012, -0.5798, -0.6166,  0.9651]],\n",
      "\n",
      "          [[-1.0685,  0.3365,  0.1506,  0.2719],\n",
      "           [ 0.1849,  0.0252,  1.0254,  1.9663],\n",
      "           [-1.4094,  1.0115, -1.4079,  0.9346],\n",
      "           [-0.4001, -0.5567, -1.8854, -0.7720]]],\n",
      "\n",
      "\n",
      "         [[[-0.3165, -1.1498, -0.5744, -0.2253],\n",
      "           [ 1.6958, -0.5919, -0.1342,  1.1019],\n",
      "           [-1.8055,  0.1473,  0.1420, -1.0106],\n",
      "           [ 1.0829, -0.1657,  1.4005, -1.4362]],\n",
      "\n",
      "          [[ 0.9996,  0.6191, -0.5313,  0.3384],\n",
      "           [-0.8015,  0.2262,  0.2488, -0.1946],\n",
      "           [ 0.2123, -1.0924, -0.2291, -0.5421],\n",
      "           [-2.2684, -1.2347, -0.1073, -1.3328]],\n",
      "\n",
      "          [[ 0.2989,  0.8185,  0.0939,  0.2116],\n",
      "           [-0.2428, -0.6248,  0.3532,  1.7604],\n",
      "           [-2.7199, -0.6337, -0.4902, -0.8287],\n",
      "           [ 0.6333,  0.0939, -1.2737, -0.5892]],\n",
      "\n",
      "          [[ 0.7015, -0.6316, -0.2707, -0.3455],\n",
      "           [ 0.0851, -1.3145, -1.0747,  0.7634],\n",
      "           [ 0.1432,  0.2270,  0.9109,  2.0588],\n",
      "           [-0.2568,  1.2669,  0.6580,  1.2376]]]]])\n",
      "output tensor tensor([[[[[ 2.3265e-01, -8.6029e-01, -4.3387e-01,  9.8906e-01],\n",
      "           [-6.0349e-01,  2.8191e-01, -2.7783e-01,  1.9644e+00],\n",
      "           [-1.0853e+00, -4.9104e-01, -1.0846e+00,  6.8347e-01],\n",
      "           [ 4.6333e-01,  4.6185e-01, -4.0842e-01,  2.4447e-01]],\n",
      "\n",
      "          [[ 2.6700e-01,  1.3228e+00, -4.2474e-01,  9.0510e-02],\n",
      "           [-1.4537e+00,  8.4265e-01,  1.0083e+00, -4.1210e-01],\n",
      "           [ 2.0686e-01, -9.4631e-01,  2.5116e-01,  2.6961e-01],\n",
      "           [-4.9355e-01, -4.7051e-01,  4.0096e-01, -1.7933e-01]],\n",
      "\n",
      "          [[-1.0639e-01, -1.2365e+00, -6.9397e-01, -3.9742e-01],\n",
      "           [-1.0513e+00,  1.5081e+00,  2.3504e+00, -2.2184e-01],\n",
      "           [ 1.1678e+00, -9.7460e-01,  1.1512e+00, -1.2415e+00],\n",
      "           [-3.0375e-01,  5.1497e-01, -5.4563e-01, -1.4999e+00]],\n",
      "\n",
      "          [[ 1.3897e+00,  1.3320e+00, -9.5147e-01, -1.2222e+00],\n",
      "           [-2.3012e-01, -3.1466e-02,  1.4666e+00,  1.1260e+00],\n",
      "           [ 4.9409e-01, -5.9190e-01, -1.4335e+00, -3.1361e-01],\n",
      "           [ 1.0298e+00, -1.8977e-01,  1.4477e-01, -1.5895e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.1266e-01,  7.2995e-01, -9.5024e-01,  1.5943e+00],\n",
      "           [ 1.0439e+00,  7.9977e-01,  5.5709e-02, -7.7644e-01],\n",
      "           [ 2.0181e-01,  1.8936e+00,  4.2770e-02, -9.1003e-01],\n",
      "           [-1.2701e-01,  4.3715e-01, -3.6693e-01,  2.6589e-01]],\n",
      "\n",
      "          [[-6.3202e-02, -5.7677e-01,  5.3073e-01,  5.4737e-01],\n",
      "           [-1.3170e+00,  2.1411e+00, -6.9120e-01,  4.0950e-01],\n",
      "           [ 5.7010e-01, -1.2497e-01, -6.0687e-01,  1.2419e+00],\n",
      "           [-1.7534e+00, -2.2027e-01, -7.8479e-01, -3.5604e-01]],\n",
      "\n",
      "          [[-7.2655e-01, -1.8820e-01,  1.8385e+00, -1.0776e-01],\n",
      "           [ 2.9892e-01, -2.3761e-02,  1.6181e+00, -7.8498e-01],\n",
      "           [ 9.9212e-02, -1.8108e+00, -9.5893e-01, -1.2168e+00],\n",
      "           [ 5.8927e-01, -3.6757e-01,  2.4155e+00, -2.4235e-02]],\n",
      "\n",
      "          [[-6.5574e-01, -5.5519e-01,  1.2344e+00, -1.4667e+00],\n",
      "           [-9.6181e-01,  5.7461e-01, -1.0405e+00,  4.1983e-01],\n",
      "           [ 5.9734e-01, -8.4622e-01, -6.4547e-01,  2.2263e-01],\n",
      "           [-8.4284e-01,  1.5241e-01,  4.7383e-01, -1.0534e+00]]],\n",
      "\n",
      "\n",
      "         [[[-7.4933e-01, -1.6385e+00,  8.9672e-01,  4.6010e-01],\n",
      "           [-2.8036e-01, -1.4642e+00,  2.3993e+00,  1.9220e-01],\n",
      "           [-6.3207e-01, -6.2920e-01,  2.2686e-01, -1.6593e+00],\n",
      "           [ 1.2873e+00, -1.8482e-01,  1.8304e+00, -9.7919e-01]],\n",
      "\n",
      "          [[ 2.2203e+00, -1.4012e-01,  1.2339e-01,  1.0113e+00],\n",
      "           [-3.1725e-02,  9.7139e-01,  4.9438e-01,  5.9045e-02],\n",
      "           [-3.6727e-01,  6.3546e-01,  8.2083e-01, -1.4426e+00],\n",
      "           [-1.6531e+00, -9.9610e-01, -1.7428e+00,  1.0062e-01]],\n",
      "\n",
      "          [[ 2.4201e-01, -1.3982e+00, -1.7701e+00,  8.5457e-01],\n",
      "           [ 9.0468e-02, -1.2220e-01,  4.0570e-02,  3.1129e-01],\n",
      "           [-3.3869e-01, -3.1969e-01,  2.4468e-01,  4.8252e-01],\n",
      "           [ 4.3950e-01,  6.1943e-02, -6.5557e-01,  2.1183e+00]],\n",
      "\n",
      "          [[-3.3132e-01,  1.8656e+00, -1.9898e-01,  3.0457e-01],\n",
      "           [ 9.7889e-01, -2.8755e-01, -3.8963e-01,  5.6765e-01],\n",
      "           [ 1.1412e+00,  7.0162e-01,  1.1968e+00, -1.3929e+00],\n",
      "           [-1.0260e+00, -1.0531e+00, -4.8956e-01, -1.3806e+00]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.7997e-01, -1.5606e-02, -6.9361e-01,  1.5915e+00],\n",
      "           [ 5.6592e-01, -1.3438e+00,  4.9204e-01,  2.2213e+00],\n",
      "           [ 1.2269e+00,  6.1458e-01, -4.8399e-01, -4.4486e-02],\n",
      "           [-1.1955e+00, -2.5918e+00,  9.9527e-01, -5.3956e-01]],\n",
      "\n",
      "          [[-1.8144e+00, -7.1280e-01,  3.0883e-01,  1.2120e-01],\n",
      "           [-2.2499e+00,  1.2446e+00, -1.4682e+00, -1.0437e+00],\n",
      "           [-3.4419e-01, -8.1260e-02,  3.7972e-01,  5.6199e-01],\n",
      "           [ 1.2121e+00,  4.0689e-01,  1.7186e+00, -7.2764e-01]],\n",
      "\n",
      "          [[ 1.5649e+00, -2.7493e-01,  3.5497e-02, -1.3997e+00],\n",
      "           [ 1.4389e+00,  3.6840e-01,  1.3745e+00, -1.1762e-01],\n",
      "           [-5.6410e-01,  1.6518e-01, -1.4292e+00, -1.0100e-01],\n",
      "           [ 1.2401e+00, -9.0695e-01, -2.2847e-03,  1.0800e+00]],\n",
      "\n",
      "          [[ 5.8761e-01, -1.1161e+00,  5.8767e-01,  5.2221e-01],\n",
      "           [-1.7316e-01,  7.9913e-01,  2.6632e-01, -9.0285e-01],\n",
      "           [-2.6295e+00,  4.3435e-02, -5.5799e-01, -1.3610e-01],\n",
      "           [ 3.0294e+00,  4.2184e-02, -1.0738e+00, -5.2659e-01]]],\n",
      "\n",
      "\n",
      "         [[[-4.1996e-01, -3.9848e-01, -3.4246e-01,  1.0861e+00],\n",
      "           [-1.0743e+00,  1.5898e+00,  5.5445e-01,  3.7192e-01],\n",
      "           [ 1.2437e+00,  9.5696e-01,  4.9395e-02,  1.1112e+00],\n",
      "           [-1.1177e+00, -8.2790e-01, -3.6897e-01,  6.5785e-01]],\n",
      "\n",
      "          [[ 2.0626e-01, -1.2090e+00, -1.9165e+00, -5.0056e-01],\n",
      "           [ 2.8366e+00,  2.3235e+00, -1.4627e-01,  1.9953e-01],\n",
      "           [ 1.0988e+00, -1.3331e+00, -2.7333e-01, -2.1919e+00],\n",
      "           [-9.8103e-01,  6.6725e-01,  2.2231e-01,  1.7243e+00]],\n",
      "\n",
      "          [[-4.6651e-01, -1.0066e+00,  1.7357e+00, -1.8431e-01],\n",
      "           [-1.0723e+00, -2.0708e+00, -5.2226e-01,  1.1416e-01],\n",
      "           [ 6.9483e-01, -6.6049e-01,  2.9561e-01, -4.8214e-01],\n",
      "           [-1.9049e-01, -4.6824e-01, -5.0497e-01,  1.0719e+00]],\n",
      "\n",
      "          [[-9.5544e-01,  4.4526e-01,  2.5993e-01,  3.8086e-01],\n",
      "           [ 2.9417e-01,  1.3494e-01,  1.1320e+00,  2.0702e+00],\n",
      "           [-1.2953e+00,  1.1182e+00, -1.2939e+00,  1.0416e+00],\n",
      "           [-2.8909e-01, -4.4519e-01, -1.7699e+00, -6.5986e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.1973e-01, -1.0950e+00, -4.9061e-01, -1.2402e-01],\n",
      "           [ 1.8938e+00, -5.0897e-01, -2.8284e-02,  1.2701e+00],\n",
      "           [-1.7837e+00,  2.6737e-01,  2.6186e-01, -9.4884e-01],\n",
      "           [ 1.2501e+00, -6.1384e-02,  1.5837e+00, -1.3958e+00]],\n",
      "\n",
      "          [[ 1.1626e+00,  7.6294e-01, -4.4542e-01,  4.6807e-01],\n",
      "           [-7.2918e-01,  3.5027e-01,  3.7398e-01, -9.1731e-02],\n",
      "           [ 3.3569e-01, -1.0347e+00, -1.2799e-01, -4.5674e-01],\n",
      "           [-2.2700e+00, -1.1841e+00, -7.7643e-05, -1.2872e+00]],\n",
      "\n",
      "          [[ 4.2665e-01,  9.7241e-01,  2.1131e-01,  3.3494e-01],\n",
      "           [-1.4237e-01, -5.4356e-01,  4.8361e-01,  1.9617e+00],\n",
      "           [-2.7442e+00, -5.5296e-01, -4.0224e-01, -7.5770e-01],\n",
      "           [ 7.7784e-01,  2.1130e-01, -1.2251e+00, -5.0620e-01]],\n",
      "\n",
      "          [[ 8.4947e-01, -5.5073e-01, -1.7161e-01, -2.5025e-01],\n",
      "           [ 2.0202e-01, -1.2680e+00, -1.0162e+00,  9.1449e-01],\n",
      "           [ 2.6305e-01,  3.5110e-01,  1.0694e+00,  2.2752e+00],\n",
      "           [-1.5703e-01,  1.4434e+00,  8.0379e-01,  1.4125e+00]]]]])\n",
      "updated running mean:\n",
      " tensor([ 0.0065, -0.0110, -0.0107])\n",
      "updated running variance:\n",
      " tensor([0.0980, 0.1014, 0.0914])\n"
     ]
    }
   ],
   "source": [
    "#batch_norm\n",
    "# applies batchnormlization to the input tensor.\n",
    "# batch normalization normalizes the input across a batch of ata for each channel , stbilizing and accelrating training in deep neural networks\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.randn(2,3,4,4,4)# batch_size, channels, height, width\n",
    "running_mean = torch.zeros(3)\n",
    "running_var = torch.zeros(3)\n",
    "\n",
    "weight = torch.ones(3) #scale parameter gamma\n",
    "bias = torch.zeros(3) #scale parameter (beta)\n",
    "\n",
    "#apply batch normalization\n",
    "training = True #whether the model is in training mode\n",
    "momentum = 0.1 # momentum for updating running statistics\n",
    "eps = 1e-5 #small constant for numerical stability\n",
    "\n",
    "output_tensor = F.batch_norm(\n",
    "    input=input_tensor,\n",
    "    running_mean=running_mean,\n",
    "    running_var=running_var,\n",
    "    bias = bias,\n",
    "    training=training,\n",
    "    momentum=momentum,\n",
    "    eps=eps\n",
    ")\n",
    "\n",
    "print('input tensor:', input_tensor)\n",
    "print('output tensor',output_tensor)\n",
    "\n",
    "print('updated running mean:\\n', running_mean)\n",
    "print('updated running variance:\\n',running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor\n",
      " tensor([[[[-0.9950,  0.3942,  1.2846,  1.2953],\n",
      "          [ 1.1153,  0.0882, -0.0327, -0.1738],\n",
      "          [ 0.9755,  0.2276,  0.7907,  0.0203],\n",
      "          [-0.8627, -0.4963,  0.8088, -1.0085]],\n",
      "\n",
      "         [[ 0.0282,  0.3838, -0.3063, -1.9295],\n",
      "          [-1.1972, -1.6479,  0.2804, -1.7825],\n",
      "          [-0.5665,  1.4817,  0.8787, -2.2114],\n",
      "          [ 1.3869, -1.4940,  0.2617, -0.4820]],\n",
      "\n",
      "         [[-0.1168,  0.0420,  1.0844, -0.2193],\n",
      "          [ 0.1458,  0.7663,  0.4907, -2.0230],\n",
      "          [-2.4124, -0.6406,  1.4944, -1.3344],\n",
      "          [-0.9236,  0.7850,  0.2408, -0.5281]],\n",
      "\n",
      "         [[-1.1853, -1.4353, -0.9999, -0.3563],\n",
      "          [ 1.1517, -1.2574, -1.4784, -0.5392],\n",
      "          [ 1.7553,  0.5330,  0.3643, -0.0301],\n",
      "          [-0.0555,  1.2492,  1.6412,  1.9839]],\n",
      "\n",
      "         [[-1.1844, -0.6003,  0.1958, -0.0630],\n",
      "          [ 0.6549, -0.1772,  0.5408, -1.1960],\n",
      "          [-0.2786, -1.2881, -1.0027,  1.3418],\n",
      "          [-1.4563, -1.1602,  1.4336, -1.7977]],\n",
      "\n",
      "         [[-0.6822,  0.0277,  0.2082,  1.2955],\n",
      "          [ 0.4089, -0.6298,  0.1297, -0.3645],\n",
      "          [ 0.5796,  0.3038, -0.6955, -1.3708],\n",
      "          [-0.4478, -0.7956,  0.9843, -1.0546]]],\n",
      "\n",
      "\n",
      "        [[[-0.4289, -0.4008,  0.4573, -0.5782],\n",
      "          [ 0.0515, -0.5021,  0.2534,  0.0510],\n",
      "          [-0.9935,  1.6752, -0.2710,  0.4135],\n",
      "          [-0.1856, -2.0295, -1.5122,  1.5554]],\n",
      "\n",
      "         [[-0.0363, -0.5212, -0.8754, -0.0290],\n",
      "          [-1.3274, -1.0681, -0.5635, -0.8419],\n",
      "          [-0.0594, -0.4831, -0.4609,  0.0462],\n",
      "          [ 0.7525, -2.0255, -0.1731, -0.2878]],\n",
      "\n",
      "         [[-0.7892, -0.1201, -1.7738,  1.6307],\n",
      "          [-0.9034, -0.8288, -0.9299, -0.7827],\n",
      "          [-0.8793,  0.3816,  0.4733, -0.2069],\n",
      "          [ 1.6163, -1.1339,  0.8491, -1.1492]],\n",
      "\n",
      "         [[ 0.2874,  0.8142, -1.0189, -0.8913],\n",
      "          [-0.3381, -0.5086, -1.0372, -0.2433],\n",
      "          [-0.7018, -0.2785, -0.4037, -1.6032],\n",
      "          [-0.1073,  0.5733, -0.6053, -0.4629]],\n",
      "\n",
      "         [[ 1.0180, -0.3907, -0.3836,  0.1315],\n",
      "          [ 1.5216,  1.2461, -0.5994,  1.0821],\n",
      "          [ 0.0903, -1.0386,  0.3641,  0.5238],\n",
      "          [ 1.2035, -1.0547,  0.3255, -0.4827]],\n",
      "\n",
      "         [[-0.8117, -0.0254, -0.2113, -1.6629],\n",
      "          [ 0.6749, -0.5201,  0.5994,  0.5515],\n",
      "          [-1.2182,  0.3320, -1.2321,  1.2613],\n",
      "          [-1.7958,  0.4002, -0.9803, -0.0768]]]])\n",
      "\n",
      "output tensor\n",
      " tensor([[[[-8.6382e-01,  4.9045e-01,  1.3584e+00,  1.3689e+00],\n",
      "          [ 1.1934e+00,  1.9214e-01,  7.4299e-02, -6.3327e-02],\n",
      "          [ 1.0571e+00,  3.2800e-01,  8.7700e-01,  1.2591e-01],\n",
      "          [-7.3485e-01, -3.7770e-01,  8.9464e-01, -8.7694e-01]],\n",
      "\n",
      "         [[ 1.3360e-01,  4.8027e-01, -1.9249e-01, -1.7748e+00],\n",
      "          [-1.0609e+00, -1.5003e+00,  3.7951e-01, -1.6315e+00],\n",
      "          [-4.4608e-01,  1.5505e+00,  9.6277e-01, -2.0497e+00],\n",
      "          [ 1.4581e+00, -1.3502e+00,  3.6130e-01, -3.6371e-01]],\n",
      "\n",
      "         [[-5.4090e-02,  8.8283e-02,  1.0231e+00, -1.4596e-01],\n",
      "          [ 1.8135e-01,  7.3784e-01,  4.9069e-01, -1.7633e+00],\n",
      "          [-2.1126e+00, -5.2382e-01,  1.3907e+00, -1.1459e+00],\n",
      "          [-7.7750e-01,  7.5456e-01,  2.6661e-01, -4.2291e-01]],\n",
      "\n",
      "         [[-1.0122e+00, -1.2364e+00, -8.4599e-01, -2.6884e-01],\n",
      "          [ 1.0834e+00, -1.0768e+00, -1.2751e+00, -4.3288e-01],\n",
      "          [ 1.6246e+00,  5.2860e-01,  3.7734e-01,  2.3630e-02],\n",
      "          [ 8.6742e-04,  1.1708e+00,  1.5223e+00,  1.8296e+00]],\n",
      "\n",
      "         [[-1.0778e+00, -4.0095e-01,  5.2180e-01,  2.2185e-01],\n",
      "          [ 1.0539e+00,  8.9435e-02,  9.2167e-01, -1.0914e+00],\n",
      "          [-2.8028e-02, -1.1981e+00, -8.6729e-01,  1.8500e+00],\n",
      "          [-1.3930e+00, -1.0499e+00,  1.9564e+00, -1.7888e+00]],\n",
      "\n",
      "         [[-4.9579e-01,  3.2699e-01,  5.3620e-01,  1.7964e+00],\n",
      "          [ 7.6875e-01, -4.3507e-01,  4.4515e-01, -1.2757e-01],\n",
      "          [ 9.6665e-01,  6.4694e-01, -5.1127e-01, -1.2940e+00],\n",
      "          [-2.2414e-01, -6.2727e-01,  1.4357e+00, -9.2743e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.2848e-01, -9.3816e-02,  9.6727e-01, -3.1312e-01],\n",
      "          [ 4.6549e-01, -2.1908e-01,  7.1504e-01,  4.6485e-01],\n",
      "          [-8.2662e-01,  2.4732e+00,  6.6696e-02,  9.1302e-01],\n",
      "          [ 1.7226e-01, -2.1077e+00, -1.4680e+00,  2.3250e+00]],\n",
      "\n",
      "         [[ 3.5694e-01, -2.4264e-01, -6.8064e-01,  3.6594e-01],\n",
      "          [-1.2395e+00, -9.1891e-01, -2.9497e-01, -6.3917e-01],\n",
      "          [ 3.2837e-01, -1.9552e-01, -1.6814e-01,  4.5895e-01],\n",
      "          [ 1.3322e+00, -2.1026e+00,  1.8773e-01,  4.5879e-02]],\n",
      "\n",
      "         [[-5.4671e-01,  2.7869e-01, -1.7613e+00,  2.4384e+00],\n",
      "          [-6.8767e-01, -5.9560e-01, -7.2037e-01, -5.3868e-01],\n",
      "          [-6.5794e-01,  8.9753e-01,  1.0107e+00,  1.7157e-01],\n",
      "          [ 2.4207e+00, -9.7191e-01,  1.4743e+00, -9.9086e-01]],\n",
      "\n",
      "         [[ 7.8131e-01,  1.4311e+00, -8.3015e-01, -6.7274e-01],\n",
      "          [ 9.6986e-03, -2.0064e-01, -8.5265e-01,  1.2672e-01],\n",
      "          [-4.3893e-01,  8.3294e-02, -7.1247e-02, -1.5508e+00],\n",
      "          [ 2.9448e-01,  1.1340e+00, -3.1994e-01, -1.4425e-01]],\n",
      "\n",
      "         [[ 1.1923e+00, -4.0092e-01, -3.9294e-01,  1.8971e-01],\n",
      "          [ 1.7618e+00,  1.4502e+00, -6.3694e-01,  1.2647e+00],\n",
      "          [ 1.4306e-01, -1.1337e+00,  4.5277e-01,  6.3339e-01],\n",
      "          [ 1.4020e+00, -1.1519e+00,  4.0910e-01, -5.0498e-01]],\n",
      "\n",
      "         [[-8.7710e-01,  1.2182e-02, -1.9800e-01, -1.8398e+00],\n",
      "          [ 8.0427e-01, -5.4730e-01,  7.1888e-01,  6.6471e-01],\n",
      "          [-1.3368e+00,  4.1644e-01, -1.3525e+00,  1.4675e+00],\n",
      "          [-1.9901e+00,  4.9359e-01, -1.0677e+00, -4.5961e-02]]]])\n"
     ]
    }
   ],
   "source": [
    "#group_norm\n",
    "# function applies group normalization to the input tensor\n",
    "# group normalization devides the channels of the input into groups and normalizes the alues withint each group\n",
    "# useful in scenarions where batch sizes are small or when batch normlization is not suitable\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.randn(2,6,4,4) #batch_size, channels, height , width\n",
    "num_groups = 3\n",
    "weight = torch.ones(6)\n",
    "bias = torch.zeros(6)\n",
    "\n",
    "eps = 1e-5\n",
    "output_tensor = F.group_norm(\n",
    "    input = input_tensor,\n",
    "    bias = bias ,\n",
    "    num_groups = num_groups,\n",
    "    weight = weight,\n",
    "    eps = eps\n",
    ")\n",
    "\n",
    "print('input tensor\\n',input_tensor)\n",
    "print('\\noutput tensor\\n',output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[[[ 0.4429,  0.0564, -0.8291,  1.1433],\n",
      "          [-0.6939, -0.7697, -1.4194,  0.2286],\n",
      "          [-1.4104, -0.1500,  0.2020, -1.0591],\n",
      "          [ 0.3582, -0.1656,  0.2051, -1.1076]],\n",
      "\n",
      "         [[ 0.5590, -0.9749, -2.4325, -0.0278],\n",
      "          [ 0.6364,  1.4038,  1.1722, -0.2091],\n",
      "          [-0.0190,  2.5039, -0.5707, -1.3462],\n",
      "          [ 0.1076,  1.9312,  0.6722,  0.5899]],\n",
      "\n",
      "         [[ 0.1542, -0.6337,  0.1058,  0.3186],\n",
      "          [ 0.9883, -0.1966, -0.2011,  0.7814],\n",
      "          [-0.2530,  0.2210, -0.9094, -0.3013],\n",
      "          [-0.8912,  1.2809,  0.5716,  3.4703]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7105, -0.7458, -0.8224,  0.4514],\n",
      "          [ 0.3341, -0.7917, -1.1918, -2.2718],\n",
      "          [ 1.2324,  0.7007, -0.3812,  0.6503],\n",
      "          [-0.8143, -0.4616,  0.5654, -0.6793]],\n",
      "\n",
      "         [[ 0.0203,  0.8249, -0.6871,  2.0697],\n",
      "          [-1.0145,  0.1541,  2.2623,  0.7888],\n",
      "          [-1.2894, -0.0743,  1.2140,  0.1584],\n",
      "          [-0.2534,  1.0432,  0.2157, -0.1156]],\n",
      "\n",
      "         [[ 0.9860,  1.3922, -0.1893, -1.0633],\n",
      "          [-0.6511,  0.9991, -0.8709,  1.4521],\n",
      "          [ 0.3859,  1.4596,  1.4790, -1.9693],\n",
      "          [ 0.2353, -0.8115, -1.0819, -0.5446]]]])\n",
      "\n",
      "output tensor:\n",
      " tensor([[[[ 1.0404,  0.5067, -0.7160,  2.0075],\n",
      "          [-0.5294, -0.6341, -1.5312,  0.7444],\n",
      "          [-1.5188,  0.2216,  0.7078, -1.0337],\n",
      "          [ 0.9234,  0.2002,  0.7120, -1.1007]],\n",
      "\n",
      "         [[ 0.2583, -1.0231, -2.2408, -0.2319],\n",
      "          [ 0.3230,  0.9641,  0.7706, -0.3833],\n",
      "          [-0.2245,  1.8832, -0.6854, -1.3333],\n",
      "          [-0.1188,  1.4047,  0.3529,  0.2842]],\n",
      "\n",
      "         [[-0.1245, -0.8941, -0.1718,  0.0361],\n",
      "          [ 0.6903, -0.4671, -0.4715,  0.4881],\n",
      "          [-0.5222, -0.0592, -1.1634, -0.5694],\n",
      "          [-1.1455,  0.9760,  0.2833,  3.1145]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0431, -0.5899, -0.6759,  0.7525],\n",
      "          [ 0.6210, -0.6414, -1.0901, -2.3011],\n",
      "          [ 1.6283,  1.0321, -0.1811,  0.9756],\n",
      "          [-0.6667, -0.2713,  0.8803, -0.5154]],\n",
      "\n",
      "         [[-0.3240,  0.5114, -1.0584,  1.8039],\n",
      "          [-1.3984, -0.1850,  2.0038,  0.4739],\n",
      "          [-1.6838, -0.4222,  0.9154, -0.1806],\n",
      "          [-0.6081,  0.7381, -0.1211, -0.4651]],\n",
      "\n",
      "         [[ 0.8400,  1.2147, -0.2443, -1.0505],\n",
      "          [-0.6703,  0.8521, -0.8730,  1.2700],\n",
      "          [ 0.2864,  1.2769,  1.2948, -1.8863],\n",
      "          [ 0.1475, -0.8182, -1.0677, -0.5720]]]])\n"
     ]
    }
   ],
   "source": [
    "#instance_norm\n",
    "# Instance Normalization , normalize sample independently across its spatial dimensions for each channel.\n",
    "# commonly used in tasks like style transfer where normzalization within individual sampes is more appropriate than across a batch\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "inout_tensor = torch.randn(2,3,4,4)\n",
    "running_mean = None \n",
    "running_var = None \n",
    "weight = torch.ones(3)\n",
    "bias = torch.zeros(3)\n",
    "use_input_stats = True \n",
    "momentum = 0.1\n",
    "eps = 1e-5\n",
    "\n",
    "output_tensor = F.instance_norm(\n",
    "    input=input_tensor,\n",
    "    running_mean=running_mean,\n",
    "    running_var=running_var,\n",
    "    weight=weight,\n",
    "    bias=bias,\n",
    "    use_input_stats=use_input_stats,\n",
    "    momentum=momentum,\n",
    "    eps=eps\n",
    ")\n",
    "print('input tensor:\\n', input_tensor)\n",
    "print('\\noutput tensor:\\n',output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[[[-1.3399, -0.5054, -0.2237],\n",
      "          [ 1.0391, -0.8920,  0.5449],\n",
      "          [ 0.4306, -0.5703, -1.0075]],\n",
      "\n",
      "         [[ 0.6220,  0.6217, -1.3201],\n",
      "          [ 2.2047,  1.0517,  0.6067],\n",
      "          [ 0.4296,  1.9671,  0.3063]],\n",
      "\n",
      "         [[ 1.6000,  2.6749, -0.6814],\n",
      "          [-0.1662,  0.5952,  2.2210],\n",
      "          [-0.4480,  0.4725,  2.3571]],\n",
      "\n",
      "         [[ 0.5503, -0.6633, -0.4572],\n",
      "          [ 0.8336,  0.2932, -0.1070],\n",
      "          [-0.7641,  1.1423, -0.7377]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7553, -1.3207, -0.3951],\n",
      "          [ 0.3857,  0.3825,  1.8887],\n",
      "          [-1.6279, -0.9746, -0.0910]],\n",
      "\n",
      "         [[ 0.0063, -0.1847,  0.3210],\n",
      "          [ 2.1754, -1.0228,  0.1505],\n",
      "          [-0.5886, -0.3755,  1.2899]],\n",
      "\n",
      "         [[ 0.4226, -1.7317, -0.3964],\n",
      "          [ 0.4475, -0.2443, -0.8554],\n",
      "          [-0.3420,  0.7340,  0.0734]],\n",
      "\n",
      "         [[ 1.4618,  1.1563,  1.0729],\n",
      "          [-0.0700,  0.8824,  0.6557],\n",
      "          [-0.3382, -0.0596,  1.3355]]]])\n",
      "\n",
      "Output Tensor (Layer Normalized):\n",
      " tensor([[[[-1.4086, -0.2991,  0.0755],\n",
      "          [ 1.7545, -0.8131,  1.0974],\n",
      "          [ 0.9454, -0.3853, -0.9667]],\n",
      "\n",
      "         [[-0.1030, -0.1033, -2.1231],\n",
      "          [ 1.5431,  0.3438, -0.1189],\n",
      "          [-0.3032,  1.2960, -0.4314]],\n",
      "\n",
      "         [[ 0.5297,  1.4170, -1.3535],\n",
      "          [-0.9283, -0.2998,  1.0423],\n",
      "          [-1.1609, -0.4011,  1.1546]],\n",
      "\n",
      "         [[ 0.7932, -0.9884, -0.6859],\n",
      "          [ 1.2091,  0.4157, -0.1718],\n",
      "          [-1.1364,  1.6622, -1.0977]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8287, -1.1576, -0.2720],\n",
      "          [ 0.4751,  0.4720,  1.9131],\n",
      "          [-1.4516, -0.8265,  0.0189]],\n",
      "\n",
      "         [[-0.2054, -0.4113,  0.1339],\n",
      "          [ 2.1334, -1.3151, -0.0500],\n",
      "          [-0.8469, -0.6171,  1.1786]],\n",
      "\n",
      "         [[ 0.8862, -2.1305, -0.2607],\n",
      "          [ 0.9211, -0.0477, -0.9035],\n",
      "          [-0.1845,  1.3223,  0.3972]],\n",
      "\n",
      "         [[ 1.2385,  0.7561,  0.6244],\n",
      "          [-1.1802,  0.3236, -0.0343],\n",
      "          [-1.6037, -1.1637,  1.0391]]]])\n"
     ]
    }
   ],
   "source": [
    "#layer_norm\n",
    "# layer normalization , normalizes teh last few dimensions of the input tensor, dimension except the batch dimension.\n",
    "# it computes the mean and variance over these dimension for each sample independently making it indeendent of btach size\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define an input tensor (batch_size=2, features=4, height=3, width=3)\n",
    "input_tensor = torch.randn(2, 4, 3, 3)  # Shape: (batch_size, features, height, width)\n",
    "\n",
    "# Specify the dimensions to normalize (last two dimensions: height and width)\n",
    "normalized_shape = (3, 3)  # Normalize over the last two dimensions (height, width)\n",
    "\n",
    "# Define learnable parameters (optional)\n",
    "weight = torch.ones(3 * 3).reshape(normalized_shape)  # Scale parameter (gamma), one value per normalized element\n",
    "bias = torch.zeros(3 * 3).reshape(normalized_shape)   # Shift parameter (beta), one value per normalized element\n",
    "\n",
    "# Apply Layer Normalization\n",
    "eps = 1e-5  # Small constant for numerical stability\n",
    "output_tensor = F.layer_norm(\n",
    "    input=input_tensor,\n",
    "    normalized_shape=normalized_shape,\n",
    "    weight=weight,\n",
    "    bias=bias,\n",
    "    eps=eps\n",
    ")\n",
    "\n",
    "# Print the output tensor\n",
    "print(\"Input Tensor:\\n\", input_tensor)\n",
    "print(\"\\nOutput Tensor (Layer Normalized):\\n\", output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[[[ 2.0667e-01, -1.0599e-01, -2.4010e+00],\n",
      "          [ 1.7566e+00, -1.4337e+00,  1.7986e+00],\n",
      "          [ 2.5884e-01,  9.1898e-01,  4.5527e-01]],\n",
      "\n",
      "         [[ 1.2623e-02, -8.4083e-01,  1.3972e+00],\n",
      "          [-5.5456e-01, -5.8515e-01,  1.6556e+00],\n",
      "          [-5.2487e-01, -1.4630e+00,  4.9386e-02]],\n",
      "\n",
      "         [[-1.2961e+00,  6.3021e-01,  1.2197e-01],\n",
      "          [-6.3145e-01,  7.6239e-02, -2.3349e-01],\n",
      "          [ 6.1112e-01, -2.8929e-01, -1.2086e+00]],\n",
      "\n",
      "         [[ 9.8071e-01, -1.1314e+00, -5.0179e-01],\n",
      "          [ 4.4344e-01,  5.2481e-01, -1.7056e+00],\n",
      "          [-2.2077e+00,  1.6969e+00,  3.7873e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.4024e-01, -2.1024e-01,  1.4448e+00],\n",
      "          [ 2.1806e-01,  9.4254e-01, -1.7191e+00],\n",
      "          [-8.0877e-02, -1.0069e-03,  7.7004e-01]],\n",
      "\n",
      "         [[-1.1582e+00, -1.8372e-01, -1.6108e-03],\n",
      "          [ 8.0797e-01, -7.6964e-01, -1.1793e-01],\n",
      "          [-8.5453e-01, -9.9000e-02, -1.6620e+00]],\n",
      "\n",
      "         [[-6.1338e-01, -8.1826e-01, -2.0167e-01],\n",
      "          [ 1.3704e-01,  8.9162e-01, -3.0222e-02],\n",
      "          [-1.1289e+00, -2.0020e+00,  4.9782e-01]],\n",
      "\n",
      "         [[-1.1663e-01, -1.6271e-02,  1.7725e-01],\n",
      "          [-6.3695e-01,  1.3094e+00,  1.0954e+00],\n",
      "          [-1.0867e+00,  9.0821e-01,  6.8355e-02]]]])\n",
      "\n",
      "output:\n",
      " tensor([[[[ 1.1622e+00, -5.9595e-01, -1.3476e+01],\n",
      "          [ 9.8696e+00, -8.0574e+00,  1.0099e+01],\n",
      "          [ 1.4555e+00,  5.1640e+00,  2.5600e+00]],\n",
      "\n",
      "         [[ 7.0954e-02, -4.7270e+00,  7.8420e+00],\n",
      "          [-3.1156e+00, -3.2886e+00,  9.2960e+00],\n",
      "          [-2.9510e+00, -8.2205e+00,  2.7760e-01]],\n",
      "\n",
      "         [[-7.2837e+00,  3.5418e+00,  6.8551e-01],\n",
      "          [-3.5501e+00,  4.2866e-01, -1.3112e+00],\n",
      "          [ 3.4318e+00, -1.6247e+00, -6.7938e+00]],\n",
      "\n",
      "         [[ 5.5113e+00, -6.3595e+00, -2.8216e+00],\n",
      "          [ 2.4933e+00,  2.9510e+00, -9.5842e+00],\n",
      "          [-1.2398e+01,  9.5353e+00,  2.1289e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1607e+00, -1.1823e+00,  8.1206e+00],\n",
      "          [ 1.2260e+00,  5.2983e+00, -9.6598e+00],\n",
      "          [-4.5472e-01, -5.6624e-03,  4.3266e+00]],\n",
      "\n",
      "         [[-6.5094e+00, -1.0329e+00, -9.0531e-03],\n",
      "          [ 4.5427e+00, -4.3256e+00, -6.6267e-01],\n",
      "          [-4.8030e+00, -5.5616e-01, -9.3375e+00]],\n",
      "\n",
      "         [[-3.4478e+00, -4.6006e+00, -1.1341e+00],\n",
      "          [ 7.7042e-01,  5.0101e+00, -1.6990e-01],\n",
      "          [-6.3432e+00, -1.1244e+01,  2.7973e+00]],\n",
      "\n",
      "         [[-6.5582e-01, -9.1483e-02,  9.9674e-01],\n",
      "          [-3.5815e+00,  7.3588e+00,  6.1581e+00],\n",
      "          [-6.1071e+00,  5.1011e+00,  3.8436e-01]]]])\n"
     ]
    }
   ],
   "source": [
    "#local_response_norm\n",
    "\n",
    "#LRN normalizes each pixel across nearby channels (feature maps) and is commonly used in covolutional neural networks (CNNs), particularly in older architectures like AlesNet.\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "input_tensor = torch.randn(2,4,3,3)\n",
    "size =3\n",
    "alpha = 0.0001\n",
    "beta = 0.75 \n",
    "k = 0.1\n",
    "\n",
    "output_tensor = F.local_response_norm(\n",
    "    input=input_tensor,\n",
    "    size=size,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    k=k\n",
    ")\n",
    "\n",
    "print('input:\\n',input_tensor)\n",
    "print('\\noutput:\\n',output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[[[ 0.1922, -0.3542, -0.0786],\n",
      "          [-0.6105, -0.2181, -0.2393],\n",
      "          [-0.4442, -1.9372,  1.3535]],\n",
      "\n",
      "         [[ 0.0453,  1.2244,  0.9363],\n",
      "          [ 0.7094,  0.9371,  1.0721],\n",
      "          [-1.2750,  1.0188,  0.2003]],\n",
      "\n",
      "         [[-0.7419, -1.9866,  0.2004],\n",
      "          [ 0.7826,  1.0550,  1.4504],\n",
      "          [-0.2918, -1.5483,  2.3967]],\n",
      "\n",
      "         [[-1.1690, -0.5543, -1.0750],\n",
      "          [ 0.1287, -0.3554, -0.3362],\n",
      "          [-0.0775,  0.0403,  1.4425]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4881,  1.2022, -0.6943],\n",
      "          [ 0.4945, -0.6746, -1.6062],\n",
      "          [ 0.7551,  0.8657,  0.0080]],\n",
      "\n",
      "         [[ 0.3827,  1.7225, -0.0801],\n",
      "          [ 1.9365,  1.2369,  0.3345],\n",
      "          [-0.1362, -2.0459,  0.6690]],\n",
      "\n",
      "         [[-0.6671, -0.2857,  1.5563],\n",
      "          [ 1.8626, -0.2039, -0.9786],\n",
      "          [ 0.2453,  1.0418,  0.4889]],\n",
      "\n",
      "         [[ 0.2505, -0.3716,  1.2659],\n",
      "          [-0.7632,  0.6880,  0.1976],\n",
      "          [ 0.7440, -0.8440,  0.2968]]]])\n",
      "\n",
      "output tensor (rms normalized):\n",
      " tensor([[[[ 0.2274, -0.4191, -0.0930],\n",
      "          [-0.7224, -0.2580, -0.2831],\n",
      "          [-0.5256, -2.2922,  1.6015]],\n",
      "\n",
      "         [[ 0.0492,  1.3313,  1.0180],\n",
      "          [ 0.7713,  1.0190,  1.1657],\n",
      "          [-1.3863,  1.1078,  0.2178]],\n",
      "\n",
      "         [[-0.5464, -1.4631,  0.1476],\n",
      "          [ 0.5764,  0.7770,  1.0682],\n",
      "          [-0.2149, -1.1403,  1.7651]],\n",
      "\n",
      "         [[-1.5418, -0.7311, -1.4178],\n",
      "          [ 0.1698, -0.4688, -0.4434],\n",
      "          [-0.1022,  0.0531,  1.9025]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5630,  1.3866, -0.8008],\n",
      "          [ 0.5703, -0.7781, -1.8527],\n",
      "          [ 0.8710,  0.9986,  0.0092]],\n",
      "\n",
      "         [[ 0.3164,  1.4243, -0.0663],\n",
      "          [ 1.6012,  1.0227,  0.2766],\n",
      "          [-0.1127, -1.6917,  0.5532]],\n",
      "\n",
      "         [[-0.6746, -0.2889,  1.5737],\n",
      "          [ 1.8835, -0.2062, -0.9895],\n",
      "          [ 0.2481,  1.0535,  0.4944]],\n",
      "\n",
      "         [[ 0.3645, -0.5406,  1.8416],\n",
      "          [-1.1103,  1.0010,  0.2875],\n",
      "          [ 1.0824, -1.2279,  0.4318]]]])\n"
     ]
    }
   ],
   "source": [
    "#rms_nrom\n",
    "#applies root mean square normalization\n",
    "#normalize input tensor by dividing it by the root mean sqyare of its value over the specified dimensions. \n",
    "#RMSNorm is a lightweight normalization technoque that does not compute a mean or require learnable bias parameters , making it computationally efficient\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.randn(2,4,3,3)\n",
    "normalized_shape = (3,3) # normalize over the last two dimensions\n",
    "weight = torch.ones(3*3).reshape(normalized_shape)\n",
    "eps = 1e-6 # small constant for numerical stability\n",
    "\n",
    "#apply rms norm\n",
    "output_tensor = F.rms_norm(\n",
    "    input=input_tensor,\n",
    "    normalized_shape=normalized_shape,\n",
    "    weight=weight,\n",
    "    eps=eps\n",
    ")\n",
    "\n",
    "print('input tensor:\\n', input_tensor)\n",
    "print('\\noutput tensor (rms normalized):\\n',output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4.]])\n",
      "normalized tenspr:\n",
      " tensor([[0.3235, 0.4313, 0.5392, 0.6470],\n",
      "        [0.1826, 0.3651, 0.5477, 0.7303]])\n",
      "\n",
      "L2 Norm of Each Row tensor([1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "#normalize \n",
    "# function performs Lp normalization of the input tensor over a specified dimension.\n",
    "# it normalizes each vector along the specified dimension by deviding it by its L[ norm\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([[3.0,4.0,5.0,6.0],[1.0,2.0,3.0,4.0]])\n",
    "print('input tensor:\\n',input_tensor)\n",
    "\n",
    "# parameter for normalization\n",
    "p = 2.0 # L2 nrom (euclidean norm)\n",
    "dim = 1 #normalize along dimension 1 (feautres)\n",
    "eps = 1e-12 \n",
    "\n",
    "normalized_tensor = F.normalize(input=input_tensor,p=p,eps=eps,dim=dim)\n",
    "print('normalized tenspr:\\n',normalized_tensor)\n",
    "\n",
    "#verify the l2 norm of each row is approximately 1\n",
    "print('\\nL2 Norm of Elach Row',torch.norm(normalized_tensor,p=2,dim=dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[-1.0893, -0.3072, -0.6570, -0.6048],\n",
      "        [-1.9077,  1.0850,  0.6637, -1.1992]])\n",
      "\n",
      "Weight Matrix:\n",
      " tensor([[-2.1312, -1.9938,  0.6720, -1.2265],\n",
      "        [ 1.0330,  0.4254, -0.7774,  0.1815],\n",
      "        [ 0.1109, -0.0977, -2.3333, -0.9830]])\n",
      "\n",
      "Bias Vector:\n",
      " tensor([0.7982, 0.0718, 0.2012])\n",
      "\n",
      "Output Tensor (Linear Transformation):\n",
      " tensor([[ 4.0324, -0.7831,  2.2380],\n",
      "        [ 4.6174, -2.1708, -0.4861]])\n"
     ]
    }
   ],
   "source": [
    "#linaer \n",
    "# this function applies linear transformation to the input tensor\n",
    "#operatin is commonly used in neural network for full =y connected layres\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.randn(2,4)\n",
    "weight_matrix = torch.randn(3,4)\n",
    "bias_vector = torch.randn(3)\n",
    "output_tensor = F.linear(input=input_tensor, weight=weight_matrix, bias = bias_vector)\n",
    "\n",
    "print(\"Input Tensor:\\n\", input_tensor)\n",
    "print(\"\\nWeight Matrix:\\n\", weight_matrix)\n",
    "print(\"\\nBias Vector:\\n\", bias_vector)\n",
    "print(\"\\nOutput Tensor (Linear Transformation):\\n\", output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor 1:\n",
      " tensor([[-0.3954, -0.9825,  0.6825, -0.5252],\n",
      "        [-2.1812, -0.5727,  2.0892, -0.2917]])\n",
      "\n",
      "Input Tensor 2:\n",
      " tensor([[-1.0951, -2.5553, -0.2692],\n",
      "        [-0.7462,  1.1191,  1.1455]])\n",
      "\n",
      "Weight Tensor:\n",
      " tensor([[[-0.0739,  0.3247, -0.6910],\n",
      "         [ 0.8234, -1.1381,  1.0265],\n",
      "         [ 1.2938, -0.8794,  0.2502],\n",
      "         [ 0.5947,  1.0882, -0.3124]],\n",
      "\n",
      "        [[-0.7460, -0.6583,  1.2932],\n",
      "         [ 1.6261, -0.0850, -1.4893],\n",
      "         [-1.3521,  0.0175, -0.1038],\n",
      "         [ 0.5686, -1.3340,  1.8783]],\n",
      "\n",
      "        [[ 0.5009,  1.0569, -0.8461],\n",
      "         [-1.4141, -0.5980,  0.2224],\n",
      "         [-0.8126,  1.5270, -0.3843],\n",
      "         [ 0.1493, -0.1760,  1.2149]],\n",
      "\n",
      "        [[ 0.1895,  1.2277, -1.8962],\n",
      "         [ 1.5365, -1.7857,  0.3528],\n",
      "         [ 1.1865, -0.2864, -0.4948],\n",
      "         [ 0.1859, -0.2852,  0.9319]],\n",
      "\n",
      "        [[-0.1267, -1.0174, -1.2453],\n",
      "         [ 0.3359,  1.0251,  1.5602],\n",
      "         [-0.6472,  1.2854, -1.8520],\n",
      "         [-0.0937,  0.8802, -0.5476]]])\n",
      "\n",
      "Bias Tensor:\n",
      " tensor([-0.8138, -1.3804, -0.1964,  0.3024, -1.6059])\n",
      "\n",
      "Output Tensor (Bilinear Transformation):\n",
      " tensor([[-0.0122, -1.2871, -3.9296, -1.7544,  0.1586],\n",
      "        [-3.1880, -0.6602,  3.3875,  0.0400,  1.7034]])\n"
     ]
    }
   ],
   "source": [
    "#bilinear\n",
    "# operation is useful in tasks like bilinear pooling where interactions between two sets of features are modeled\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input1 = torch.randn(2,4)\n",
    "input2= torch.randn(2,3)\n",
    "\n",
    "weight = torch.randn(5,4,3)\n",
    "bias = torch.randn(5)\n",
    "output_tensor = F.bilinear(input1=input1, input2=input2, weight=weight, bias=bias)\n",
    "\n",
    "print(\"Input Tensor 1:\\n\", input1)\n",
    "print(\"\\nInput Tensor 2:\\n\", input2)\n",
    "print(\"\\nWeight Tensor:\\n\", weight)\n",
    "print(\"\\nBias Tensor:\\n\", bias)\n",
    "print(\"\\nOutput Tensor (Bilinear Transformation):\\n\", output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor (after dropout):\n",
      " tensor([[2., 2., 2., 2., 2.],\n",
      "        [0., 0., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "#dropout\n",
    "#regulirization technique that randomly zeroes soem elements of the input tensor during training with a specified probability p/\n",
    "#dropout helps prevent overfitting by reducing co-adaption between beurons\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "# input_tensor = torch.tensor([2.0,5.0,-1.0,6.0,3.0])\n",
    "input_tensor = torch.ones(2,5)\n",
    "\n",
    "training =  True\n",
    "inplace = False \n",
    "p = 0.5 #probability of zeroing and element\n",
    "\n",
    "output_tensor = F.dropout(\n",
    "    input=input_tensor,\n",
    "    training=training,\n",
    "    p=p,\n",
    "    inplace=inplace,\n",
    ")\n",
    "print('output tensor (after dropout):\\n',output_tensor)\n",
    "\n",
    "#each eement has a 50% chance of being zeroed.\n",
    "# for example if thes econd and fourth elements are zerored, in the first row the reaming elements are scled by (1/(1-0.5) = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor:\n",
      " tensor([[-0.7792, -0.7792, -0.7792, -0.7792,  0.3448,  3.1374],\n",
      "        [-0.7792,  1.1091,  0.4981, -0.7792, -0.7792,  1.3498]])\n"
     ]
    }
   ],
   "source": [
    "# alpha_dropout\n",
    "# variant of dropout designed to work with self-normalizing neural networks.\n",
    "# e.g using SELU activation function\n",
    "# alpha dropout ensures that the mean and variance of the activations remain unchanged, even after droout is applied\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.randn(2,6)\n",
    "\n",
    "#can add\n",
    "# if self.training: output.F.alpha_dropout()\n",
    "output_tensor = F.alpha_dropout(\n",
    "    input=input_tensor,\n",
    "    p=0.5,# probability of zeroing and element\n",
    "    training=True,\n",
    "    inplace=False\n",
    ")\n",
    "print('output tensor:\\n',output_tensor)\n",
    "\n",
    "#custom dropout probability, experiment with different probabilities\n",
    "# in-place operation, perform alpha dropout in-place ot save money\n",
    "# batch processing, apply aplha dropout to batches of data for efficient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor (training mode)\n",
      " tensor([[-0.7792, -0.7792, -0.7792, -0.7792,  0.3448,  3.1374],\n",
      "        [-0.7792,  1.1091,  0.4981, -0.7792, -0.7792,  1.3498]])\n",
      "output tensor (evaluation mode):\n",
      " tensor([[-2.9115,  0.5659,  0.8973,  0.7014,  2.5634],\n",
      "        [ 1.4822,  1.1028,  1.0060,  0.8892,  2.2446]])\n"
     ]
    }
   ],
   "source": [
    "#feature_alpha_dropout\n",
    "# function applies Feature Alpha Dropout, a variant of droout that randmoly masks entire feauture channels.\n",
    "# variant of dropout that randomly masks entire feature channles\n",
    "# it is designed to work with self-normalizing networks\n",
    "# ensures that the mean and variance of the activations remain unchanged\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "\n",
    "    def forward(self,input_tensor):\n",
    "        if self.training:\n",
    "            output = F.feature_alpha_dropout(\n",
    "                 p = 0.5,\n",
    "                training = True,\n",
    "                inplace = False,\n",
    "                input=input_tensor\n",
    "            )\n",
    "        else:\n",
    "            output = input_tensor\n",
    "        return output\n",
    "\n",
    "input_tensor = torch.randn(2,5)\n",
    "model = MyModel()\n",
    "model.train()\n",
    "output_train = model(input_tensor)\n",
    "print('output tensor (training mode)\\n', output_tensor)\n",
    "\n",
    "model.eval()\n",
    "output_eval = model(input_tensor)\n",
    "print('output tensor (evaluation mode):\\n',output_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n",
      "\n",
      "Output Tensor (After 1D Dropout):\n",
      " tensor([[[2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [2., 2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2., 2.]]])\n"
     ]
    }
   ],
   "source": [
    "#dropout1d\n",
    "# This function applies 1D dropout , which randomly zeroes out entire channels (1D feature maps) in the input tensor. It is commonly used in 1D convolutional neural networks (e.g., for time-series data or audio signals).\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define an input tensor (batch_size=2, channels=3, length=5)\n",
    "input_tensor = torch.ones(2, 3, 5)  # Shape: (batch_size, channels, length)\n",
    "print(\"Input Tensor:\\n\", input_tensor)\n",
    "\n",
    "# Parameters for 1D dropout\n",
    "p = 0.5           # Probability of zeroing out a channel (50% chance)\n",
    "training = True   # Apply dropout only during training\n",
    "inplace = False   # Do not modify the input tensor in-place\n",
    "\n",
    "# Apply 1D dropout\n",
    "output_tensor = F.dropout1d(input=input_tensor, p=p, training=training, inplace=inplace)\n",
    "\n",
    "# Print the output tensor\n",
    "print(\"\\nOutput Tensor (After 1D Dropout):\\n\", output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]])\n",
      "\n",
      "Output Tensor (After 2D Dropout):\n",
      " tensor([[[[2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.],\n",
      "          [2., 2., 2., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "#dropoutwd\n",
    "# this function applies 2D dropout , which randomly zeroes out entire channels (2D feature maps) in the input tensor. It is commonly used in 2D convolutional neural networks (e.g., for image data).\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define an input tensor (batch_size=2, channels=3, height=4, width=4)\n",
    "input_tensor = torch.ones(2, 3, 4, 4)  # Shape: (batch_size, channels, height, width)\n",
    "print(\"Input Tensor:\\n\", input_tensor)\n",
    "\n",
    "# Parameters for 2D dropout\n",
    "p = 0.5           # Probability of zeroing out a channel (50% chance)\n",
    "training = True   # Apply dropout only during training\n",
    "inplace = False   # Do not modify the input tensor in-place\n",
    "\n",
    "# Apply 2D dropout\n",
    "output_tensor = F.dropout2d(input=input_tensor, p=p, training=training, inplace=inplace)\n",
    "\n",
    "# Print the output tensor\n",
    "print(\"\\nOutput Tensor (After 2D Dropout):\\n\", output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[[[[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1.]]]]])\n",
      "\n",
      "Output Tensor (After 3D Dropout):\n",
      " tensor([[[[[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.],\n",
      "           [0., 0., 0., 0.]]]]])\n"
     ]
    }
   ],
   "source": [
    "#dropout3d\n",
    "# This function applies 3D dropout , which randomly zeroes out entire channels (3D feature maps) in the input tensor. It is commonly used in 3D convolutional neural networks (e.g., for volumetric data such as medical imaging or video processing)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define an input tensor (batch_size=2, channels=3, depth=4, height=4, width=4)\n",
    "input_tensor = torch.ones(2, 3, 4, 4, 4)  # Shape: (batch_size, channels, depth, height, width)\n",
    "print(\"Input Tensor:\\n\", input_tensor)\n",
    "\n",
    "# Parameters for 3D dropout\n",
    "p = 0.5           # Probability of zeroing out a channel (50% chance)\n",
    "training = True   # Apply dropout only during training\n",
    "inplace = False   # Do not modify the input tensor in-place\n",
    "\n",
    "# Apply 3D dropout\n",
    "output_tensor = F.dropout3d(input=input_tensor, p=p, training=training, inplace=inplace)\n",
    "\n",
    "# Print the output tensor\n",
    "print(\"\\nOutput Tensor (After 3D Dropout):\\n\", output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix:\n",
      " tensor([[ 0.0068, -2.6321,  0.2775],\n",
      "        [ 0.0899,  0.2752, -0.2612],\n",
      "        [ 0.0830,  2.0061, -0.2114],\n",
      "        [-1.6072, -2.3703,  0.0961],\n",
      "        [-1.1281,  0.5882, -1.3024],\n",
      "        [ 1.0387,  0.8942,  0.1183],\n",
      "        [-0.4841, -1.0705, -0.9068],\n",
      "        [-0.8395,  1.2017,  1.4339],\n",
      "        [-0.6019,  2.0140,  0.8370],\n",
      "        [-0.2726, -0.5958,  1.2552]])\n",
      "\n",
      "Input Tensor (Indices):\n",
      " tensor([[1, 2, 4, 5],\n",
      "        [4, 3, 2, 9]])\n",
      "\n",
      "Output Tensor (Embeddings):\n",
      " tensor([[[ 0.0899,  0.2752, -0.2612],\n",
      "         [ 0.0830,  2.0061, -0.2114],\n",
      "         [-1.1281,  0.5882, -1.3024],\n",
      "         [ 1.0387,  0.8942,  0.1183]],\n",
      "\n",
      "        [[-1.1281,  0.5882, -1.3024],\n",
      "         [-1.6072, -2.3703,  0.0961],\n",
      "         [ 0.0830,  2.0061, -0.2114],\n",
      "         [-0.2726, -0.5958,  1.2552]]])\n",
      "\n",
      "Output Tensor (Embeddings):\n",
      " tensor([[[-2.8683, -1.0660, -1.4782],\n",
      "         [ 0.8882,  0.1742,  0.2513],\n",
      "         [-1.0397,  0.0435, -0.8875],\n",
      "         [-0.1824,  0.6750, -0.2242]],\n",
      "\n",
      "        [[-1.0397,  0.0435, -0.8875],\n",
      "         [ 0.4150,  0.5355,  0.7830],\n",
      "         [ 0.8882,  0.1742,  0.2513],\n",
      "         [-0.4468, -0.5225, -0.3968]]])\n"
     ]
    }
   ],
   "source": [
    "#embedding\n",
    "# function retrieves embeddings from a fixed dictionary (embedding matrix) based on input indices, it is commonly used in natural languege processing\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define the embedding matrix (v=10, embedding_dims=3)\n",
    "#V = vocabulart size (number of unique tokens), embedding_dims=3\n",
    "embedding_matrix = torch.randn(10,3) #Shape: (V=10, embedding_dims=3)\n",
    "\n",
    "#define the input tensor (batch_size=2, sequence_length=4)\n",
    "#each element in the input tensor is an index into the embedding matrix\n",
    "input_tensor = torch.tensor([[1,2,4,5],[4,3,2,9]])#batch_size, sequenec_length\n",
    "\n",
    "output_tensor = F.embedding(input=input_tensor, weight=embedding_matrix)\n",
    "\n",
    "print(\"Embedding Matrix:\\n\", embedding_matrix)\n",
    "print(\"\\nInput Tensor (Indices):\\n\", input_tensor)\n",
    "print(\"\\nOutput Tensor (Embeddings):\\n\", output_tensor)\n",
    "\n",
    "# can specified padding_idx, embedding vector for at that index is not updated during training and remains fixed.\n",
    "weights = torch.randn(10,3)\n",
    "weights[0,:].zero_() # set the embedding vector for padding_idx = 0 to zeros\n",
    "output_tensor = F.embedding(input=input_tensor, weight=weights, padding_idx=0)\n",
    "print('\\nOutput Tensor (Embeddings):\\n',output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor (class indices):\n",
      " tensor([0, 1, 2, 1, 0])\n",
      "\n",
      "One-Hot Encoded Tensor (Inferred Classes):\n",
      " tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "\n",
      "One-Hot Encoded Tensor (Specified Classes):\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "input tensor (2d class indics):\n",
      " tensor([[0, 1],\n",
      "        [2, 0]])\n",
      "one-hot encoded tensor (2d-input):\n",
      " tensor([[[1, 0, 0],\n",
      "         [0, 1, 0]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [1, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "#one_hot\n",
    "# function converts a tensor of class indices into a one-hot encoded tensor\n",
    "# one-hot encoding is commonly used in machine learning tasks suc as classification, where categorical dat is represented as binary vectors\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class_indices = torch.tensor([0,1,2,1,0])\n",
    "print('input tensor (class indices):\\n',class_indices)\n",
    "\n",
    "#apply one-hot encoding with inferred number of classes\n",
    "one_hot_tensor_inferred = F.one_hot(class_indices)\n",
    "print('\\nOne-Hot Encoded Tensor (Inferred Classes):\\n', one_hot_tensor_inferred)\n",
    "\n",
    "#apply one-hot encoding with a specified number of classes\n",
    "num_classes = 5\n",
    "one_hot_tensor_specified = F.one_hot(class_indices, num_classes=num_classes)\n",
    "print('\\nOne-Hot Encoded Tensor (Specified Classes):\\n', one_hot_tensor_specified)\n",
    "\n",
    "#example with multi-dimensional input\n",
    "class_indices_2d = torch.tensor([[0,1],[2,0]])\n",
    "print('input tensor (2d class indics):\\n', class_indices_2d)\n",
    "\n",
    "one_hot_tensor_2d = F.one_hot(class_indices_2d, num_classes=3)\n",
    "print('one-hot encoded tensor (2d-input):\\n',one_hot_tensor_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor x1:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "input tensor x2:\n",
      " tensor([[1.5000, 2.5000, 3.5000],\n",
      "        [4.5000, 5.5000, 6.5000]])\n",
      "\n",
      "pairwise distances:\n",
      " tensor([0.8660, 0.8660])\n"
     ]
    }
   ],
   "source": [
    "#pairwise \n",
    "#pairwise distance between two tensors along a specified dimension using Lp norm (euclidean distance for p= 2)\n",
    "#it is commonly used iin tasks like similarity computation clustering and metric learning.\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define two input tensor with same shape\n",
    "x1 = torch.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "x2 = torch.tensor([[1.5,2.5,3.5],[4.5,5.5,6.5]])\n",
    "\n",
    "#parameter for pairwise distance\n",
    "p = 2.0 #use l2 norm (euclidean distance)\n",
    "eps = 1e-6 #small value to avoid division by zero\n",
    "keepdim = False # donot keep the reduced dimension\n",
    "\n",
    "pairwise_distances = F.pairwise_distance(x1=x1, x2=x2, p=p, eps=eps, keepdim=keepdim)\n",
    "\n",
    "#print the result\n",
    "print('input tensor x1:\\n',x1)\n",
    "print('input tensor x2:\\n',x2)\n",
    "print('\\npairwise distances:\\n',pairwise_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor x1:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Input Tensor x2:\n",
      " tensor([[1., 2., 3.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Cosine Similarities:\n",
      " tensor([1.0000, 0.9869])\n"
     ]
    }
   ],
   "source": [
    "#cosine_similarity\n",
    "# function computes the cosine similarity between two tensors along a specified dimension\n",
    "# cosine similarity measures the cosine of the angle between two vectors and is commonly used in task like texts similarity, recommendation systems and metric learning\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "x1 = torch.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "x2 = torch.tensor([[1.0,2.0,3.0],[1.0,1.0,1.0]])\n",
    "\n",
    "dim = 1\n",
    "eps = 1e-8\n",
    "\n",
    "cosine_similarities = F.cosine_similarity(x1=x1,x2=x2,dim=dim,eps=eps)\n",
    "\n",
    "print(\"Input Tensor x1:\\n\", x1)\n",
    "print(\"\\nInput Tensor x2:\\n\", x2)\n",
    "print(\"\\nCosine Similarities:\\n\", cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:\n",
      " tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "\n",
      "pairwise distances (upper triangular portion):\n",
      " tensor([ 5.1962, 10.3923, 15.5885,  5.1962, 10.3923,  5.1962])\n"
     ]
    }
   ],
   "source": [
    "#pdist\n",
    "#this function computes the pairwise Lp-Norm distances between every pair of row vectors in the input tensor.\n",
    "# it is commonly used in tasks like clustering , similarity computation and metric learning\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([\n",
    "    [1.0,2.0,3.0],\n",
    "    [4.0,5.0,6.0],\n",
    "    [7.0,8.0,9.0],\n",
    "    [10.0,11.0,12.0]\n",
    "])\n",
    "\n",
    "pairwise_distances = F.pdist(input=input_tensor, p=p)\n",
    "print('input tensor:\\n', input_tensor)\n",
    "print('\\npairwise distances (upper triangular portion):\\n',pairwise_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Predicted Probabilities):\n",
      " tensor([0.0000, 0.6000, 0.2000])\n",
      "\n",
      "Target Tensor (Ground Truth Labels):\n",
      " tensor([1., 1., 0.])\n",
      "\n",
      "Binary Cross-Entropy Loss:\n",
      " tensor(33.5780)\n"
     ]
    }
   ],
   "source": [
    "#binary_cross_entropy\n",
    "# computes binary cross-entropy loss between predicted probabilities and target labels.\n",
    "# commonly used in binary classification \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor(\n",
    "    [0.0,0.6,0.2]\n",
    ")\n",
    "\n",
    "target_tensor = torch.tensor(\n",
    "    [1.0,1.0,0.0]\n",
    ")\n",
    "\n",
    "loss = F.binary_cross_entropy(input=input_tensor, target=target_tensor, reduction='mean')\n",
    "\n",
    "print(\"Input Tensor (Predicted Probabilities):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Ground Truth Labels):\\n\", target_tensor)\n",
    "print(\"\\nBinary Cross-Entropy Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Logits):\n",
      " tensor([ 2.0000, -1.0000,  0.5000])\n",
      "\n",
      "Target Tensor (Ground Truth Labels):\n",
      " tensor([1., 0., 1.])\n",
      "\n",
      "Binary Cross-Entropy Loss with Logits:\n",
      " tensor(0.4049)\n"
     ]
    }
   ],
   "source": [
    "#binary_cross_entropy_with_logits\n",
    "# computes binary cross-entropy between target labels and input logits , applying a sigmoid activation internally.\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([\n",
    "    2.0,-1.0,0.5 \n",
    "])\n",
    "\n",
    "target_tensor = torch.tensor([\n",
    "    1.0,0.0,1.0 #batch_size = 3, num_classes = 1, values betweet 0 and 1 # ground truth table for each samples\n",
    "])\n",
    "\n",
    "pos_weight = torch.tensor([1.5])\n",
    "\n",
    "#compute binary cross-enttropy loss with default reduction ='mean'\n",
    "loss = F.binary_cross_entropy_with_logits(\n",
    "    input=input_tensor,\n",
    "    target=target_tensor,\n",
    "    pos_weight=pos_weight,\n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "print(\"Input Tensor (Logits):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Ground Truth Labels):\\n\", target_tensor)\n",
    "print(\"\\nBinary Cross-Entropy Loss with Logits:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Predicted Expectations):\n",
      " tensor([2., 5., 1.])\n",
      "\n",
      "Target Tensor (Observed Counts):\n",
      " tensor([1., 4., 2.])\n",
      "\n",
      "Poisson Negative Log-Likelihood Loss:\n",
      " tensor(44.8402)\n"
     ]
    }
   ],
   "source": [
    "#poison_nll_loss\n",
    "\n",
    "#function computes the Poisson negative log-likelihood loss\n",
    "# commonly used in tasks involving count data or Poisson-distributed targets\n",
    "# it measures the difference between the predicted expectation and the observer counts \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input_tensor = torch.tensor([\n",
    "    2.00,5.0,1.0#predicted expectations\n",
    "])\n",
    "\n",
    "target_tensor = torch.tensor([\n",
    "    1.0,4.0,2.0 #observerd counts\n",
    "])\n",
    "\n",
    "log_input = True \n",
    "full = False \n",
    "reduction = 'mean'\n",
    "\n",
    "#compute poisson negative log-likelihood loss\n",
    "loss = F.poisson_nll_loss(\n",
    "    input=input_tensor,\n",
    "    target=target_tensor,\n",
    "    log_input=log_input,\n",
    "    full=full,\n",
    "    reduction=reduction,\n",
    ")\n",
    "\n",
    "print(\"Input Tensor (Predicted Expectations):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Observed Counts):\\n\", target_tensor)\n",
    "print(\"\\nPoisson Negative Log-Likelihood Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor 1:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Input Tensor 2:\n",
      " tensor([[1., 2., 3.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Target Tensor (1=similar, -1=dissimilar):\n",
      " tensor([ 1, -1])\n",
      "\n",
      "Cosine Embedding Loss:\n",
      " tensor(0.2435)\n"
     ]
    }
   ],
   "source": [
    "#cosine_embedding_loss\n",
    "#function computes the cosine embedding loss which is used to measure the similarity between two tensors \n",
    "# based on their cosine similarity\n",
    "# commonly used in tasks like metric laerning, where you want to push similar pairs closed together and dissimilar pairs farther apart\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input1 = torch.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "input2 = torch.tensor([[1.0,2.0,3.0],[1.0,1.0,1.0]])\n",
    "\n",
    "#define target tensor whether the pairs are similar or dissimiar\n",
    "target = torch.tensor([1,-1])\n",
    "\n",
    "#parameter for cosine embeddig loss\n",
    "margin = 0.5 #margin for dissimlar pairs\n",
    "reduction = 'mean' #average the loss acorss samples\n",
    "\n",
    "#compute cosine embedding loss\n",
    "loss = F.cosine_embedding_loss(\n",
    "    input1=input1,\n",
    "    input2=input2,\n",
    "    target=target,\n",
    "    margin=margin,\n",
    "    reduction=reduction\n",
    ")\n",
    "# Print the results\n",
    "print(\"Input Tensor 1:\\n\", input1)\n",
    "print(\"\\nInput Tensor 2:\\n\", input2)\n",
    "print(\"\\nTarget Tensor (1=similar, -1=dissimilar):\\n\", target)\n",
    "print(\"\\nCosine Embedding Loss:\\n\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Logits):\n",
      " tensor([[ 2.0000,  1.0000,  0.1000, -0.1000,  0.5000],\n",
      "        [-1.0000,  3.0000,  2.5000,  0.0000, -0.5000],\n",
      "        [ 0.2000,  0.8000,  0.5000,  1.0000,  2.0000]])\n",
      "\n",
      "Target Tensor (Class Indices):\n",
      " tensor([0, 1, 4])\n",
      "\n",
      "Cross-Entropy Loss:\n",
      " tensor(0.6257)\n"
     ]
    }
   ],
   "source": [
    "# cross_entropy\n",
    "# function computes the cross-entropy loss between unnormalized logits input, and target labels. \n",
    "# commonly used in mult-class classification tasks\n",
    "\n",
    "input_tensor = torch.tensor([\n",
    "    [2.0,1.0,0.1,-0.1,0.5],\n",
    "    [-1.0,3.0,2.5,0.0,-0.5],\n",
    "    [0.2,0.8,0.5,1.0,2.0]\n",
    "])\n",
    "\n",
    "target_tensor = torch.tensor([0,1,4])\n",
    "\n",
    "class_weights = torch.tensor([1.0,1.0,1.0,1.0,1.0])\n",
    "\n",
    "loss = F.cross_entropy(\n",
    "    input=input_tensor,\n",
    "    target=target_tensor,\n",
    "    weight=class_weights,\n",
    "    reduction='mean',\n",
    "    label_smoothing=0.0\n",
    ")\n",
    "\n",
    "print(\"Input Tensor (Logits):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Class Indices):\\n\", target_tensor)\n",
    "print(\"\\nCross-Entropy Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probabilities (log_probs):\n",
      " tensor([[[-1.5283, -3.0880, -1.9405, -1.2894, -1.7417, -1.9434],\n",
      "         [-0.9785, -2.3023, -1.2762, -1.9838, -3.4500, -2.5811]],\n",
      "\n",
      "        [[-1.9167, -1.0028, -3.1757, -2.0431, -1.6209, -2.1460],\n",
      "         [-2.0156, -1.6047, -3.5327, -2.3179, -1.0484, -1.6735]],\n",
      "\n",
      "        [[-2.3723, -2.3309, -0.9706, -1.7361, -1.5490, -3.1697],\n",
      "         [-0.8324, -3.2492, -4.7505, -2.7841, -2.9472, -0.9081]],\n",
      "\n",
      "        [[-1.1662, -1.9477, -2.7091, -2.9340, -0.9358, -3.3879],\n",
      "         [-1.4907, -1.3069, -3.4306, -1.5904, -3.9991, -1.3880]],\n",
      "\n",
      "        [[-2.3020, -0.5954, -2.0446, -2.5151, -3.4745, -2.2316],\n",
      "         [-3.3717, -2.6175, -1.2592, -2.7947, -0.6716, -3.3022]]])\n",
      "\n",
      "Targets:\n",
      " tensor([[1, 2, 3],\n",
      "        [2, 3, 4]])\n",
      "\n",
      "Input Lengths:\n",
      " tensor([5, 4])\n",
      "\n",
      "Target Lengths:\n",
      " tensor([3, 2])\n",
      "\n",
      "CTC Loss:\n",
      " tensor(2.2098)\n"
     ]
    }
   ],
   "source": [
    "#cts_loss\n",
    "#function computes Connectionist Temporal Clasification CTC loss\n",
    "# which is commonly used in sequencet-to-sequence task like speech recognition and hadwriting recognition\n",
    "# CTF handles variable-lenth input output alignments and oes not require pre-aligned data\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define log probabilites (log_probs)\n",
    "#shape (t=5, n=2, c=6), where\n",
    "# t = input seuqnce length\n",
    "# N _batch size\n",
    "# c = number of characaters in the alphabet (including the blank token)\n",
    "log_probs = torch.randn(5,2,6).log_softmax(dim=2) #log probabilities for each time step and batch \n",
    "\n",
    "#define target sequences \n",
    "#sahpe : (n=2, s=3), where S is the maximum target sequence length\n",
    "targets = torch.tensor([[1,2,3],[2,3,4]])\n",
    "\n",
    "#define input lenghts\n",
    "# shape : n=2, lengths of input sequences for each sample in the batch\n",
    "input_lengths = torch.tensor([5,4])\n",
    "\n",
    "#define input lengths\n",
    "#shape : n=2, lengths of target sequences for each sample in the batch\n",
    "target_lengths = torch.tensor([3,2])\n",
    "\n",
    "#compute CTC loss with default reduction='mean'\n",
    "loss = F.ctc_loss(\n",
    "    log_probs=log_probs,\n",
    "    targets=targets,\n",
    "    input_lengths=input_lengths,\n",
    "    target_lengths=target_lengths,\n",
    "    blank=0, #blank label index\n",
    "    reduction='mean', #average the loss across samples\n",
    "    zero_infinity=False #do not zero infinite losses\n",
    ")\n",
    "\n",
    "print(\"Log Probabilities (log_probs):\\n\", log_probs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "print(\"\\nInput Lengths:\\n\", input_lengths)\n",
    "print(\"\\nTarget Lengths:\\n\", target_lengths)\n",
    "print(\"\\nCTC Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Predicted Means):\n",
      " tensor([2., 5., 1.])\n",
      "\n",
      "Target Tensor (Observed Values):\n",
      " tensor([0, 1, 4])\n",
      "\n",
      "Variance Tensor (Predicted Variances):\n",
      " tensor([0.5000, 0.3000, 0.2000])\n",
      "\n",
      "Gaussian Negative Log-Likelihood Loss:\n",
      " tensor(17.1378)\n"
     ]
    }
   ],
   "source": [
    "#gaussian_nll_loss\n",
    "#computes teh gaussian negative log-likelihoood (nll) loss which is commmonly used in probabilitisc regression tasks where both the mean input abd variance of a Gaussian distribution are predicted \n",
    "# evaluates how well the predicted gaussian distribution matcehs the observer target.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define the input tensor (predicted mean of the gaussiain distribution)\n",
    "#shape representing the predicted means\n",
    "input_tensor = torch.tensor([2.0,5.0,1.0]) #predicted means\n",
    "\n",
    "#define the variance tensor (predicted variance of the gaussian distribution)\n",
    "#shape: (batch_size=3), representing the predicted variances \n",
    "variance_tensor = torch.tensor([0.5,0.3,0.2]) #predicted variances\n",
    "\n",
    "#parameter for gaussian nll loss\n",
    "full = False #do not include the constant term in the loss calculation\n",
    "eps = 1e-6 #small vaue added to variance for numerical stability\n",
    "reduction = 'mean' #average the loss across samples\n",
    "\n",
    "#commpute gaussian nll loss\n",
    "loss = F.gaussian_nll_loss(\n",
    "    input=input_tensor,\n",
    "    target=target_tensor,\n",
    "    var=variance_tensor,\n",
    "    full=full,\n",
    "    eps=eps,\n",
    "    reduction=reduction\n",
    ")\n",
    "# Print the results\n",
    "print(\"Input Tensor (Predicted Means):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Observed Values):\\n\", target_tensor)\n",
    "print(\"\\nVariance Tensor (Predicted Variances):\\n\", variance_tensor)\n",
    "print(\"\\nGaussian Negative Log-Likelihood Loss:\\n\", loss)\n",
    "\n",
    "#default reduction\n",
    "    #use the deafult reduction 'mean' to compute the average loss\n",
    "#sum reduction\n",
    "    # use 'sum' to compute the total loss\n",
    "#no reduction\n",
    "    #  use 'none' to get the loss for each sample individually\n",
    "#include constant term\n",
    "    # set full=True to include the constant term in the loss calculation\n",
    "#honosdastic variance \n",
    "    #use a scalar value if itis the same for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Similarity/Distances):\n",
      " tensor([ 0.9000, -0.4000,  1.2000, -0.7000])\n",
      "\n",
      "Target Tensor (1=similar, -1=dissimilar):\n",
      " tensor([ 1, -1,  1, -1])\n",
      "\n",
      "Hinge Embedding Loss:\n",
      " tensor(1.3000)\n"
     ]
    }
   ],
   "source": [
    "#hinge embedding loss\n",
    "# used in tasks like metric learning and similarity-based models\n",
    "# encourage similar pairs of embeddings to have a small distance and dissimilar pairs to have a large distance\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define input tensor (pairwise distancs or similarities)\n",
    "input_tensor = torch.tensor([0.9,-0.4,1.2,-0.7]) #similarity distance values\n",
    "\n",
    "#define the target tensor(indicating whether the pairs are similar or dissimilar)\n",
    "# 1 means similar, -1 means dissimilar\n",
    "target_tensor = torch.tensor([1,-1,1,-1])\n",
    "\n",
    "#parameters for hinge embedding loss\n",
    "margin =  1.0 # margin for dissimilar pairs\n",
    "reduction = 'mean'  #average the loss across samples\n",
    "\n",
    "#compute for hinge embedding loss\n",
    "loss = F.hinge_embedding_loss(\n",
    "    input=input_tensor,\n",
    "    target=target_tensor,\n",
    "    margin=margin,\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "print(\"Input Tensor (Similarity/Distances):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (1=similar, -1=dissimilar):\\n\", target_tensor)\n",
    "print(\"\\nHinge Embedding Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Log-Probabilities):\n",
      " tensor([[-1.4076, -2.4076, -0.4076],\n",
      "        [-3.1698, -0.1698, -2.1698]])\n",
      "\n",
      "Target Tensor (Probabilities):\n",
      " tensor([[0.2546, 0.4640, 0.2814],\n",
      "        [0.3514, 0.3883, 0.2603]])\n",
      "\n",
      "KL Divergence Loss:\n",
      " tensor(0.5941)\n"
     ]
    }
   ],
   "source": [
    "#kl_div\n",
    "#computes Kullback-Leibler (KL) divergence loss\n",
    "# which measures difference between two probability distribution\n",
    "# commonly used in task like generative modelling, reinforcement learning and probabilitic modelling\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define the input tensor (log-probabilities)\n",
    "#log probabilities for each class\n",
    "input_tensor = torch.log_softmax(torch.tensor([[2.0,1.0,3.0],[1.0,4.0,2.0]]),dim=1)\n",
    "\n",
    "#define the target tensor (target probabilities)\n",
    "target_tensor = torch.softmax(torch.tensor([[0.1,0.7,0.2],[0.4,0.5,0.1]]),dim=1)\n",
    "\n",
    "#parameters for KL divergence loss\n",
    "reduction = 'batchmean' #alligns with mathematical definition\n",
    "log_target = False #target is not in log space\n",
    "\n",
    "#compute KL divergence loss\n",
    "loss = F.kl_div(\n",
    "    input=input_tensor,\n",
    "    target=target_tensor,\n",
    "    reduction=reduction,\n",
    "    log_target=log_target\n",
    ")\n",
    "\n",
    "print(\"Input Tensor (Log-Probabilities):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Probabilities):\\n\", target_tensor)\n",
    "print(\"\\nKL Divergence Loss:\\n\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Predictions):\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "\n",
      "Target Tensor (Ground Truth):\n",
      " tensor([[1.5000, 2.5000],\n",
      "        [3.5000, 4.5000],\n",
      "        [5.5000, 6.5000]])\n",
      "\n",
      "L1 Loss:\n",
      " tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "#l1_loss \n",
    "#computes the L1 oss, mean absolute difference between the predicted values (input) and the ground turth values.\n",
    "# it is commonly used in regression tasks where robustness to outliers is desired\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define the input tensor (predicted values)\n",
    "#represent the prediction\n",
    "input_tensor = torch.tensor([[1.0,2.0],[3.0,4.0],[5.0,6.0]]) #predicted values\n",
    "\n",
    "#define the target tensor (ground truth values)\n",
    "#representing ground truth\n",
    "target_tensor = torch.tensor([[1.5,2.5],[3.5,4.5],[5.5,6.5]]) #ground truth values\n",
    "\n",
    "#parameters for L1 loss\n",
    "reduction = 'mean' #average the loss across all elements \n",
    "\n",
    "#compute L1 loss\n",
    "loss = F.l1_loss(\n",
    "    input = input_tensor,\n",
    "    target=target_tensor,\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "print(\"Input Tensor (Predictions):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Ground Truth):\\n\", target_tensor)\n",
    "print(\"\\nL1 Loss:\\n\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Predictions):\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "\n",
      "Target Tensor (Ground Truth):\n",
      " tensor([[1.5000, 2.5000],\n",
      "        [3.5000, 4.5000],\n",
      "        [5.5000, 6.5000]])\n",
      "\n",
      "Mean Squared Error Loss:\n",
      " tensor(0.2500)\n"
     ]
    }
   ],
   "source": [
    "#mse loss\n",
    "# MSE loss which measures the average squared difference between predicted values and ground truth values. it is commonly used in used in regression tasks\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "#define the input (predicted values)\n",
    "input_tensor = torch.tensor([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "\n",
    "#define the target tensor (groun turth values)\n",
    "#shape (batch_size=3, num_features=2), representing ground truth\n",
    "target_tensor = torch.tensor([[1.5,2.5],[3.5,4.5],[5.5,6.5]])\n",
    "\n",
    "#optional : define weights for each sample\n",
    "weights = torch.tensor([1.0,2.0,1.0]) #weights for each sample in the batch\n",
    "\n",
    "#parameters for mse loss\n",
    "reduction = 'mean'\n",
    "\n",
    "#compute mse loss\n",
    "loss = F.mse_loss(\n",
    "    input = input_tensor,\n",
    "    target=target_tensor,\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Tensor (Predictions):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Ground Truth):\\n\", target_tensor)\n",
    "print(\"\\nMean Squared Error Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor 1 (Scores for Item 1):\n",
      " tensor([0.9000, 0.6000, 0.8000])\n",
      "\n",
      "Input Tensor 2 (Scores for Item 2):\n",
      " tensor([0.7000, 0.8000, 0.5000])\n",
      "\n",
      "Target Tensor (Ranking Preferences):\n",
      " tensor([ 1, -1,  1])\n",
      "\n",
      "Margin Ranking Loss:\n",
      " tensor(0.2667)\n"
     ]
    }
   ],
   "source": [
    "#margin_ranking_loss\n",
    "# computes margin ranking loss, which is commonly used in tasks like learning-to-rank or paiwise ranking.\n",
    "# encourages one input to be ranked higher than another input based on a target value\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define the input tensors (scores for two items)\n",
    "input1 = torch.tensor([0.9,0.6,0.8]) #scores for item 1\n",
    "input2 = torch.tensor([0.7,0.8,0.5]) #scores for item 2\n",
    "\n",
    "#define the target tensor (indication the drsired ranking)\n",
    "# 1 means input1 should rank higher than input2 , -1 means input2 should rank higher than 1input1\n",
    "target = torch.tensor([1,-1,1]) #ranking preferences\n",
    "\n",
    "#parameters for maring ranking loss \n",
    "margin = 0.5 #margin by which the diffence between scores must exceed\n",
    "reduction = 'mean' #average the loss across samples\n",
    "\n",
    "# compute margin ranking loss\n",
    "loss = F.margin_ranking_loss(\n",
    "    input1=input1,\n",
    "    input2=input2,\n",
    "    target=target,\n",
    "    margin=margin,\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Tensor 1 (Scores for Item 1):\\n\", input1)\n",
    "print(\"\\nInput Tensor 2 (Scores for Item 2):\\n\", input2)\n",
    "print(\"\\nTarget Tensor (Ranking Preferences):\\n\", target)\n",
    "print(\"\\nMargin Ranking Loss:\\n\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Raw Scores):\n",
      " tensor([[ 0.1000,  0.2000,  0.4000, -0.1000, -0.3000],\n",
      "        [-0.5000,  0.8000,  0.6000,  0.3000, -0.2000]])\n",
      "\n",
      "Target Tensor (Correct Labels with Padding):\n",
      " tensor([[ 0,  2, -1, -1, -1],\n",
      "        [ 1,  2, -1, -1, -1]])\n",
      "\n",
      "Multilabel Margin Loss:\n",
      " tensor(0.5500)\n"
     ]
    }
   ],
   "source": [
    "#margin_loss\n",
    "#comput the multilable margin loss\n",
    "# commonly used in multilabel classification tasks\n",
    "# it encourages the correct labels to have higher scores than inccorrect labels by at least a margin of 1\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define the input tensor (raw scores for each class)\n",
    "input_tensor = torch.tensor([\n",
    "    [0.1,0.2,0.4,-0.1,-0.3], # raw scores for sample 1\n",
    "    [-0.5,0.8,0.6,0.3,-0.2]  # raw scores for sample 2\n",
    "])\n",
    "\n",
    "#define the target tensor (ground truth labels)\n",
    "#shape : (batch_size=2, num_classes=5), where:\n",
    "# - positive integers represent the indices of correct labels.\n",
    "# - negative integer sare used to pad the target tensor.\n",
    "target_tensor = torch.tensor([\n",
    "    [0,2,-1,-1,-1], # correct labels for sample 1 are classes 0 and 2\n",
    "    [1,2,-1,-1,-1] # correct labels for sample 1 are classes 1 and 2\n",
    "])\n",
    "\n",
    "# parameters for multilabel margin loss\n",
    "reduction = 'mean'\n",
    "\n",
    "# compute multilabel margin loss\n",
    "loss = F.multilabel_margin_loss(\n",
    "    input=input_tensor,\n",
    "    target=target_tensor,\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Tensor (Raw Scores):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Correct Labels with Padding):\\n\", target_tensor)\n",
    "print(\"\\nMultilabel Margin Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (Logits):\n",
      " tensor([[ 0.1000,  0.2000,  0.4000, -0.1000,  0.3000],\n",
      "        [-0.5000,  0.8000,  0.6000,  0.3000, -0.2000]])\n",
      "\n",
      "Target Tensor (Ground Truth Labels):\n",
      " tensor([[1, 0, 1, 0, 1],\n",
      "        [0, 1, 1, 0, 0]])\n",
      "\n",
      "MultiLabel Soft Margin Loss:\n",
      " tensor(0.5889)\n"
     ]
    }
   ],
   "source": [
    "#multilabel_soft_margin_loss\n",
    "# computes the multi-label soft margin loss\n",
    "# which is used for mult-label classifation tasks it applies a sigmoid actiation to the input logits and then computes the binary cross-entropy loss for each class independently\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "#define the input tensor (logits for each class)\n",
    "input_tensor = torch.tensor([\n",
    "    [0.1,0.2,0.4,-0.1,0.3], #logits for sample 1\n",
    "    [-0.5,0.8,0.6,0.3,-0.2]  #logits for sample 2\n",
    "])\n",
    "\n",
    "#define the target tensor (groud turh labes)\n",
    "# shape : (batch_size=2, num_classes=5), where:\n",
    "# -1 indidicates the presence of a label\n",
    "# -0 indiicates teh absence of a label\n",
    "\n",
    "target_tensor = torch.tensor([\n",
    "    [1,0,1,0,1], # labels for sample 1\n",
    "    [0,1,1,0,0] # labels for sample 2\n",
    "])\n",
    "\n",
    "# parameters for multilabel soft margin loss\n",
    "weight = None # optional : class weights (none means equal weight for all classes)\n",
    "reduction = 'mean' #average the loss across samples \n",
    "\n",
    "#compute multilabel soft margin loss\n",
    "loss = F.multilabel_soft_margin_loss(\n",
    "    input=input_tensor,\n",
    "    target=target_tensor,\n",
    "    weight=weight,\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Tensor (Logits):\\n\", input_tensor)\n",
    "print(\"\\nTarget Tensor (Ground Truth Labels):\\n\", target_tensor)\n",
    "print(\"\\nMultiLabel Soft Margin Loss:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-margin loss: 0.4000000059604645\n"
     ]
    }
   ],
   "source": [
    "## multi_margin_loss\n",
    "\n",
    "# function computes the multi-class margin loss which is commonly used in classificationt tasks. it measures teh difference betweent the predicted scores for the correct class adn the cores for other classes , penalizing predicitons where the maring betweent the correct class the scores for other classes ,\n",
    "# penalizing predictions where the margin between the correct class and incorrect classes is too small\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input = torch.tensor([[0.2,0.5,0.1,0.4,0.8],[0.7,0.3,0.9,0.2,0.4],[0.1,0.6,0.3,0.8,0.2]])\n",
    "\n",
    "target = torch.tensor([4,2,3])\n",
    "\n",
    "loss = F.multi_margin_loss(input, target, p=1,margin=1.0,reduction='mean')\n",
    "\n",
    "print('multi-margin loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-margin loss: 0.5333333611488342\n"
     ]
    }
   ],
   "source": [
    "#nll_loss\n",
    "# computes the nagative log likelihood loss\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input = torch.tensor([[ -0.5,  -1.2,  -2.1,  -0.8,  -1.7],  # Log-probs for sample 1\n",
    "                      [ -1.3,  -0.4,  -1.1,  -1.8,  -2.0],  # Log-probs for sample 2\n",
    "                      [ -1.9,  -1.6,  -0.7,  -1.4,  -1.0]]) # Log-probs for sample 3\n",
    "\n",
    "target = torch.tensor([0, 1, 2])  # Correct class indices\n",
    "\n",
    "loss = F.nll_loss(input, target,reduction='mean')\n",
    "\n",
    "print('multi-margin loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huber loss: 0.4424999952316284\n"
     ]
    }
   ],
   "source": [
    "#huber_loss\n",
    "# computes the huber loss , with optional weighting\n",
    "# huber loss is a combination of L1 (absilute error) and L2 (squared error) losses.\n",
    "# it uses the squared term when the absolute error is small (below threshold delta) and switches to a scaled L1 term when the error is alrge\n",
    "# this makes it robust to outliers while still being smooth near zero\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input = torch.tensor([1.0,2.0,3.0,4.0])\n",
    "target = torch.tensor([1.5,1.8,3.5,6.0])\n",
    "loss = F.huber_loss(input,target,reduction='mean',delta=1.0)\n",
    "print('huber loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smooth l1 loss: 0.4424999952316284\n"
     ]
    }
   ],
   "source": [
    "#smooth_l1_loss \n",
    "# is a combination of L1 (absolute error) and L2 (squared error) losses. it uses the squared term when the absolute error is small\n",
    "# and switches to the L1 term when the error is large. this makes it robust to outliers while still ebing smooth near zero\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "input = torch.tensor([1.0,2.0,3.0,4.0])\n",
    "target = torch.tensor([1.5,1.8,3.5,6.0])\n",
    "loss = F.smooth_l1_loss(input,target,reduction='mean',beta=1.0)\n",
    "print('smooth l1 loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft margin loss: 0.05709948018193245\n"
     ]
    }
   ],
   "source": [
    "#soft margin loss\n",
    "#used for binary classification where target label either 1 or -1\n",
    "# computes the logistic loss between the predicted values and the ground turth labels\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "input = torch.tensor([1.0,2.0,3.0,4.0])\n",
    "target = torch.tensor([1.5,1.8,3.5,6.0])\n",
    "loss = F.soft_margin_loss(input,target,reduction='mean')\n",
    "print('soft margin loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Margin Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# triplet margin loss is commonly used in tasks like metric learning where the goal is to learn embeddings such that similar items are closed together than dissimilar items (negative pairs)\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "anchor = torch.tensor([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]])\n",
    "positive = torch.tensor([[1.1, 2.1, 3.1, 4.1], [5.1, 6.1, 7.1, 8.1]])\n",
    "negative = torch.tensor([[2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0]])\n",
    "\n",
    "# Compute the Triplet Margin Loss with margin=1.0 and p=2 (L2 norm)\n",
    "loss = F.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, reduction='mean')\n",
    "\n",
    "print(\"Triplet Margin Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Margin Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "#triplet_margin_loss\n",
    "# same to triplet_margin_loss but it allows you to specify a custom distance function\n",
    "# this flexivility makes it useful when you want to compute distances in ways other than the defaule l2 norm\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "anchor = torch.tensor([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]])\n",
    "positive = torch.tensor([[1.1, 2.1, 3.1, 4.1], [5.1, 6.1, 7.1, 8.1]])\n",
    "negative = torch.tensor([[2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0]])\n",
    "\n",
    "def manhattan_distance(x1,x2):\n",
    "    return torch.sum(torch.abs(x1- x2), dim=1)\n",
    "\n",
    "\n",
    "# Compute the Triplet Margin Loss with margin=1.0 and p=2 (L2 norm)\n",
    "loss = F.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=manhattan_distance,margin=1.0, reduction='mean')\n",
    "\n",
    "\n",
    "print(\"Triplet Margin Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplet margin with distance loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "#triplet_margin_with_distance_loss\n",
    "#use in tasks like metric learning, where the egoal is to laern embeddings such that similar items are closer together than disimalr items\n",
    "#allow specification of custom distance function\n",
    "# which provides flexibility beyonf the default L2 norm\n",
    "\n",
    "\"\"\"d(A,) -d(A,N) + margin <= 0\n",
    "* A is the anchor embedding\n",
    "* P is the positive embedding (similar to the anchor)\n",
    "* N is the negative embeedding (dissimilar to the anchor)\n",
    "* d(.,.) is the distance function\n",
    "* margin is a hyperparameter that defines the minium distance between the positive and negative pairs\n",
    "\"\"\"\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "anchor = torch.tensor([[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0]])\n",
    "positive = torch.tensor([[1.1,2.1,3.1,4.1],[5.1,6.1,7.1,8.1]])\n",
    "negative = torch.tensor([[2.0,3.0,4.0,5.0],[6.0,7.0,8.0,9.0]])\n",
    "\n",
    "def manhattan_distancee(x1,x2):\n",
    "    return torch.sum(torch.abs(x1 - x2), dim=-1)\n",
    "\n",
    "loss = F.triplet_margin_with_distance_loss(\n",
    "    anchor, positive, negative, \n",
    "    distance_function = manhattan_distance,\n",
    "    margin=1.0,\n",
    "    reduction = 'mean'\n",
    ")\n",
    "\n",
    "print('triplet margin with distance loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:\n",
      " torch.Size([1, 9, 4, 4])\n",
      "output shape:\n",
      " torch.Size([1, 1, 12, 12])\n",
      "unshuffle output shape:\n",
      " torch.Size([1, 9, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "#pixel_shuffle\n",
    "\n",
    "\"\"\"\n",
    "* rearranges element in a tensor to increat its spatial resolution by a factor called upscale_factor\n",
    "* commonly used in tasks like super-resolution where low-resolution images are  upscaled to higher resolutions\n",
    "(*, Cx r2, H, W)\n",
    "* is any number in batch dimensions\n",
    "C nuber of channels\n",
    "r upscale factor\n",
    "H is height \n",
    "W is width\n",
    "\n",
    "rerrange elements into a tensor of shape. this increase spatial resolution )height and width by a faactor of r, while reducing the channel dimenson by r2\n",
    "\"\"\"\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "input = torch.randn(1,9,4,4)\n",
    "output = F.pixel_shuffle(input, upscale_factor=3)\n",
    "print('input shape:\\n',input.shape)\n",
    "print('output shape:\\n',output.shape)\n",
    "\n",
    "# height and width increase by a factor of 3, 4x2 = 12\n",
    "# channels reduced by r2 , 3^2, 9/9 =1\n",
    "\n",
    "#pixel_unshuffle\n",
    "# reverse operation of pixel shuffle, rearrange elements in a tensor to decrease spatial resolutioon by downsale factor,\n",
    "# while incrase the channel dimension\n",
    "# commonly used in tasks like downsampling or reversing super resolution operations\n",
    "output = F.pixel_unshuffle(output, downscale_factor=3)\n",
    "print('unshuffle output shape:\\n',output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor: tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11]]]])\n",
      "tensor([[[[ 0,  0,  1,  2,  3,  0,  0],\n",
      "          [ 0,  4,  5,  6,  7,  0,  0],\n",
      "          [ 0,  8,  9, 10, 11,  0,  0]]]])\n",
      "shape: torch.Size([1, 1, 3, 7])\n",
      "\n",
      "padded tensor (height and width):\n",
      "tensor([[[[-1, -1, -1, -1, -1, -1, -1],\n",
      "          [-1, -1, -1, -1, -1, -1, -1],\n",
      "          [-1, -1, -1, -1, -1, -1, -1],\n",
      "          [-1,  0,  1,  2,  3, -1, -1],\n",
      "          [-1,  4,  5,  6,  7, -1, -1],\n",
      "          [-1,  8,  9, 10, 11, -1, -1],\n",
      "          [-1, -1, -1, -1, -1, -1, -1],\n",
      "          [-1, -1, -1, -1, -1, -1, -1],\n",
      "          [-1, -1, -1, -1, -1, -1, -1],\n",
      "          [-1, -1, -1, -1, -1, -1, -1]]]])\n",
      "shape torch.Size([1, 1, 10, 7])\n",
      "\n",
      "padded tensor (height and width):\n",
      "tensor([[[[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99,  0,  1,  2,  3, 99, 99],\n",
      "          [99,  4,  5,  6,  7, 99, 99],\n",
      "          [99,  8,  9, 10, 11, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]],\n",
      "\n",
      "         [[99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99],\n",
      "          [99, 99, 99, 99, 99, 99, 99]]]])\n",
      "shape torch.Size([1, 12, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "#pad \n",
    "# adds padding to a tensor along specified dimensions.\n",
    "# padding can be applied symmetrically or asymmetrically\n",
    "# depending on the values provided in the pad tuple\n",
    "# the padding size of described starting from the last dimension and moving forward \n",
    "\n",
    "# pad , specify padding size for each dimension since tuple has the form for 1d padding , 2d padding , 3d padding\n",
    "\n",
    "# mode\n",
    "# constant pads with contast value\n",
    "# reflect, pads with reflection of the input \n",
    "# replicate pads by replicating the edge values of the input\n",
    "# circular pads with the circular repeition of the input \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "t4d = torch.arange(12).reshape(1,1,3,4)\n",
    "\n",
    "print('original tensor:',t4d)\n",
    "\n",
    "#pad the last dimensioon (width) by (1,2)\n",
    "p1d = (1,2) #leff = 1 right =2\n",
    "out1 = F.pad(t4d, p1d, mode='constant', value=0)\n",
    "print(out1)\n",
    "print('shape:',out1.shape)\n",
    "\n",
    "#pad the last two dinemsion (height, width)\n",
    "p2d = (1,2,3,4) #pad width=(1,2), height=(3,4)\n",
    "out2 = F.pad(t4d, p2d, mode='constant', value=-1)\n",
    "print('\\npadded tensor (height and width):')\n",
    "print(out2)\n",
    "print('shape',out2.shape)\n",
    "\n",
    "#pad the last three dinemsion (depth, height, width)\n",
    "p3d = (1,2,3,4,5,6) #pad width=(1,2), height=(3,4) depth=(5,6)\n",
    "out3 = F.pad(t4d, p3d, mode='constant', value=99)\n",
    "print('\\npadded tensor (height and width):')\n",
    "print(out3)\n",
    "print('shape',out3.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor:\n",
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "shape: torch.Size([1, 1, 4, 4])\n",
      "\n",
      "unsampled tensor (bilinear):\n",
      "tensor([[[[ 1.0000,  1.2500,  1.7500,  2.2500,  2.7500,  3.2500,  3.7500,\n",
      "            4.0000],\n",
      "          [ 2.0000,  2.2500,  2.7500,  3.2500,  3.7500,  4.2500,  4.7500,\n",
      "            5.0000],\n",
      "          [ 4.0000,  4.2500,  4.7500,  5.2500,  5.7500,  6.2500,  6.7500,\n",
      "            7.0000],\n",
      "          [ 6.0000,  6.2500,  6.7500,  7.2500,  7.7500,  8.2500,  8.7500,\n",
      "            9.0000],\n",
      "          [ 8.0000,  8.2500,  8.7500,  9.2500,  9.7500, 10.2500, 10.7500,\n",
      "           11.0000],\n",
      "          [10.0000, 10.2500, 10.7500, 11.2500, 11.7500, 12.2500, 12.7500,\n",
      "           13.0000],\n",
      "          [12.0000, 12.2500, 12.7500, 13.2500, 13.7500, 14.2500, 14.7500,\n",
      "           15.0000],\n",
      "          [13.0000, 13.2500, 13.7500, 14.2500, 14.7500, 15.2500, 15.7500,\n",
      "           16.0000]]]])\n",
      "shape: tensor([[[[ 1.0000,  1.2500,  1.7500,  2.2500,  2.7500,  3.2500,  3.7500,\n",
      "            4.0000],\n",
      "          [ 2.0000,  2.2500,  2.7500,  3.2500,  3.7500,  4.2500,  4.7500,\n",
      "            5.0000],\n",
      "          [ 4.0000,  4.2500,  4.7500,  5.2500,  5.7500,  6.2500,  6.7500,\n",
      "            7.0000],\n",
      "          [ 6.0000,  6.2500,  6.7500,  7.2500,  7.7500,  8.2500,  8.7500,\n",
      "            9.0000],\n",
      "          [ 8.0000,  8.2500,  8.7500,  9.2500,  9.7500, 10.2500, 10.7500,\n",
      "           11.0000],\n",
      "          [10.0000, 10.2500, 10.7500, 11.2500, 11.7500, 12.2500, 12.7500,\n",
      "           13.0000],\n",
      "          [12.0000, 12.2500, 12.7500, 13.2500, 13.7500, 14.2500, 14.7500,\n",
      "           15.0000],\n",
      "          [13.0000, 13.2500, 13.7500, 14.2500, 14.7500, 15.2500, 15.7500,\n",
      "           16.0000]]]])\n",
      "\n",
      "downsampled tensor (area):\n",
      "tensor([[[[ 3.5000,  5.5000],\n",
      "          [11.5000, 13.5000]]]])\n",
      "shape: torch.Size([1, 1, 2, 2])\n",
      "\n",
      "unsamppled tensor (nearest):\n",
      "tensor([[[[ 1.,  1.,  1.,  2.,  2.,  2.,  3.,  3.,  3.,  4.,  4.,  4.],\n",
      "          [ 1.,  1.,  1.,  2.,  2.,  2.,  3.,  3.,  3.,  4.,  4.,  4.],\n",
      "          [ 1.,  1.,  1.,  2.,  2.,  2.,  3.,  3.,  3.,  4.,  4.,  4.],\n",
      "          [ 5.,  5.,  5.,  6.,  6.,  6.,  7.,  7.,  7.,  8.,  8.,  8.],\n",
      "          [ 5.,  5.,  5.,  6.,  6.,  6.,  7.,  7.,  7.,  8.,  8.,  8.],\n",
      "          [ 5.,  5.,  5.,  6.,  6.,  6.,  7.,  7.,  7.,  8.,  8.,  8.],\n",
      "          [ 9.,  9.,  9., 10., 10., 10., 11., 11., 11., 12., 12., 12.],\n",
      "          [ 9.,  9.,  9., 10., 10., 10., 11., 11., 11., 12., 12., 12.],\n",
      "          [ 9.,  9.,  9., 10., 10., 10., 11., 11., 11., 12., 12., 12.],\n",
      "          [13., 13., 13., 14., 14., 14., 15., 15., 15., 16., 16., 16.],\n",
      "          [13., 13., 13., 14., 14., 14., 15., 15., 15., 16., 16., 16.],\n",
      "          [13., 13., 13., 14., 14., 14., 15., 15., 15., 16., 16., 16.]]]])\n",
      "shape: torch.Size([1, 1, 12, 12])\n",
      "\n",
      "downsampled tensor (bicubic with anti-aliasing):\n",
      "tensor([[[[ 3.9339,  5.7603],\n",
      "          [11.2397, 13.0661]]]])\n",
      "shape: torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "#interpolate\n",
    "\n",
    "\"\"\" \n",
    "The interpolate function is used to resize tensors by either specifying the target size (size) or a scaling factor (scale_factor). It supports various interpolation modes for 1D, 2D, and 3D data. The most common use case is resizing spatial dimensions (e.g., height and width) for images or volumetric data.\n",
    "\n",
    "Key Parameters\n",
    "input : The input tensor to resize.\n",
    "size : The target output size for the spatial dimensions (height, width, depth). Can be an integer or a tuple.\n",
    "scale_factor : A multiplier for the spatial dimensions. Can be a float or a tuple.\n",
    "mode : Specifies the interpolation algorithm:\n",
    "'nearest': Nearest neighbor interpolation.\n",
    "'bilinear': Bilinear interpolation (2D only).\n",
    "'bicubic': Bicubic interpolation (2D only).\n",
    "'trilinear': Trilinear interpolation (3D only).\n",
    "'area': Area-based interpolation (downsampling only).\n",
    "'nearest-exact': Exact nearest neighbor interpolation.\n",
    "align_corners : Controls alignment of corner pixels for interpolation modes like 'bilinear' and 'bicubic'. Default is False.\n",
    "antialias : Applies anti-aliasing when downsampling. Supported modes are 'bilinear' and 'bicubic'.\n",
    "\"\"\"\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "input = torch.arange(1,17,dtype=torch.float32).reshape(1,1,4,4)\n",
    "\n",
    "print('original tensor:')\n",
    "print(input)\n",
    "print('shape:',input.shape)\n",
    "\n",
    "#using sampe scacle_factor with 'bilinear' mode\n",
    "output_bilinear = F.interpolate(input, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "print('\\nunsampled tensor (bilinear):')\n",
    "print(output_bilinear)\n",
    "print('shape:',output_bilinear)\n",
    "\n",
    "#downsample using size  with 'areaa' mode\n",
    "output_area = F.interpolate(input, size=(2,2), mode='area')\n",
    "print('\\ndownsampled tensor (area):')\n",
    "print(output_area)\n",
    "print('shape:', output_area.shape)\n",
    "\n",
    "#unsample using scale-factor with 'nearest' mode\n",
    "output_nearest = F.interpolate(input, scale_factor=3, mode='nearest' )\n",
    "print('\\nunsamppled tensor (nearest):')\n",
    "print(output_nearest)\n",
    "print('shape:',output_nearest.shape)\n",
    "\n",
    "#apply anti-aliasing with 'bicubic mode for downsampling\n",
    "output_antialias = F.interpolate(input, size=(2,2), mode ='bicubic', align_corners=False, antialias=True)\n",
    "print('\\ndownsampled tensor (bicubic with anti-aliasing):')\n",
    "print(output_antialias)\n",
    "print('shape:',output_antialias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsample\n",
    "#upsample_nearest\n",
    "#upsample_billinear\n",
    "\n",
    "\"\"\" \n",
    "this function have been replace with interpolate since its deprecated, \n",
    "\n",
    "upsample function resizes the input tensor to a specified size or scale factor using various interpolation algorithms \n",
    "supports upsamping for 1D,2D , 3D data. \n",
    "the behaviour of upsample is identical to interpolate but its deprecaed in favor of interpolate\n",
    "\n",
    "can refer to code above , also for the upsample nearest with mode='nearest'\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "# output_upsample_nearest = F.upsample_nearest(input, scale_factoro=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor:\n",
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "shape: torch.Size([1, 1, 4, 4])\n",
      "\n",
      "sampled output:\n",
      "tensor([[[[0.2500, 1.0000],\n",
      "          [3.2500, 4.0000]]]])\n",
      "shape: torch.Size([1, 1, 2, 2])\n",
      "\n",
      "transformed output:\n",
      "tensor([[[[ 0.4900,  1.5300,  3.1000,  4.9000],\n",
      "          [ 3.4200,  5.5000,  7.3000,  9.1000],\n",
      "          [ 7.9000,  9.7000, 11.5000, 11.8800],\n",
      "          [12.1000, 13.9000, 13.7700,  7.8400]]]])\n",
      "shape: torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "#grids_sample\n",
    "#affine_grid \n",
    "\n",
    "\"\"\" \n",
    "used to sample values from an input tensor at locations specified by a normlized grid.\n",
    "comonly used in tasks like spatioal transformations, image warping and spatial transformer networks (STNs)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "#example input tensor of shape (batch_size, channels , height, width)\n",
    "input = torch.arange(1,17, dtype=torch.float32).reshape(1,1,4,4)\n",
    "\n",
    "\n",
    "print('original tensor:')\n",
    "print(input)\n",
    "print('shape:',input.shape)\n",
    "\n",
    "\n",
    "#create a grid for sampling\n",
    "# the grid specifies normalized coordinates [-1,1] for x and y.\n",
    "# here we create a gird that samples the center of the input\n",
    "\n",
    "grid = torch.tensor([[\n",
    "    [[-1.0,-1.0],[1.0,-1.0]], #top-left and top-right corners\n",
    "    [[-1.0,1.0],[1.0,1.0]] #bottom-left and bottom-right corners\n",
    "]])\n",
    "\n",
    "output = F.grid_sample(input,grid,mode='bilinear',padding_mode='zeros',align_corners=False)\n",
    "print('\\nsampled output:')\n",
    "print(output)\n",
    "print('shape:',output.shape)\n",
    "\n",
    "#define a grid using affine_grid\n",
    "theta = torch.tensor([[[1.0,0.2,0],[0.2,1.0,0]]])\n",
    "\n",
    "#generate a grid usign affine_grid\n",
    "grid = F.affine_grid(theta, input.size(), align_corners=False)\n",
    "\n",
    "#apply grid_sample with bilinear interpolation\n",
    "output = F.grid_sample(input, grid, mode='bilinear',padding_mode='zeros', align_corners=False)\n",
    "\n",
    "#print the transformed output\n",
    "print('\\ntransformed output:')\n",
    "print(output)\n",
    "print('shape:',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is not available. Please run this code on a machine with GPUs",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m     output_device = \u001b[32m0\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mCUDA is not available. Please run this code on a machine with GPUs\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m model = SimpleModel().cuda(device_ids[\u001b[32m0\u001b[39m])\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#create random input data \u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA is not available. Please run this code on a machine with GPUs"
     ]
    }
   ],
   "source": [
    "#parallel.data_parallel\n",
    "\n",
    "\"\"\" \n",
    "function allows you to evaluate a model across multiple GPUs in parallel , splitting the input data across the specified devices and collecting the results oont the output device\n",
    "\n",
    "\n",
    "data_parallel function is functional version of the DataParallel module. it distributes the computation of a model acorss multiple GPUs, making it useful for training or inference with large models or datasets\n",
    "\"\"\"\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_ids = [0,1]\n",
    "    output_device = 0 \n",
    "else:\n",
    "    raise RuntimeError('CUDA is not available. Please run this code on a machine with GPUs')\n",
    "\n",
    "model = SimpleModel().cuda(device_ids[0])\n",
    "\n",
    "#create random input data \n",
    "input_data = torch.randn(16,10).cuda(device_ids[0])\n",
    "\n",
    "#use data_parallel to evaluate the model across multiple GPUs\n",
    "\n",
    "output = nn.parallel.data_parallel(\n",
    "    module = model,\n",
    "    inputs = input_data,\n",
    "    device_ids = device_ids,\n",
    "    output_device = output_device\n",
    ")\n",
    "\n",
    "print('Output:')\n",
    "print(output)\n",
    "print('shape:',output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
