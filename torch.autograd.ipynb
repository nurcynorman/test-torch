{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and set requires_grad=True to track computations\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Define a computation\n",
    "z = x * y  # z = x * y => dz/dx = y, dz/dy = x\n",
    "\n",
    "# Compute gradients by calling .backward()\n",
    "z.backward()\n",
    "\n",
    "# Print the computed gradients\n",
    "print(\"Gradient of z with respect to x:\", x.grad)  # Should be 3.0\n",
    "print(\"Gradient of z with respect to y:\", y.grad)  # Should be 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function output (primal): 10.0\n",
      "Forward gradient (tangent): 7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import forward_ad\n",
    "\n",
    "# Define a simple function\n",
    "def func(x):\n",
    "    return x**2 + 3 * x\n",
    "\n",
    "# Create an input tensor\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Use the context manager to manage dual level automatically\n",
    "with forward_ad.dual_level():\n",
    "    # Create the dual tensor with value and its tangent (seed = 1.0)\n",
    "    dual_x = forward_ad.make_dual(x, torch.tensor(1.0))\n",
    "\n",
    "    # Perform computation using dual tensor\n",
    "    dual_y = func(dual_x)\n",
    "\n",
    "    # Unpack the dual tensor to get primal and tangent\n",
    "    unpacked = forward_ad.unpack_dual(dual_y)\n",
    "\n",
    "    print(f\"Function output (primal): {unpacked.primal.item()}\")\n",
    "    print(f\"Forward gradient (tangent): {unpacked.tangent.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian:\n",
      " tensor([[5., 0., 0.],\n",
      "        [0., 7., 0.],\n",
      "        [0., 0., 9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import functional as F\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2 + 3 * x\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "jac = F.jacobian(f, x)\n",
    "print(\"Jacobian:\\n\", jac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian:\n",
      " tensor([[2., 0., 0.],\n",
      "        [0., 2., 0.],\n",
      "        [0., 0., 2.]])\n"
     ]
    }
   ],
   "source": [
    "def scalar_fn(x):\n",
    "    return (x ** 2).sum()\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "hess = F.hessian(scalar_fn, x)\n",
    "print(\"Hessian:\\n\", hess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JVP:\n",
      " tensor([  3.,   0., -27.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 3\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "v = torch.tensor([1.0, 0.0, -1.0])\n",
    "_, jvp = F.jvp(f, x, v)\n",
    "print(\"JVP:\\n\", jvp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VJP:\n",
      " tensor([ 3.,  0., -7.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 2 + x\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "v = torch.tensor([1.0, 0.0, -1.0])\n",
    "_, vjp = F.vjp(f, x, v)\n",
    "print(\"VJP:\\n\", vjp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVP:\n",
      " tensor([  6.,   0., -18.])\n"
     ]
    }
   ],
   "source": [
    "def scalar_fn(x):\n",
    "    return (x ** 3).sum()\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "v = torch.tensor([1.0, 0.0, -1.0])\n",
    "_, hvp = F.hvp(scalar_fn, x, v)\n",
    "print(\"HVP:\\n\", hvp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VHP:\n",
      " tensor([0.6000, 2.4000, 5.4000])\n"
     ]
    }
   ],
   "source": [
    "def scalar_fn(x):\n",
    "    return (x ** 3).sum()\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "v = torch.tensor([0.1, 0.2, 0.3])\n",
    "_, vhp = F.vhp(scalar_fn, x, v)\n",
    "print(\"VHP:\\n\", vhp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian with lambda wrapper:\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def my_func(x, const, flag=True):\n",
    "    return x * const if flag else x + const\n",
    "\n",
    "const = torch.tensor([2.0, 2.0, 2.0])\n",
    "input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "jac = F.jacobian(lambda x: my_func(x, const, flag=False), input_tensor)\n",
    "print(\"Jacobian with lambda wrapper:\\n\", jac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is leaf: True\n",
      "z is leaf: False\n",
      "Hook called with grad: tensor([4., 6.])\n",
      "Gradients of x: tensor([4., 6.])\n",
      "Gradients of y: tensor([1., 1.])\n",
      "Detached x requires_grad: False\n",
      "In-place detached x is leaf: True\n",
      "Post accumulate hook called with grad: tensor([2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with requires_grad=True so it will track computations\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Leaf tensor with requires_grad=True\n",
    "print(\"Is leaf:\", x.is_leaf)  # True\n",
    "\n",
    "# Perform operations on the tensor\n",
    "y = x * x + 2  # y = x^2 + 2\n",
    "z = y.sum()    # z = sum(y)\n",
    "\n",
    "# z is not a leaf tensor because it's a result of operations\n",
    "print(\"z is leaf:\", z.is_leaf)  # False\n",
    "\n",
    "# Register a hook to see gradients\n",
    "def hook_fn(grad):\n",
    "    print(\"Hook called with grad:\", grad)\n",
    "\n",
    "x.register_hook(hook_fn)\n",
    "\n",
    "# Retain gradient for non-leaf y\n",
    "y.retain_grad()\n",
    "\n",
    "# Perform backpropagation\n",
    "z.backward()  # Computes dz/dx\n",
    "\n",
    "print(\"Gradients of x:\", x.grad)  # dz/dx = 2x → [4.0, 6.0]\n",
    "print(\"Gradients of y:\", y.grad)  # dy/dx = 2x → [4.0, 6.0]\n",
    "\n",
    "# Detach to stop tracking\n",
    "x_detached = x.detach()\n",
    "print(\"Detached x requires_grad:\", x_detached.requires_grad)  # False\n",
    "\n",
    "# In-place detach (rarely used directly)\n",
    "x.detach_()\n",
    "print(\"In-place detached x is leaf:\", x.is_leaf)\n",
    "\n",
    "# Demonstrate register_post_accumulate_grad_hook (requires PyTorch >=1.13+)\n",
    "def post_hook(grad):\n",
    "    print(\"Post accumulate hook called with grad:\", grad)\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "x.retain_grad()\n",
    "x.register_post_accumulate_grad_hook(post_hook)\n",
    "\n",
    "y = (x ** 2).sum()\n",
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@piyushkashyap045/understanding-pytorch-autograd-a-complete-guide-for-deep-learning-practitioners-f5dd1f43b417\n",
    "\n",
    "autograd provide automatic differentiations for tensor operations.\n",
    "\n",
    "its critical for optimization algorithms like gradient descent.\n",
    "\n",
    "we often have to deal with hundreds/thousands of nested functions\n",
    "\n",
    "autgrad builds a directed acyclic graph (DAG) that racks all operations.\n",
    "\n",
    "gradient accumulate by default in pytorch, to prevent this , clear gradietns between backward passes.\n",
    "\n",
    "sometimes you wantt to disable gradient tracking, especially during model evaluations\n",
    "\n",
    "\n",
    "best practices\n",
    "1. clear gradietn regularly, each backward pass during training\n",
    "2. use no_grad for evaluation\n",
    "3. check gradient flow \n",
    "4. memory management , release computaiton graphs when not needed using detach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw: 0.008227287791669369\n",
      "dL/db: 0.0012279534712433815\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Create input and target\n",
    "x = torch.tensor(6.7, requires_grad=True)\n",
    "y = torch.tensor(0.0)\n",
    "# Initialize weights and bias\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "# Forward pass\n",
    "z = w * x + b\n",
    "y_pred = torch.sigmoid(z)\n",
    "# Calculate loss\n",
    "loss = y_pred * (1 - y) + (1 - y_pred) * y\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "# Get gradients\n",
    "print(f\"dL/dw: {w.grad}\")\n",
    "print(f\"dL/db: {b.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
